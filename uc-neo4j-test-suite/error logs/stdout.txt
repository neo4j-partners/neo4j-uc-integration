CONF_DRIVER_JAVA_OPTS = -Djava.io.tmpdir=/local_disk0/tmp -XX:-OmitStackTraceInFastThrow -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -Xlog:gc:stdout:time,uptime,level,tags -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true  -XX:+UseBiasedLocking  -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED -XX:+UseParallelGC -Ddatabricks.serviceName=driver-1 -Xms7850m -Xmx7850m -Dspark.ui.port=40001
CONF_EXECUTOR_JAVA_OPTS = -Dspark.executor.extraJavaOptions="-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -Xlog:gc:stdout:time,uptime,level,tags -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true  -XX:+UseBiasedLocking   -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED -XX:+UseParallelGC -Ddatabricks.serviceName=spark-executor-1" -Dspark.executor.memory=8874m
JAVA_OPTS =   -Djava.io.tmpdir=/local_disk0/tmp -XX:-OmitStackTraceInFastThrow -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -Xlog:gc:stdout:time,uptime,level,tags -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true  -XX:+UseBiasedLocking  -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED -XX:+UseParallelGC -Ddatabricks.serviceName=driver-1 -Xms7850m -Xmx7850m -Dspark.ui.port=40001 -Dspark.executor.extraJavaOptions="-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -Xlog:gc:stdout:time,uptime,level,tags -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true  -XX:+UseBiasedLocking   -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED -XX:+UseParallelGC -Ddatabricks.serviceName=spark-executor-1" -Dspark.executor.memory=8874m -Dspark.executor.extraClassPath=/databricks/hadoop-safety-jars/*:/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*
Using Spark master local[4]
AppCDS disabled: See Chauffeur logs for details.
Thu Jan  8 16:06:34 UTC 2026
Starting driver
Driver command is /usr/lib/jvm/zulu17-ca-amd64//bin/java   -Djava.io.tmpdir=/local_disk0/tmp -XX:-OmitStackTraceInFastThrow -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -Xlog:gc:stdout:time,uptime,level,tags -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true  -XX:+UseBiasedLocking  -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED -XX:+UseParallelGC -Ddatabricks.serviceName=driver-1 -Xms7850m -Xmx7850m -Dspark.ui.port=40001 -Dspark.executor.extraJavaOptions="-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -Xlog:gc:stdout:time,uptime,level,tags -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true  -XX:+UseBiasedLocking   -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED -XX:+UseParallelGC -Ddatabricks.serviceName=spark-executor-1" -Dspark.executor.memory=8874m -Dspark.executor.extraClassPath=/databricks/hadoop-safety-jars/*:/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*  -cp /databricks/hadoop-safety-jars/*:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/log4j/driver:/databricks/hive/conf:/databricks/spark/dbconf/hadoop:/databricks/jars/* com.databricks.backend.daemon.driver.DriverDaemon &
Working directory is /databricks/driver
[2026-01-08T16:06:34.802+0000][0.007s][info][gc] Using Parallel
[Global flags]
      int ActiveProcessorCount                     = -1                                        {product} {default}
    uintx AdaptiveSizeDecrementScaleFactor         = 4                                         {product} {default}
    uintx AdaptiveSizeMajorGCDecayTimeScale        = 10                                        {product} {default}
    uintx AdaptiveSizePolicyCollectionCostMargin   = 50                                        {product} {default}
    uintx AdaptiveSizePolicyInitializingSteps      = 20                                        {product} {default}
    uintx AdaptiveSizePolicyOutputInterval         = 0                                         {product} {default}
    uintx AdaptiveSizePolicyWeight                 = 10                                        {product} {default}
    uintx AdaptiveSizeThroughPutPolicy             = 0                                         {product} {default}
    uintx AdaptiveTimeWeight                       = 25                                        {product} {default}
     bool AdjustStackSizeForTLS                    = false                                     {product} {default}
     bool AggressiveHeap                           = false                                     {product} {default}
     intx AliasLevel                               = 3                                      {C2 product} {default}
     bool AlignVector                              = false                                  {C2 product} {default}
    ccstr AllocateHeapAt                           =                                           {product} {default}
     intx AllocateInstancePrefetchLines            = 1                                         {product} {default}
     intx AllocatePrefetchDistance                 = 192                                       {product} {default}
     intx AllocatePrefetchInstr                    = 3                                         {product} {default}
     intx AllocatePrefetchLines                    = 4                                         {product} {default}
     intx AllocatePrefetchStepSize                 = 64                                        {product} {default}
     intx AllocatePrefetchStyle                    = 1                                         {product} {default}
     bool AllowParallelDefineClass                 = false                                     {product} {default}
     bool AllowRedefinitionToAddDeleteMethods      = false                                     {product} {default}
     bool AllowUserSignalHandlers                  = false                                     {product} {default}
     bool AllowVectorizeOnDemand                   = true                                   {C2 product} {default}
     bool AlwaysActAsServerClassMachine            = false                                     {product} {default}
     bool AlwaysCompileLoopMethods                 = false                                     {product} {default}
     bool AlwaysLockClassLoader                    = false                                     {product} {default}
     bool AlwaysPreTouch                           = false                                     {product} {default}
     bool AlwaysRestoreFPU                         = false                                     {product} {default}
     bool AlwaysTenure                             = false                                     {product} {default}
    ccstr ArchiveClassesAtExit                     =                                           {product} {default}
     intx ArrayCopyLoadStoreMaxElem                = 8                                      {C2 product} {default}
   size_t AsyncLogBufferSize                       = 2097152                                   {product} {default}
     intx AutoBoxCacheMax                          = 128                                    {C2 product} {default}
    ccstr AzCRSArguments                           =                                           {product} {default}
    ccstr AzCRSMode                                = off                                       {product} {default}
     intx BCEATraceLevel                           = 0                                         {product} {default}
     bool BackgroundCompilation                    = true                                   {pd product} {default}
   size_t BaseFootPrintEstimate                    = 268435456                                 {product} {default}
     intx BiasedLockingBulkRebiasThreshold         = 20                                        {product} {default}
     intx BiasedLockingBulkRevokeThreshold         = 40                                        {product} {default}
     intx BiasedLockingDecayTime                   = 25000                                     {product} {default}
     intx BiasedLockingStartupDelay                = 0                                         {product} {default}
     bool BlockLayoutByFrequency                   = true                                   {C2 product} {default}
     intx BlockLayoutMinDiamondPercentage          = 20                                     {C2 product} {default}
     bool BlockLayoutRotateLoops                   = true                                   {C2 product} {default}
     intx C1InlineStackLimit                       = 5                                      {C1 product} {default}
     intx C1MaxInlineLevel                         = 9                                      {C1 product} {default}
     intx C1MaxInlineSize                          = 35                                     {C1 product} {default}
     intx C1MaxRecursiveInlineLevel                = 1                                      {C1 product} {default}
     intx C1MaxTrivialSize                         = 6                                      {C1 product} {default}
     bool C1OptimizeVirtualCallProfiling           = true                                   {C1 product} {default}
     bool C1ProfileBranches                        = true                                   {C1 product} {default}
     bool C1ProfileCalls                           = true                                   {C1 product} {default}
     bool C1ProfileCheckcasts                      = true                                   {C1 product} {default}
     bool C1ProfileInlinedCalls                    = true                                   {C1 product} {default}
     bool C1ProfileVirtualCalls                    = true                                   {C1 product} {default}
     bool C1UpdateMethodData                       = true                                   {C1 product} {default}
     intx CICompilerCount                          = 3                                         {product} {ergonomic}
     bool CICompilerCountPerCPU                    = true                                      {product} {default}
     bool CITime                                   = false                                     {product} {default}
     bool CheckJNICalls                            = false                                     {product} {default}
     bool ClassUnloading                           = true                                      {product} {default}
     bool ClassUnloadingWithConcurrentMark         = true                                      {product} {default}
     bool ClipInlining                             = true                                      {product} {default}
    uintx CodeCacheExpansionSize                   = 65536                                  {pd product} {default}
     bool CompactStrings                           = true                                   {pd product} {default}
    ccstr CompilationMode                          = default                                   {product} {default}
ccstrlist CompileCommand                           =                                           {product} {default}
    ccstr CompileCommandFile                       =                                           {product} {default}
ccstrlist CompileOnly                              =                                           {product} {default}
     intx CompileThreshold                         = 10000                                  {pd product} {default}
   double CompileThresholdScaling                  = 1.000000                                  {product} {default}
     intx CompilerThreadPriority                   = -1                                        {product} {default}
     intx CompilerThreadStackSize                  = 1024                                   {pd product} {default}
   size_t CompressedClassSpaceSize                 = 1073741824                                {product} {default}
     uint ConcGCThreads                            = 0                                         {product} {default}
     intx ConditionalMoveLimit                     = 3                                   {C2 pd product} {default}
     intx ContendedPaddingWidth                    = 128                                       {product} {default}
     bool CrashOnOutOfMemoryError                  = false                                     {product} {default}
     bool CreateCoredumpOnCrash                    = true                                      {product} {default}
     bool CriticalJNINatives                       = false                                     {product} {default}
     bool DTraceAllocProbes                        = false                                     {product} {default}
     bool DTraceMethodProbes                       = false                                     {product} {default}
     bool DTraceMonitorProbes                      = false                                     {product} {default}
     bool DisableAttachMechanism                   = false                                     {product} {default}
     bool DisableExplicitGC                        = false                                     {product} {default}
     bool DisplayVMOutputToStderr                  = false                                     {product} {default}
     bool DisplayVMOutputToStdout                  = false                                     {product} {default}
     bool DoEscapeAnalysis                         = true                                   {C2 product} {default}
     bool DoReserveCopyInSuperWord                 = true                                   {C2 product} {default}
     bool DontCompileHugeMethods                   = true                                      {product} {default}
     bool DontYieldALot                            = false                                  {pd product} {default}
    ccstr DumpLoadedClassList                      =                                           {product} {default}
     bool DumpReplayDataOnError                    = true                                      {product} {default}
     bool DumpSharedSpaces                         = false                                     {product} {default}
     bool DynamicDumpSharedSpaces                  = false                                     {product} {default}
     bool EagerXrunInit                            = false                                     {product} {default}
     intx EliminateAllocationArraySizeLimit        = 64                                     {C2 product} {default}
     bool EliminateAllocations                     = true                                   {C2 product} {default}
     bool EliminateAutoBox                         = true                                   {C2 product} {default}
     bool EliminateLocks                           = true                                   {C2 product} {default}
     bool EliminateNestedLocks                     = true                                   {C2 product} {default}
     bool EnableContended                          = true                                      {product} {default}
     bool EnableDynamicAgentLoading                = true                                      {product} {default}
   size_t ErgoHeapSizeLimit                        = 0                                         {product} {default}
    ccstr ErrorFile                                =                                           {product} {default}
     bool ErrorFileToStderr                        = false                                     {product} {default}
     bool ErrorFileToStdout                        = false                                     {product} {default}
 uint64_t ErrorLogTimeout                          = 120                                       {product} {default}
   double EscapeAnalysisTimeout                    = 20.000000                              {C2 product} {default}
     bool EstimateArgEscape                        = true                                      {product} {default}
     bool ExecutingUnitTests                       = false                                     {product} {default}
     bool ExitOnOutOfMemoryError                   = false                                     {product} {default}
     bool ExplicitGCInvokesConcurrent              = false                                     {product} {default}
     bool ExtendedDTraceProbes                     = false                                     {product} {default}
     bool ExtensiveErrorReports                    = false                                     {product} {default}
    ccstr ExtraSharedClassListFile                 =                                           {product} {default}
     bool FilterSpuriousWakeups                    = true                                      {product} {default}
     bool FlightRecorder                           = false                                     {product} {default}
    ccstr FlightRecorderOptions                    =                                           {product} {default}
     bool ForceTimeHighResolution                  = false                                     {product} {default}
     intx FreqInlineSize                           = 325                                 {C2 pd product} {default}
   double G1ConcMarkStepDurationMillis             = 10.000000                                 {product} {default}
    uintx G1ConcRSHotCardLimit                     = 4                                         {product} {default}
   size_t G1ConcRSLogCacheSize                     = 10                                        {product} {default}
   size_t G1ConcRefinementGreenZone                = 0                                         {product} {default}
   size_t G1ConcRefinementRedZone                  = 0                                         {product} {default}
    uintx G1ConcRefinementServiceIntervalMillis    = 300                                       {product} {default}
     uint G1ConcRefinementThreads                  = 0                                         {product} {default}
   size_t G1ConcRefinementThresholdStep            = 2                                         {product} {default}
   size_t G1ConcRefinementYellowZone               = 0                                         {product} {default}
    uintx G1ConfidencePercent                      = 50                                        {product} {default}
   size_t G1HeapRegionSize                         = 0                                         {product} {default}
    uintx G1HeapWastePercent                       = 5                                         {product} {default}
    uintx G1MixedGCCountTarget                     = 8                                         {product} {default}
    uintx G1PeriodicGCInterval                     = 0                                      {manageable} {default}
     bool G1PeriodicGCInvokesConcurrent            = true                                      {product} {default}
   double G1PeriodicGCSystemLoadThreshold          = 0.000000                               {manageable} {default}
     intx G1RSetRegionEntries                      = 0                                         {product} {default}
     intx G1RSetSparseRegionEntries                = 0                                         {product} {default}
     intx G1RSetUpdatingPauseTimePercent           = 10                                        {product} {default}
     uint G1RefProcDrainInterval                   = 1000                                      {product} {default}
    uintx G1ReservePercent                         = 10                                        {product} {default}
    uintx G1SATBBufferEnqueueingThresholdPercent   = 60                                        {product} {default}
   size_t G1SATBBufferSize                         = 1024                                      {product} {default}
   size_t G1UpdateBufferSize                       = 256                                       {product} {default}
     bool G1UseAdaptiveConcRefinement              = true                                      {product} {default}
     bool G1UseAdaptiveIHOP                        = true                                      {product} {default}
    uintx GCDrainStackTargetSize                   = 64                                        {product} {default}
    uintx GCHeapFreeLimit                          = 2                                         {product} {default}
    uintx GCLockerEdenExpansionPercent             = 5                                         {product} {default}
    uintx GCPauseIntervalMillis                    = 0                                         {product} {default}
    uintx GCTimeLimit                              = 98                                        {product} {default}
    uintx GCTimeRatio                              = 99                                        {product} {default}
   size_t HeapBaseMinAddress                       = 2147483648                             {pd product} {default}
     bool HeapDumpAfterFullGC                      = false                                  {manageable} {default}
     bool HeapDumpBeforeFullGC                     = false                                  {manageable} {default}
     intx HeapDumpGzipLevel                        = 0                                      {manageable} {default}
     bool HeapDumpOnOutOfMemoryError               = false                                  {manageable} {default}
    ccstr HeapDumpPath                             =                                        {manageable} {default}
    uintx HeapFirstMaximumCompactionCount          = 3                                         {product} {default}
    uintx HeapMaximumCompactionInterval            = 20                                        {product} {default}
    uintx HeapSearchSteps                          = 3                                         {product} {default}
   size_t HeapSizePerGCThread                      = 43620760                                  {product} {default}
     bool IgnoreEmptyClassPaths                    = false                                     {product} {default}
     bool IgnoreUnrecognizedVMOptions              = true                                      {product} {command line}
    uintx IncreaseFirstTierCompileThresholdAt      = 50                                        {product} {default}
     bool IncrementalInline                        = true                                   {C2 product} {default}
    uintx InitialCodeCacheSize                     = 2555904                                {pd product} {default}
   size_t InitialHeapSize                          = 8231321600                                {product} {command line}
    uintx InitialRAMFraction                       = 64                                        {product} {default}
   double InitialRAMPercentage                     = 1.562500                                  {product} {default}
    uintx InitialSurvivorRatio                     = 8                                         {product} {default}
    uintx InitialTenuringThreshold                 = 7                                         {product} {default}
    uintx InitiatingHeapOccupancyPercent           = 45                                        {product} {default}
     bool Inline                                   = true                                      {product} {default}
    ccstr InlineDataFile                           =                                           {product} {default}
     intx InlineSmallCode                          = 2500                                {C2 pd product} {default}
     bool InlineSynchronizedMethods                = true                                   {C1 product} {default}
     intx InteriorEntryAlignment                   = 16                                  {C2 pd product} {default}
     intx InterpreterProfilePercentage             = 33                                        {product} {default}
     bool JavaMonitorsInStackTrace                 = true                                      {product} {default}
     intx JavaPriority10_To_OSPriority             = -1                                        {product} {default}
     intx JavaPriority1_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority2_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority3_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority4_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority5_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority6_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority7_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority8_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority9_To_OSPriority              = -1                                        {product} {default}
   size_t LargePageHeapSizeThreshold               = 134217728                                 {product} {default}
   size_t LargePageSizeInBytes                     = 0                                         {product} {default}
     intx LiveNodeCountInliningCutoff              = 40000                                  {C2 product} {default}
     bool LoadExecStackDllInVMThread               = true                                      {product} {default}
     intx LoopMaxUnroll                            = 16                                     {C2 product} {default}
     intx LoopOptsCount                            = 43                                     {C2 product} {default}
     intx LoopPercentProfileLimit                  = 10                                  {C2 pd product} {default}
    uintx LoopStripMiningIter                      = 0                                      {C2 product} {default}
    uintx LoopStripMiningIterShortLoop             = 0                                      {C2 product} {default}
     intx LoopUnrollLimit                          = 60                                  {C2 pd product} {default}
     intx LoopUnrollMin                            = 4                                      {C2 product} {default}
     bool LoopUnswitching                          = true                                   {C2 product} {default}
     bool ManagementServer                         = false                                     {product} {default}
   size_t MarkStackSize                            = 4194304                                   {product} {default}
   size_t MarkStackSizeMax                         = 536870912                                 {product} {default}
     uint MarkSweepAlwaysCompactCount              = 4                                         {product} {default}
    uintx MarkSweepDeadRatio                       = 1                                         {product} {default}
     intx MaxBCEAEstimateLevel                     = 5                                         {product} {default}
     intx MaxBCEAEstimateSize                      = 150                                       {product} {default}
 uint64_t MaxDirectMemorySize                      = 0                                         {product} {default}
     bool MaxFDLimit                               = true                                      {product} {default}
    uintx MaxGCMinorPauseMillis                    = 18446744073709551615                      {product} {default}
    uintx MaxGCPauseMillis                         = 18446744073709551614                      {product} {default}
    uintx MaxHeapFreeRatio                         = 100                                    {manageable} {default}
   size_t MaxHeapSize                              = 8231321600                                {product} {command line}
     intx MaxInlineLevel                           = 15                                     {C2 product} {default}
     intx MaxInlineSize                            = 35                                     {C2 product} {default}
     intx MaxJNILocalCapacity                      = 65536                                     {product} {default}
     intx MaxJavaStackTraceDepth                   = 1024                                      {product} {default}
     intx MaxJumpTableSize                         = 65000                                  {C2 product} {default}
     intx MaxJumpTableSparseness                   = 5                                      {C2 product} {default}
     intx MaxLabelRootDepth                        = 1100                                   {C2 product} {default}
     intx MaxLoopPad                               = 11                                     {C2 product} {default}
   size_t MaxMetaspaceExpansion                    = 5439488                                   {product} {default}
    uintx MaxMetaspaceFreeRatio                    = 70                                        {product} {default}
   size_t MaxMetaspaceSize                         = 18446744073709551615                      {product} {default}
   size_t MaxNewSize                               = 2743599104                                {product} {ergonomic}
     intx MaxNodeLimit                             = 80000                                  {C2 product} {default}
 uint64_t MaxRAM                                   = 137438953472                           {pd product} {default}
    uintx MaxRAMFraction                           = 4                                         {product} {default}
   double MaxRAMPercentage                         = 25.000000                                 {product} {default}
     intx MaxRecursiveInlineLevel                  = 1                                      {C2 product} {default}
    uintx MaxTenuringThreshold                     = 15                                        {product} {default}
     intx MaxTrivialSize                           = 6                                      {C2 product} {default}
     intx MaxVectorSize                            = 64                                     {C2 product} {default}
    ccstr MetaspaceReclaimPolicy                   = balanced                                  {product} {default}
   size_t MetaspaceSize                            = 22020096                                  {product} {default}
     bool MethodFlushing                           = true                                      {product} {default}
   size_t MinHeapDeltaBytes                        = 524288                                    {product} {ergonomic}
    uintx MinHeapFreeRatio                         = 0                                      {manageable} {default}
   size_t MinHeapSize                              = 8231321600                                {product} {command line}
     intx MinInliningThreshold                     = 250                                       {product} {default}
     intx MinJumpTableSize                         = 10                                  {C2 pd product} {default}
   size_t MinMetaspaceExpansion                    = 327680                                    {product} {default}
    uintx MinMetaspaceFreeRatio                    = 40                                        {product} {default}
    uintx MinRAMFraction                           = 2                                         {product} {default}
   double MinRAMPercentage                         = 50.000000                                 {product} {default}
    uintx MinSurvivorRatio                         = 3                                         {product} {default}
   size_t MinTLABSize                              = 2048                                      {product} {default}
     intx MultiArrayExpandLimit                    = 6                                      {C2 product} {default}
    uintx NUMAChunkResizeWeight                    = 20                                        {product} {default}
   size_t NUMAInterleaveGranularity                = 2097152                                   {product} {default}
    uintx NUMAPageScanRate                         = 256                                       {product} {default}
   size_t NUMASpaceResizeRate                      = 1073741824                                {product} {default}
     bool NUMAStats                                = false                                     {product} {default}
    ccstr NativeMemoryTracking                     = off                                       {product} {default}
     bool NeverActAsServerClassMachine             = false                                  {pd product} {default}
     bool NeverTenure                              = false                                     {product} {default}
    uintx NewRatio                                 = 2                                         {product} {default}
   size_t NewSize                                  = 2743599104                                {product} {ergonomic}
   size_t NewSizeThreadIncrease                    = 5320                                   {pd product} {default}
     intx NmethodSweepActivity                     = 10                                        {product} {default}
     intx NodeLimitFudgeFactor                     = 2000                                   {C2 product} {default}
    uintx NonNMethodCodeHeapSize                   = 5832780                                {pd product} {ergonomic}
    uintx NonProfiledCodeHeapSize                  = 122912730                              {pd product} {ergonomic}
     intx NumberOfLoopInstrToAlign                 = 4                                      {C2 product} {default}
     intx ObjectAlignmentInBytes                   = 8                              {product lp64_product} {default}
   size_t OldPLABSize                              = 1024                                      {product} {default}
   size_t OldSize                                  = 5487722496                                {product} {ergonomic}
     bool OmitStackTraceInFastThrow                = false                                     {product} {command line}
ccstrlist OnError                                  =                                           {product} {default}
ccstrlist OnOutOfMemoryError                       =                                           {product} {default}
     intx OnStackReplacePercentage                 = 140                                    {pd product} {default}
     bool OptimizeFill                             = false                                  {C2 product} {default}
     bool OptimizePtrCompare                       = true                                   {C2 product} {default}
     bool OptimizeStringConcat                     = true                                   {C2 product} {default}
     bool OptoBundling                             = false                               {C2 pd product} {default}
     intx OptoLoopAlignment                        = 16                                     {pd product} {default}
     bool OptoRegScheduling                        = true                                {C2 pd product} {default}
     bool OptoScheduling                           = false                               {C2 pd product} {default}
     bool OverrideVMProperties                     = false                                     {product} {default}
    uintx PLABWeight                               = 75                                        {product} {default}
     bool PSChunkLargeArrays                       = true                                      {product} {default}
      int ParGCArrayScanChunk                      = 50                                        {product} {default}
    uintx ParallelGCBufferWastePct                 = 10                                        {product} {default}
     uint ParallelGCThreads                        = 4                                         {product} {default}
   size_t ParallelOldDeadWoodLimiterMean           = 50                                        {product} {default}
   size_t ParallelOldDeadWoodLimiterStdDev         = 80                                        {product} {default}
     bool ParallelRefProcBalancingEnabled          = true                                      {product} {default}
     bool ParallelRefProcEnabled                   = true                                      {product} {default}
     bool PartialPeelAtUnsignedTests               = true                                   {C2 product} {default}
     bool PartialPeelLoop                          = true                                   {C2 product} {default}
     intx PartialPeelNewPhiDelta                   = 0                                      {C2 product} {default}
    uintx PausePadding                             = 1                                         {product} {default}
     intx PerBytecodeRecompilationCutoff           = 200                                       {product} {default}
     intx PerBytecodeTrapLimit                     = 4                                         {product} {default}
     intx PerMethodRecompilationCutoff             = 400                                       {product} {default}
     intx PerMethodTrapLimit                       = 100                                       {product} {default}
     bool PerfAllowAtExitRegistration              = false                                     {product} {default}
     bool PerfBypassFileSystemCheck                = false                                     {product} {default}
     intx PerfDataMemorySize                       = 32768                                     {product} {default}
     intx PerfDataSamplingInterval                 = 50                                        {product} {default}
    ccstr PerfDataSaveFile                         =                                           {product} {default}
     bool PerfDataSaveToFile                       = false                                     {product} {default}
     bool PerfDisableSharedMem                     = false                                     {product} {default}
     intx PerfMaxStringConstLength                 = 1024                                      {product} {default}
   size_t PreTouchParallelChunkSize                = 4194304                                {pd product} {default}
     bool PreferContainerQuotaForCPUCount          = true                                      {product} {default}
     bool PreferInterpreterNativeStubs             = false                                  {pd product} {default}
     intx PrefetchCopyIntervalInBytes              = 576                                       {product} {default}
     intx PrefetchFieldsAhead                      = 1                                         {product} {default}
     intx PrefetchScanIntervalInBytes              = 576                                       {product} {default}
     bool PreserveAllAnnotations                   = false                                     {product} {default}
     bool PreserveFramePointer                     = false                                  {pd product} {default}
   size_t PretenureSizeThreshold                   = 0                                         {product} {default}
     bool PrintClassHistogram                      = false                                  {manageable} {default}
     bool PrintCodeCache                           = false                                     {product} {default}
     bool PrintCodeCacheOnCompilation              = false                                     {product} {default}
     bool PrintCommandLineFlags                    = false                                     {product} {default}
     bool PrintCompilation                         = false                                     {product} {default}
     bool PrintConcurrentLocks                     = false                                  {manageable} {default}
     bool PrintExtendedThreadInfo                  = false                                     {product} {default}
     bool PrintFlagsFinal                          = true                                      {product} {command line}
     bool PrintFlagsInitial                        = false                                     {product} {default}
     bool PrintFlagsRanges                         = false                                     {product} {default}
     bool PrintGC                                  = false                                     {product} {default}
     bool PrintGCDetails                           = false                                     {product} {default}
     bool PrintHeapAtSIGBREAK                      = true                                      {product} {default}
     bool PrintSharedArchiveAndExit                = false                                     {product} {default}
     bool PrintSharedDictionary                    = false                                     {product} {default}
     bool PrintStringTableStatistics               = false                                     {product} {default}
     bool PrintTieredEvents                        = false                                     {product} {default}
     bool PrintVMOptions                           = false                                     {product} {default}
     bool PrintWarnings                            = true                                      {product} {default}
    uintx ProcessDistributionStride                = 4                                         {product} {default}
     bool ProfileInterpreter                       = true                                   {pd product} {default}
     intx ProfileMaturityPercentage                = 20                                        {product} {default}
    uintx ProfiledCodeHeapSize                     = 122912730                              {pd product} {ergonomic}
    uintx PromotedPadding                          = 3                                         {product} {default}
    uintx QueuedAllocationWarningCount             = 0                                         {product} {default}
      int RTMRetryCount                            = 5                                    {ARCH product} {default}
     bool RangeCheckElimination                    = true                                      {product} {default}
     bool ReassociateInvariants                    = true                                   {C2 product} {default}
     bool RecordDynamicDumpInfo                    = false                                     {product} {default}
     bool ReduceBulkZeroing                        = true                                   {C2 product} {default}
     bool ReduceFieldZeroing                       = true                                   {C2 product} {default}
     bool ReduceInitialCardMarks                   = true                                   {C2 product} {default}
     bool ReduceSignalUsage                        = false                                     {product} {default}
     intx RefDiscoveryPolicy                       = 0                                         {product} {default}
     bool RegisterFinalizersAtInit                 = true                                      {product} {default}
     bool RelaxAccessControlCheck                  = false                                     {product} {default}
    ccstr ReplayDataFile                           =                                           {product} {default}
     bool RequireSharedSpaces                      = false                                     {product} {default}
    uintx ReservedCodeCacheSize                    = 251658240                              {pd product} {ergonomic}
     bool ResizePLAB                               = true                                      {product} {default}
     bool ResizeTLAB                               = true                                      {product} {default}
     bool RestoreMXCSROnJNICalls                   = false                                     {product} {default}
     bool RestrictContended                        = true                                      {product} {default}
     bool RestrictReservedStack                    = true                                      {product} {default}
     bool RewriteBytecodes                         = true                                   {pd product} {default}
     bool RewriteFrequentPairs                     = true                                   {pd product} {default}
     bool SafepointTimeout                         = false                                     {product} {default}
     intx SafepointTimeoutDelay                    = 10000                                     {product} {default}
     bool ScavengeBeforeFullGC                     = true                                      {product} {default}
     bool SegmentedCodeCache                       = true                                      {product} {ergonomic}
     intx SelfDestructTimer                        = 0                                         {product} {default}
    ccstr SharedArchiveConfigFile                  =                                           {product} {default}
    ccstr SharedArchiveFile                        =                                           {product} {default}
   size_t SharedBaseAddress                        = 140037861670912                           {product} {default}
    ccstr SharedClassListFile                      =                                           {product} {default}
    uintx SharedSymbolTableBucketSize              = 4                                         {product} {default}
    ccstr ShenandoahGCHeuristics                   = adaptive                                  {product} {default}
    ccstr ShenandoahGCMode                         = satb                                      {product} {default}
     bool ShowCodeDetailsInExceptionMessages       = true                                   {manageable} {default}
     bool ShowMessageBoxOnError                    = false                                     {product} {default}
     bool ShrinkHeapInSteps                        = true                                      {product} {default}
   size_t SoftMaxHeapSize                          = 8231321600                             {manageable} {ergonomic}
     intx SoftRefLRUPolicyMSPerMB                  = 1000                                      {product} {default}
     bool SplitIfBlocks                            = true                                   {C2 product} {default}
     intx StackRedPages                            = 1                                      {pd product} {default}
     intx StackReservedPages                       = 1                                      {pd product} {default}
     intx StackShadowPages                         = 20                                     {pd product} {default}
     bool StackTraceInThrowable                    = true                                      {product} {default}
     intx StackYellowPages                         = 2                                      {pd product} {default}
    uintx StartAggressiveSweepingAt                = 10                                        {product} {default}
     bool StartAttachListener                      = false                                     {product} {default}
    ccstr StartFlightRecording                     =                                           {product} {default}
     uint StringDeduplicationAgeThreshold          = 3                                         {product} {default}
    uintx StringTableSize                          = 65536                                     {product} {default}
     bool SuperWordLoopUnrollAnalysis              = true                                {C2 pd product} {default}
     bool SuperWordReductions                      = true                                   {C2 product} {default}
     bool SuppressFatalErrorMessage                = false                                     {product} {default}
    uintx SurvivorPadding                          = 3                                         {product} {default}
    uintx SurvivorRatio                            = 8                                         {product} {default}
   double SweeperThreshold                         = 0.500000                                  {product} {default}
    uintx TLABAllocationWeight                     = 35                                        {product} {default}
    uintx TLABRefillWasteFraction                  = 64                                        {product} {default}
   size_t TLABSize                                 = 0                                         {product} {default}
     bool TLABStats                                = true                                      {product} {default}
    uintx TLABWasteIncrement                       = 4                                         {product} {default}
    uintx TLABWasteTargetPercent                   = 1                                         {product} {default}
    uintx TargetPLABWastePct                       = 10                                        {product} {default}
    uintx TargetSurvivorRatio                      = 50                                        {product} {default}
    uintx TenuredGenerationSizeIncrement           = 20                                        {product} {default}
    uintx TenuredGenerationSizeSupplement          = 80                                        {product} {default}
    uintx TenuredGenerationSizeSupplementDecay     = 2                                         {product} {default}
     intx ThreadPriorityPolicy                     = 0                                         {product} {default}
     bool ThreadPriorityVerbose                    = false                                     {product} {default}
     intx ThreadStackSize                          = 4096                                   {pd product} {command line}
    uintx ThresholdTolerance                       = 10                                        {product} {default}
     intx Tier0BackedgeNotifyFreqLog               = 10                                        {product} {default}
     intx Tier0InvokeNotifyFreqLog                 = 7                                         {product} {default}
     intx Tier0ProfilingStartPercentage            = 200                                       {product} {default}
     intx Tier23InlineeNotifyFreqLog               = 20                                        {product} {default}
     intx Tier2BackEdgeThreshold                   = 0                                         {product} {default}
     intx Tier2BackedgeNotifyFreqLog               = 14                                        {product} {default}
     intx Tier2CompileThreshold                    = 0                                         {product} {default}
     intx Tier2InvokeNotifyFreqLog                 = 11                                        {product} {default}
     intx Tier3BackEdgeThreshold                   = 60000                                     {product} {default}
     intx Tier3BackedgeNotifyFreqLog               = 13                                        {product} {default}
     intx Tier3CompileThreshold                    = 2000                                      {product} {default}
     intx Tier3DelayOff                            = 2                                         {product} {default}
     intx Tier3DelayOn                             = 5                                         {product} {default}
     intx Tier3InvocationThreshold                 = 200                                       {product} {default}
     intx Tier3InvokeNotifyFreqLog                 = 10                                        {product} {default}
     intx Tier3LoadFeedback                        = 5                                         {product} {default}
     intx Tier3MinInvocationThreshold              = 100                                       {product} {default}
     intx Tier4BackEdgeThreshold                   = 40000                                     {product} {default}
     intx Tier4CompileThreshold                    = 15000                                     {product} {default}
     intx Tier4InvocationThreshold                 = 5000                                      {product} {default}
     intx Tier4LoadFeedback                        = 3                                         {product} {default}
     intx Tier4MinInvocationThreshold              = 600                                       {product} {default}
     bool TieredCompilation                        = true                                   {pd product} {default}
     intx TieredCompileTaskTimeout                 = 50                                        {product} {default}
     intx TieredRateUpdateMaxTime                  = 25                                        {product} {default}
     intx TieredRateUpdateMinTime                  = 1                                         {product} {default}
     intx TieredStopAtLevel                        = 4                                         {product} {default}
     bool TimeLinearScan                           = false                                  {C1 product} {default}
    ccstr TraceJVMTI                               =                                           {product} {default}
     intx TrackedInitializationLimit               = 50                                     {C2 product} {default}
     bool TrapBasedNullChecks                      = false                                  {pd product} {default}
     bool TrapBasedRangeChecks                     = false                               {C2 pd product} {default}
     uint TrimNativeHeapInterval                   = 0                                         {product} {default}
     intx TypeProfileArgsLimit                     = 2                                         {product} {default}
    uintx TypeProfileLevel                         = 111                                    {pd product} {default}
     intx TypeProfileMajorReceiverPercent          = 90                                     {C2 product} {default}
     intx TypeProfileParmsLimit                    = 2                                         {product} {default}
     intx TypeProfileWidth                         = 2                                         {product} {default}
     intx UnguardOnExecutionViolation              = 0                                         {product} {default}
     bool UseAES                                   = true                                      {product} {default}
     intx UseAVX                                   = 3                                    {ARCH product} {default}
     bool UseAdaptiveGenerationSizePolicyAtMajorCollection  = true                             {product} {default}
     bool UseAdaptiveGenerationSizePolicyAtMinorCollection  = true                             {product} {default}
     bool UseAdaptiveNUMAChunkSizing               = true                                      {product} {default}
     bool UseAdaptiveSizeDecayMajorGCCost          = true                                      {product} {default}
     bool UseAdaptiveSizePolicy                    = true                                      {product} {default}
     bool UseAdaptiveSizePolicyFootprintGoal       = true                                      {product} {default}
     bool UseAdaptiveSizePolicyWithSystemGC        = false                                     {product} {default}
     bool UseAddressNop                            = true                                 {ARCH product} {default}
     bool UseBASE64Intrinsics                      = true                                      {product} {default}
     bool UseBMI1Instructions                      = true                                 {ARCH product} {default}
     bool UseBMI2Instructions                      = true                                 {ARCH product} {default}
     bool UseBiasedLocking                         = true                                      {product} {command line}
     bool UseBimorphicInlining                     = true                                   {C2 product} {default}
     bool UseCLMUL                                 = true                                 {ARCH product} {default}
     bool UseCMoveUnconditionally                  = false                                  {C2 product} {default}
     bool UseCodeAging                             = true                                      {product} {default}
     bool UseCodeCacheFlushing                     = true                                      {product} {default}
     bool UseCompiler                              = true                                      {product} {default}
     bool UseCompressedClassPointers               = true                           {product lp64_product} {ergonomic}
     bool UseCompressedOops                        = true                           {product lp64_product} {ergonomic}
     bool UseCondCardMark                          = false                                     {product} {default}
     bool UseContainerCpuShares                    = false                                     {product} {default}
     bool UseContainerSupport                      = false                                     {product} {command line}
     bool UseCountLeadingZerosInstruction          = true                                 {ARCH product} {default}
     bool UseCountTrailingZerosInstruction         = true                                 {ARCH product} {default}
     bool UseCountedLoopSafepoints                 = false                                  {C2 product} {default}
     bool UseCounterDecay                          = true                                      {product} {default}
     bool UseDivMod                                = true                                   {C2 product} {default}
     bool UseDynamicNumberOfCompilerThreads        = true                                      {product} {default}
     bool UseDynamicNumberOfGCThreads              = true                                      {product} {default}
     bool UseEmptySlotsInSupers                    = true                                      {product} {default}
     bool UseFMA                                   = true                                      {product} {default}
     bool UseFPUForSpilling                        = true                                   {C2 product} {default}
     bool UseFastJNIAccessors                      = true                                      {product} {default}
     bool UseFastStosb                             = false                                {ARCH product} {default}
     bool UseG1GC                                  = false                                     {product} {default}
     bool UseGCOverheadLimit                       = true                                      {product} {default}
     bool UseHeavyMonitors                         = false                                     {product} {default}
     bool UseHugeTLBFS                             = false                                     {product} {default}
     bool UseInlineCaches                          = true                                      {product} {default}
     bool UseInterpreter                           = true                                      {product} {default}
     bool UseJumpTables                            = true                                   {C2 product} {default}
     bool UseLargePages                            = false                                  {pd product} {default}
     bool UseLargePagesIndividualAllocation        = false                                  {pd product} {default}
     bool UseLinuxPosixThreadCPUClocks             = true                                      {product} {default}
     bool UseLoopCounter                           = true                                      {product} {default}
     bool UseLoopInvariantCodeMotion               = true                                   {C1 product} {default}
     bool UseLoopPredicate                         = true                                   {C2 product} {default}
     bool UseMaximumCompactionOnSystemGC           = true                                      {product} {default}
     bool UseNUMA                                  = false                                     {product} {default}
     bool UseNUMAInterleaving                      = false                                     {product} {default}
     bool UseNewLongLShift                         = false                                {ARCH product} {default}
     bool UseNotificationThread                    = true                                      {product} {default}
     bool UseOnStackReplacement                    = true                                   {pd product} {default}
     bool UseOnlyInlinedBimorphic                  = true                                   {C2 product} {default}
     bool UseOprofile                              = false                                     {product} {default}
     bool UseOptoBiasInlining                      = true                                   {C2 product} {default}
     bool UsePSAdaptiveSurvivorSizePolicy          = true                                      {product} {default}
     bool UseParallelGC                            = true                                      {product} {command line}
     bool UsePerfData                              = true                                      {product} {default}
     bool UsePopCountInstruction                   = true                                      {product} {default}
     bool UseProfiledLoopPredicate                 = true                                   {C2 product} {default}
     bool UseRTMDeopt                              = false                                {ARCH product} {default}
     bool UseRTMLocking                            = false                                {ARCH product} {default}
     bool UseSHA                                   = true                                      {product} {default}
     bool UseSHM                                   = false                                     {product} {default}
     intx UseSSE                                   = 4                                    {ARCH product} {default}
     bool UseSSE42Intrinsics                       = true                                 {ARCH product} {default}
     bool UseSerialGC                              = false                                     {product} {default}
     bool UseSharedSpaces                          = true                                      {product} {default}
     bool UseShenandoahGC                          = false                                     {product} {default}
     bool UseSignalChaining                        = true                                      {product} {default}
     bool UseStoreImmI16                           = false                                {ARCH product} {default}
     bool UseStringDeduplication                   = false                                     {product} {default}
     bool UseSubwordForMaxVector                   = true                                   {C2 product} {default}
     bool UseSuperWord                             = true                                   {C2 product} {default}
     bool UseSystemFloatModulo                     = false                                {ARCH product} {default}
     bool UseTLAB                                  = true                                      {product} {default}
     bool UseThreadPriorities                      = true                                   {pd product} {default}
     bool UseTransparentHugePages                  = false                                     {product} {default}
     bool UseTypeProfile                           = true                                      {product} {default}
     bool UseTypeSpeculation                       = true                                   {C2 product} {default}
     bool UseUnalignedLoadStores                   = true                                 {ARCH product} {default}
     bool UseVectorCmov                            = false                                  {C2 product} {default}
     bool UseXMMForArrayCopy                       = true                                      {product} {default}
     bool UseXMMForObjInit                         = true                                 {ARCH product} {default}
     bool UseXmmI2D                                = false                                {ARCH product} {default}
     bool UseXmmI2F                                = false                                {ARCH product} {default}
     bool UseXmmLoadAndClearUpper                  = true                                 {ARCH product} {default}
     bool UseXmmRegToRegMoveAll                    = true                                 {ARCH product} {default}
     bool UseZGC                                   = false                                     {product} {default}
     intx VMThreadPriority                         = -1                                        {product} {default}
     intx VMThreadStackSize                        = 1024                                   {pd product} {default}
     intx ValueMapInitialSize                      = 11                                     {C1 product} {default}
     intx ValueMapMaxLoopSize                      = 8                                      {C1 product} {default}
     intx ValueSearchLimit                         = 1000                                   {C2 product} {default}
     bool VerifySharedSpaces                       = false                                     {product} {default}
    uintx YoungGenerationSizeIncrement             = 20                                        {product} {default}
    uintx YoungGenerationSizeSupplement            = 80                                        {product} {default}
    uintx YoungGenerationSizeSupplementDecay       = 8                                         {product} {default}
   size_t YoungPLABSize                            = 4096                                      {product} {default}
   double ZAllocationSpikeTolerance                = 2.000000                                  {product} {default}
   double ZCollectionInterval                      = 0.000000                                  {product} {default}
   double ZFragmentationLimit                      = 25.000000                                 {product} {default}
   size_t ZMarkStackSpaceLimit                     = 8589934592                                {product} {default}
     bool ZProactive                               = true                                      {product} {default}
     bool ZUncommit                                = true                                      {product} {default}
    uintx ZUncommitDelay                           = 300                                       {product} {default}
     bool ZeroTLAB                                 = false                                     {product} {default}
[2026-01-08T16:06:41.028+0000][6.234s][info][gc] GC(0) Pause Young (Metadata GC Threshold) 353M->146M(7523M) 30.826ms
[2026-01-08T16:06:41.086+0000][6.291s][info][gc] GC(1) Pause Full (Metadata GC Threshold) 146M->144M(7523M) 57.130ms
2026-01-08T16:06:41.120096642Z main WARN The use of package scanning to locate Log4j plugins is deprecated.
Please remove the `packages` attribute from your configuration file.
See https://logging.apache.org/log4j/2.x/faq.html#package-scanning for details.
2026-01-08T16:06:41.194168424Z main WARN Some custom `Core` Log4j plugins are not properly registered:
	com.databricks.logging.log4j.ServiceLogRateLimitingFilterLog4j2
	com.databricks.logging.log4j.ServiceRewriteAppender
Please consider reporting this to the maintainers of these plugins.
See https://logging.apache.org/log4j/2.x/manual/plugins.html#plugin-registry for details.
2026-01-08T16:06:41.214834472Z main WARN RollingFileAppender 'publicFile.rolling': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:06:41.229309235Z main WARN RollingFileAppender 'privateFile.rolling': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:06:41.238542942Z main WARN RollingFileAppender 'com.databricks.UsageLogging.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:06:41.242187143Z main WARN RollingFileAppender 'com.databricks.EventLoggingStats.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:06:41.245150026Z main WARN RollingFileAppender 'com.databricks.ProductLogging.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:06:41.247688275Z main WARN RollingFileAppender 'com.databricks.LineageLogging.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:06:41.250343792Z main WARN RollingFileAppender 'com.databricks.MetricsLogging.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:06:41.252325286Z main WARN RollingFileAppender 'dltExecution.rolling': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:06:41.266193520Z main INFO Starting configuration XmlConfiguration[location=/databricks/spark/dbconf/log4j/driver/log4j2.xml, lastModified=2026-01-08T16:03:59.504Z]...
2026-01-08T16:06:41.266384697Z main INFO Start watching for changes to /databricks/spark/dbconf/log4j/driver/log4j2.xml every 0 seconds
2026-01-08T16:06:41.266625358Z main INFO Configuration XmlConfiguration[location=/databricks/spark/dbconf/log4j/driver/log4j2.xml, lastModified=2026-01-08T16:03:59.504Z] started.
2026-01-08T16:06:41.268115397Z main INFO Stopping configuration org.apache.logging.log4j.core.config.DefaultConfiguration@7e351d7...
2026-01-08T16:06:41.268494410Z main INFO Configuration org.apache.logging.log4j.core.config.DefaultConfiguration@7e351d7 stopped.
2026-01-08T16:06:41.271102307Z main WARN RollingFileAppender 'com.databricks.logging.structured.TidbPhysicalClusterStats.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.272326588Z main WARN RollingFileAppender 'com.databricks.logging.structured.HttpAccessLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.273301322Z main WARN RollingFileAppender 'com.databricks.logging.structured.ObjectStorageLibraryLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.274282919Z main WARN RollingFileAppender 'com.databricks.logging.structured.ServiceRequestLogShadowVector.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.275229191Z main WARN RollingFileAppender 'com.databricks.logging.structured.PantheonRangeQueryLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.276561306Z main WARN RollingFileAppender 'com.databricks.logging.structured.CmvOneEventDetailLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.277818479Z main WARN RollingFileAppender 'com.databricks.logging.structured.Span.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.279283267Z main WARN RollingFileAppender 'com.databricks.logging.structured.IntegrationTestLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.280539072Z main WARN RollingFileAppender 'com.databricks.logging.structured.ObjectStorageLibraryLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.281823238Z main WARN RollingFileAppender 'com.databricks.logging.structured.ServiceRequestLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.283046737Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbrActivityLogHighVolumeLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.284427237Z main WARN RollingFileAppender 'com.databricks.logging.structured.AuditLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.285605280Z main WARN RollingFileAppender 'com.databricks.logging.structured.AwsStorageInventoryManifestLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.286842708Z main WARN RollingFileAppender 'com.databricks.logging.structured.GcpStorageUsageLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.288088045Z main WARN RollingFileAppender 'com.databricks.logging.structured.AzureStorageResourceLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.289312832Z main WARN RollingFileAppender 'com.databricks.logging.structured.GcpStorageInventoryManifestLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.290464467Z main WARN RollingFileAppender 'com.databricks.logging.structured.QueryProfileLogShadowVectorLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.291678267Z main WARN RollingFileAppender 'com.databricks.logging.structured.RatelimitClientStructuredLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.292861520Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbfsClientRequestLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.294041544Z main WARN RollingFileAppender 'com.databricks.logging.structured.ServiceHealthEventLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.295167465Z main WARN RollingFileAppender 'com.databricks.logging.structured.EstoreRequestLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.296215516Z main WARN RollingFileAppender 'com.databricks.logging.structured.InboundNetworkEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.297347403Z main WARN RollingFileAppender 'com.databricks.logging.structured.AzureStorageResourceLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.298339211Z main WARN RollingFileAppender 'com.databricks.logging.structured.AlerthubServiceLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.299207011Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbrActivityLogHighVolume.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.300224270Z main WARN RollingFileAppender 'com.databricks.logging.structured.SettingsLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.301209874Z main WARN RollingFileAppender 'com.databricks.logging.structured.WorkspaceStateOperationRecordLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.302270864Z main WARN RollingFileAppender 'com.databricks.logging.structured.VectorEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.303239952Z main WARN RollingFileAppender 'com.databricks.logging.structured.GcpStorageAuditLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.304214524Z main WARN RollingFileAppender 'com.databricks.logging.structured.ProductEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.305211508Z main WARN RollingFileAppender 'com.databricks.logging.structured.CleanRoomActivityLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.306296057Z main WARN RollingFileAppender 'com.databricks.logging.structured.ApplicationMetaLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.307337617Z main WARN RollingFileAppender 'com.databricks.logging.structured.SettingsLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.308374836Z main WARN RollingFileAppender 'com.databricks.logging.structured.WorkspaceStateOperationRecordLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.309383017Z main WARN RollingFileAppender 'com.databricks.logging.structured.GcpStorageAuditLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.310414196Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbrLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.311409615Z main WARN RollingFileAppender 'com.databricks.logging.structured.GcpStorageUsageLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.312553738Z main WARN RollingFileAppender 'com.databricks.logging.structured.PantheonRangeQueryLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.313548419Z main WARN RollingFileAppender 'com.databricks.logging.structured.KubernetesEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.314550629Z main WARN RollingFileAppender 'com.databricks.logging.structured.DpHmrNonQueryEvent.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.315644301Z main WARN RollingFileAppender 'com.databricks.logging.structured.LumberjackLineageEvent.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.316559036Z main WARN RollingFileAppender 'com.databricks.logging.structured.EventplatformTimeseriesLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.317460743Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbrErrorLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.318476077Z main WARN RollingFileAppender 'com.databricks.logging.structured.QueryProfileLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.319378811Z main WARN RollingFileAppender 'com.databricks.logging.structured.EncryptionEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.320347580Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbrErrorLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.321236049Z main WARN RollingFileAppender 'com.databricks.logging.structured.SparkConnectRpcMetricsLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.322110779Z main WARN RollingFileAppender 'com.databricks.logging.structured.BrickindexUsageLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.322849622Z main WARN RollingFileAppender 'com.databricks.logging.structured.DatabaseClientQueryLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.323528394Z main WARN RollingFileAppender 'com.databricks.logging.structured.VectorEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.324232204Z main WARN RollingFileAppender 'com.databricks.logging.structured.RatelimitClientStructuredLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.324935865Z main WARN RollingFileAppender 'com.databricks.logging.structured.InboundNetworkEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.325621422Z main WARN RollingFileAppender 'com.databricks.logging.structured.ClassicDpMetricsLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.326314587Z main WARN RollingFileAppender 'com.databricks.logging.structured.AwsStorageAccessLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.327011368Z main WARN RollingFileAppender 'com.databricks.logging.structured.WafTransactionLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.327674923Z main WARN RollingFileAppender 'com.databricks.logging.structured.BouncerClientLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.328315284Z main WARN RollingFileAppender 'com.databricks.logging.structured.AzureStorageInventoryManifest.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.328926480Z main WARN RollingFileAppender 'com.databricks.logging.structured.MoneySettingsLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.329606853Z main WARN RollingFileAppender 'com.databricks.logging.structured.MetricToLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.330427165Z main WARN RollingFileAppender 'com.databricks.logging.structured.ClassicDpMetricsLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.331338132Z main WARN RollingFileAppender 'com.databricks.logging.structured.SparkConnectRpcMetricsLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.332290259Z main WARN RollingFileAppender 'com.databricks.logging.structured.RequestActivityLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.333172186Z main WARN RollingFileAppender 'com.databricks.logging.structured.FrontendLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.334056439Z main WARN RollingFileAppender 'com.databricks.logging.structured.KubernetesAuditLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.334971672Z main WARN RollingFileAppender 'com.databricks.logging.structured.KubernetesObjectLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.335839248Z main WARN RollingFileAppender 'com.databricks.logging.structured.FrontendLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.336754913Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbfsClientRequestLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.337764558Z main WARN RollingFileAppender 'com.databricks.logging.structured.DroppedMetricLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.338706326Z main WARN RollingFileAppender 'com.databricks.logging.structured.CleanRoomActivityLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.339540602Z main WARN RollingFileAppender 'com.databricks.logging.structured.EstoreRequestLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.340433138Z main WARN RollingFileAppender 'com.databricks.logging.structured.CachingTeamLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.341280015Z main WARN RollingFileAppender 'com.databricks.logging.structured.CachingTeamLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.342009416Z main WARN RollingFileAppender 'com.databricks.logging.structured.PrometheusMetricsSnapshotLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.343862775Z main WARN RollingFileAppender 'com.databricks.logging.structured.CleanRoomBillingLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.344534892Z main WARN RollingFileAppender 'com.databricks.logging.structured.EncryptionEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.345218543Z main WARN RollingFileAppender 'com.databricks.logging.structured.DetailedReplicationPlanEvent.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.345902364Z main WARN RollingFileAppender 'com.databricks.logging.structured.AuditLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.346522087Z main WARN RollingFileAppender 'com.databricks.logging.structured.AwsStorageInventoryManifest.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.347116419Z main WARN RollingFileAppender 'com.databricks.logging.structured.MetricToLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.347727784Z main WARN RollingFileAppender 'com.databricks.logging.structured.AppComputeUsageLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.348385319Z main WARN RollingFileAppender 'com.databricks.logging.structured.SpanLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.349050712Z main WARN RollingFileAppender 'com.databricks.logging.structured.AppComputeUsageLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.349833248Z main WARN RollingFileAppender 'com.databricks.logging.structured.DatabaseClientQueryLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.350577048Z main WARN RollingFileAppender 'com.databricks.logging.structured.PipelineEventLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.351287507Z main WARN RollingFileAppender 'com.databricks.logging.structured.TidbPhysicalClusterStatsLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.352184003Z main WARN RollingFileAppender 'com.databricks.logging.structured.ServiceRequestLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.353022678Z main WARN RollingFileAppender 'com.databricks.logging.structured.AwsStorageAccessLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.353905972Z main WARN RollingFileAppender 'com.databricks.logging.structured.NimbusEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.354806241Z main WARN RollingFileAppender 'com.databricks.logging.structured.IntegrationTestLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.355706164Z main WARN RollingFileAppender 'com.databricks.logging.structured.AiProductsEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.356574579Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbrActivityLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.357425201Z main WARN RollingFileAppender 'com.databricks.logging.structured.RequestActivityLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.358321109Z main WARN RollingFileAppender 'com.databricks.logging.structured.HttpAccessLogShadowAgent.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.359229469Z main WARN RollingFileAppender 'com.databricks.logging.structured.CleanRoomBillingLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.360034259Z main WARN RollingFileAppender 'com.databricks.logging.structured.DpHmrNonQueryEventLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.360960973Z main WARN RollingFileAppender 'com.databricks.logging.structured.PantheonInstantQueryLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.361934025Z main WARN RollingFileAppender 'com.databricks.logging.structured.PipelineEvent.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.362884123Z main WARN RollingFileAppender 'com.databricks.logging.structured.ReplicationPlanEventLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.363720558Z main WARN RollingFileAppender 'com.databricks.logging.structured.ReplicationPlanEvent.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.364556879Z main WARN RollingFileAppender 'com.databricks.logging.structured.CmvOneEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.365448724Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbrLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.366286897Z main WARN RollingFileAppender 'com.databricks.logging.structured.NimbusDomainPoolSnapshotLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.367113370Z main WARN RollingFileAppender 'com.databricks.logging.structured.CmvOneEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.367902955Z main WARN RollingFileAppender 'com.databricks.logging.structured.ApplicationMetaLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.368696863Z main WARN RollingFileAppender 'com.databricks.logging.structured.WafTransactionLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.369585116Z main WARN RollingFileAppender 'com.databricks.logging.structured.DbrActivityLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.370354132Z main WARN RollingFileAppender 'com.databricks.logging.structured.ChangeEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.371117639Z main WARN RollingFileAppender 'com.databricks.logging.structured.HttpAccessLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.371879836Z main WARN RollingFileAppender 'com.databricks.logging.structured.ServiceHealthEvent.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.372693320Z main WARN RollingFileAppender 'com.databricks.logging.structured.ServiceRequestLogShadowVectorLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.373448215Z main WARN RollingFileAppender 'com.databricks.logging.structured.PantheonInstantQueryLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.374235413Z main WARN RollingFileAppender 'com.databricks.logging.structured.KubernetesObjectLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.375079616Z main WARN RollingFileAppender 'com.databricks.logging.structured.ServerlessSlotBasedBillingLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.375956200Z main WARN RollingFileAppender 'com.databricks.logging.structured.PrometheusMetricsSnapshot.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.376808125Z main WARN RollingFileAppender 'com.databricks.logging.structured.QueryProfileLogShadowVector.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.377739766Z main WARN RollingFileAppender 'com.databricks.logging.structured.QueryProfileLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.378478411Z main WARN RollingFileAppender 'com.databricks.logging.structured.NimbusDomainPoolSnapshotLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.379305142Z main WARN RollingFileAppender 'com.databricks.logging.structured.ShadowedServiceRequestLogPipelinedExecution.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.380137815Z main WARN RollingFileAppender 'com.databricks.logging.structured.ProductEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.381000313Z main WARN RollingFileAppender 'com.databricks.logging.structured.NimbusEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.381772581Z main WARN RollingFileAppender 'com.databricks.logging.structured.AppStateLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.382583909Z main WARN RollingFileAppender 'com.databricks.logging.structured.AiProductsEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.383367148Z main WARN RollingFileAppender 'com.databricks.logging.structured.BouncerClientLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.384108603Z main WARN RollingFileAppender 'com.databricks.logging.structured.AzureStorageInventoryManifestLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.384856619Z main WARN RollingFileAppender 'com.databricks.logging.structured.ShadowedServiceRequestLogPipelinedExecutionLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.385617067Z main WARN RollingFileAppender 'com.databricks.logging.structured.DetailedReplicationPlanEventLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.386360455Z main WARN RollingFileAppender 'com.databricks.logging.structured.AlerthubServiceLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.387135078Z main WARN RollingFileAppender 'com.databricks.logging.structured.ChangeEventLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.387855159Z main WARN RollingFileAppender 'com.databricks.logging.structured.KubernetesAuditLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.388586817Z main WARN RollingFileAppender 'com.databricks.logging.structured.MoneySettingsLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.389306015Z main WARN RollingFileAppender 'com.databricks.logging.structured.GcpStorageInventoryManifest.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.390027033Z main WARN RollingFileAppender 'com.databricks.logging.structured.IngestionEventLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.390748640Z main WARN RollingFileAppender 'com.databricks.logging.structured.AppStateLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.391466287Z main WARN RollingFileAppender 'com.databricks.logging.structured.BackgroundActivityLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.392321912Z main WARN RollingFileAppender 'com.databricks.logging.structured.BrickindexUsageLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.393120506Z main WARN RollingFileAppender 'com.databricks.logging.structured.DroppedMetricLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.393939388Z main WARN RollingFileAppender 'com.databricks.logging.structured.KubernetesEventLogLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.397518249Z main WARN RollingFileAppender 'com.databricks.logging.structured.CmvOneEventDetail.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.398076571Z main WARN RollingFileAppender 'com.databricks.logging.structured.HttpAccessLogShadowAgentLaMigration.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:41.398635917Z main WARN RollingFileAppender 'com.databricks.logging.structured.BackgroundActivityLog.appender': The bufferSize is set to 128000 but bufferedIO is not true
[2026-01-08T16:06:42.752+0000][7.957s][info][gc] GC(2) Pause Young (Metadata GC Threshold) 340M->149M(7523M) 7.374ms
[2026-01-08T16:06:42.832+0000][8.037s][info][gc] GC(3) Pause Full (Metadata GC Threshold) 149M->148M(7523M) 79.663ms
[2026-01-08T16:06:44.642+0000][9.847s][info][gc] GC(4) Pause Young (Metadata GC Threshold) 384M->164M(7523M) 8.929ms
[2026-01-08T16:06:44.715+0000][9.920s][info][gc] GC(5) Pause Full (Metadata GC Threshold) 164M->164M(7523M) 72.938ms
2026-01-08T16:06:44.890218866Z UptimeLogger:driver WARN RollingFileAppender 'com.databricks.UsageLogging.appender': The bufferSize is set to 128000 but bufferedIO is not true
2026-01-08T16:06:47.503658953Z main WARN RollingFileAppender 'spark_usage': The bufferSize is set to 128000 but bufferedIO is not true
[2026-01-08T16:06:47.933+0000][13.138s][info][gc] GC(6) Pause Young (Metadata GC Threshold) 750M->190M(7523M) 25.564ms
[2026-01-08T16:06:48.042+0000][13.247s][info][gc] GC(7) Pause Full (Metadata GC Threshold) 190M->182M(7523M) 109.042ms
[2026-01-08T16:06:56.618+0000][21.823s][info][gc] GC(8) Pause Young (Metadata GC Threshold) 1935M->492M(7523M) 209.913ms
[2026-01-08T16:06:57.336+0000][22.541s][info][gc] GC(9) Pause Full (Metadata GC Threshold) 492M->489M(7523M) 717.695ms
[2026-01-08T16:07:10.920+0000][36.125s][info][gc] GC(10) Pause Young (Metadata GC Threshold) 2158M->524M(7560M) 35.302ms
[2026-01-08T16:07:11.078+0000][36.283s][info][gc] GC(11) Pause Full (Metadata GC Threshold) 524M->241M(7560M) 157.285ms
Thu Jan 8 16:07:33 UTC 2026 Starting R processing from BASH
Thu Jan 8 16:07:33 UTC 2026 R script: /local_disk0/tmp/_rServeScript.r18419261329944250478resource.r
Thu Jan 8 16:07:33 UTC 2026 Port number: 1100
Thu Jan 8 16:07:33 UTC 2026 cgroup: None
2026-01-08 16:07:34.125665 R process started with pid 1189 
-- running Rserve in this R session (pid=1189), 1 server(s) --
(This session will block until Rserve is shut down)
Spark package found in SPARK_HOME: /databricks/spark
DATABRICKS_STDOUT_END-68b49d29-5abe-4619-9fdc-bf50cb9ff785-1767888456089
:: loading settings :: url = jar:file:/databricks/jars/----ws_4_0--mvn--hadoop3--org.apache.ivy--ivy--org.apache.ivy__ivy__2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: resolving dependencies :: com.databricks#dbc-parent;1.0
	confs: [default]
	found org.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3 in preferred-maven-central-mirror
	found org.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3 in preferred-maven-central-mirror
	found org.neo4j#caniuse-core;1.3.0 in preferred-maven-central-mirror
	found org.neo4j#caniuse-api;1.3.0 in preferred-maven-central-mirror
	found org.jetbrains.kotlin#kotlin-stdlib;2.1.20 in preferred-maven-central-mirror
	found org.jetbrains#annotations;13.0 in preferred-maven-central-mirror
	found org.neo4j#caniuse-neo4j-detection;1.3.0 in preferred-maven-central-mirror
	found org.neo4j.driver#neo4j-java-driver-slim;4.4.21 in preferred-maven-central-mirror
	found org.reactivestreams#reactive-streams;1.0.4 in preferred-maven-central-mirror
	found io.netty#netty-handler;4.1.127.Final in preferred-maven-central-mirror
	found io.netty#netty-common;4.1.127.Final in preferred-maven-central-mirror
	found io.netty#netty-resolver;4.1.127.Final in preferred-maven-central-mirror
	found io.netty#netty-buffer;4.1.127.Final in preferred-maven-central-mirror
	found io.netty#netty-transport;4.1.127.Final in preferred-maven-central-mirror
	found io.netty#netty-transport-native-unix-common;4.1.127.Final in preferred-maven-central-mirror
	found io.netty#netty-codec;4.1.127.Final in preferred-maven-central-mirror
	found io.netty#netty-tcnative-classes;2.0.73.Final in preferred-maven-central-mirror
	found io.projectreactor#reactor-core;3.6.11 in preferred-maven-central-mirror
	found org.neo4j#neo4j-cypher-dsl;2022.11.0 in preferred-maven-central-mirror
	found org.apiguardian#apiguardian-api;1.1.2 in preferred-maven-central-mirror
	found org.neo4j.connectors#commons-authn-spi;1.0.0-rc2 in preferred-maven-central-mirror
	found org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 in preferred-maven-central-mirror
	found org.slf4j#slf4j-api;2.0.17 in preferred-maven-central-mirror
	found org.neo4j.connectors#commons-authn-provided;1.0.0-rc2 in preferred-maven-central-mirror
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/neo4j-connector-apache-spark_2.13/5.3.10_for_spark_3/neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar ...
	[SUCCESSFUL ] org.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3!neo4j-connector-apache-spark_2.13.jar (205ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/neo4j-connector-apache-spark_2.13_common/5.3.10_for_spark_3/neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar ...
	[SUCCESSFUL ] org.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3!neo4j-connector-apache-spark_2.13_common.jar (50ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/driver/neo4j-java-driver-slim/4.4.21/neo4j-java-driver-slim-4.4.21.jar ...
	[SUCCESSFUL ] org.neo4j.driver#neo4j-java-driver-slim;4.4.21!neo4j-java-driver-slim.jar (48ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/caniuse-core/1.3.0/caniuse-core-1.3.0.jar ...
	[SUCCESSFUL ] org.neo4j#caniuse-core;1.3.0!caniuse-core.jar (24ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/caniuse-neo4j-detection/1.3.0/caniuse-neo4j-detection-1.3.0.jar ...
	[SUCCESSFUL ] org.neo4j#caniuse-neo4j-detection;1.3.0!caniuse-neo4j-detection.jar (19ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/neo4j-cypher-dsl/2022.11.0/neo4j-cypher-dsl-2022.11.0.jar ...
	[SUCCESSFUL ] org.neo4j#neo4j-cypher-dsl;2022.11.0!neo4j-cypher-dsl.jar (22ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/connectors/commons-authn-spi/1.0.0-rc2/commons-authn-spi-1.0.0-rc2.jar ...
	[SUCCESSFUL ] org.neo4j.connectors#commons-authn-spi;1.0.0-rc2!commons-authn-spi.jar (52ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/connectors/commons-reauth-driver/1.0.0-rc2/commons-reauth-driver-1.0.0-rc2.jar ...
	[SUCCESSFUL ] org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2!commons-reauth-driver.jar (22ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/caniuse-api/1.3.0/caniuse-api-1.3.0.jar ...
	[SUCCESSFUL ] org.neo4j#caniuse-api;1.3.0!caniuse-api.jar (26ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/jetbrains/kotlin/kotlin-stdlib/2.1.20/kotlin-stdlib-2.1.20.jar ...
	[SUCCESSFUL ] org.jetbrains.kotlin#kotlin-stdlib;2.1.20!kotlin-stdlib.jar (45ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/jetbrains/annotations/13.0/annotations-13.0.jar ...
	[SUCCESSFUL ] org.jetbrains#annotations;13.0!annotations.jar (5ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar ...
	[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.4!reactive-streams.jar (5ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/netty/netty-handler/4.1.127.Final/netty-handler-4.1.127.Final.jar ...
	[SUCCESSFUL ] io.netty#netty-handler;4.1.127.Final!netty-handler.jar (27ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/netty/netty-tcnative-classes/2.0.73.Final/netty-tcnative-classes-2.0.73.Final.jar ...
	[SUCCESSFUL ] io.netty#netty-tcnative-classes;2.0.73.Final!netty-tcnative-classes.jar (27ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/projectreactor/reactor-core/3.6.11/reactor-core-3.6.11.jar ...
	[SUCCESSFUL ] io.projectreactor#reactor-core;3.6.11!reactor-core.jar (60ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/netty/netty-common/4.1.127.Final/netty-common-4.1.127.Final.jar ...
	[SUCCESSFUL ] io.netty#netty-common;4.1.127.Final!netty-common.jar (29ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/netty/netty-resolver/4.1.127.Final/netty-resolver-4.1.127.Final.jar ...
	[SUCCESSFUL ] io.netty#netty-resolver;4.1.127.Final!netty-resolver.jar (19ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/netty/netty-buffer/4.1.127.Final/netty-buffer-4.1.127.Final.jar ...
	[SUCCESSFUL ] io.netty#netty-buffer;4.1.127.Final!netty-buffer.jar (31ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport/4.1.127.Final/netty-transport-4.1.127.Final.jar ...
	[SUCCESSFUL ] io.netty#netty-transport;4.1.127.Final!netty-transport.jar (26ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-native-unix-common/4.1.127.Final/netty-transport-native-unix-common-4.1.127.Final.jar ...
	[SUCCESSFUL ] io.netty#netty-transport-native-unix-common;4.1.127.Final!netty-transport-native-unix-common.jar (22ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/io/netty/netty-codec/4.1.127.Final/netty-codec-4.1.127.Final.jar ...
	[SUCCESSFUL ] io.netty#netty-codec;4.1.127.Final!netty-codec.jar (27ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar ...
	[SUCCESSFUL ] org.apiguardian#apiguardian-api;1.1.2!apiguardian-api.jar (5ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/slf4j/slf4j-api/2.0.17/slf4j-api-2.0.17.jar ...
	[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.17!slf4j-api.jar (6ms)
downloading https://maven-central.storage-download.googleapis.com/maven2/org/neo4j/connectors/commons-authn-provided/1.0.0-rc2/commons-authn-provided-1.0.0-rc2.jar ...
	[SUCCESSFUL ] org.neo4j.connectors#commons-authn-provided;1.0.0-rc2!commons-authn-provided.jar (40ms)
:: resolution report :: resolve 10273ms :: artifacts dl 856ms
	:: modules in use:
	io.netty#netty-buffer;4.1.127.Final from preferred-maven-central-mirror in [default]
	io.netty#netty-codec;4.1.127.Final from preferred-maven-central-mirror in [default]
	io.netty#netty-common;4.1.127.Final from preferred-maven-central-mirror in [default]
	io.netty#netty-handler;4.1.127.Final from preferred-maven-central-mirror in [default]
	io.netty#netty-resolver;4.1.127.Final from preferred-maven-central-mirror in [default]
	io.netty#netty-tcnative-classes;2.0.73.Final from preferred-maven-central-mirror in [default]
	io.netty#netty-transport;4.1.127.Final from preferred-maven-central-mirror in [default]
	io.netty#netty-transport-native-unix-common;4.1.127.Final from preferred-maven-central-mirror in [default]
	io.projectreactor#reactor-core;3.6.11 from preferred-maven-central-mirror in [default]
	org.apiguardian#apiguardian-api;1.1.2 from preferred-maven-central-mirror in [default]
	org.jetbrains#annotations;13.0 from preferred-maven-central-mirror in [default]
	org.jetbrains.kotlin#kotlin-stdlib;2.1.20 from preferred-maven-central-mirror in [default]
	org.neo4j#caniuse-api;1.3.0 from preferred-maven-central-mirror in [default]
	org.neo4j#caniuse-core;1.3.0 from preferred-maven-central-mirror in [default]
	org.neo4j#caniuse-neo4j-detection;1.3.0 from preferred-maven-central-mirror in [default]
	org.neo4j#neo4j-connector-apache-spark_2.13;5.3.10_for_spark_3 from preferred-maven-central-mirror in [default]
	org.neo4j#neo4j-connector-apache-spark_2.13_common;5.3.10_for_spark_3 from preferred-maven-central-mirror in [default]
	org.neo4j#neo4j-cypher-dsl;2022.11.0 from preferred-maven-central-mirror in [default]
	org.neo4j.connectors#commons-authn-provided;1.0.0-rc2 from preferred-maven-central-mirror in [default]
	org.neo4j.connectors#commons-authn-spi;1.0.0-rc2 from preferred-maven-central-mirror in [default]
	org.neo4j.connectors#commons-reauth-driver;1.0.0-rc2 from preferred-maven-central-mirror in [default]
	org.neo4j.driver#neo4j-java-driver-slim;4.4.21 from preferred-maven-central-mirror in [default]
	org.reactivestreams#reactive-streams;1.0.4 from preferred-maven-central-mirror in [default]
	org.slf4j#slf4j-api;2.0.17 from preferred-maven-central-mirror in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   24  |   24  |   24  |   0   ||   24  |   24  |
	---------------------------------------------------------------------
:: retrieving :: com.databricks#dbc-parent
	confs: [default]
	24 artifacts copied, 0 already retrieved (15383kB/8ms)
[2026-01-08T16:07:54.530+0000][79.735s][info][gc] GC(12) Pause Young (Allocation Failure) 2261M->285M(7543M) 33.126ms
2026-01-08T16:12:19.431752043Z metastoreChecker WARN The use of package scanning to locate Log4j plugins is deprecated.
Please remove the `packages` attribute from your configuration file.
See https://logging.apache.org/log4j/2.x/faq.html#package-scanning for details.
2026-01-08T16:12:19.432260873Z metastoreChecker WARN Some custom `Core` Log4j plugins are not properly registered:
	com.databricks.logging.log4j.ServiceLogRateLimitingFilterLog4j2
	com.databricks.logging.log4j.ServiceRewriteAppender
Please consider reporting this to the maintainers of these plugins.
See https://logging.apache.org/log4j/2.x/manual/plugins.html#plugin-registry for details.
2026-01-08T16:12:19.437419092Z metastoreChecker WARN RollingFileAppender 'publicFile.rolling': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:12:19.438903773Z metastoreChecker WARN RollingFileAppender 'privateFile.rolling': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:12:19.440824702Z metastoreChecker WARN RollingFileAppender 'com.databricks.UsageLogging.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:12:19.442207857Z metastoreChecker WARN RollingFileAppender 'com.databricks.EventLoggingStats.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:12:19.443468301Z metastoreChecker WARN RollingFileAppender 'com.databricks.ProductLogging.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:12:19.444802019Z metastoreChecker WARN RollingFileAppender 'com.databricks.LineageLogging.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:12:19.446042314Z metastoreChecker WARN RollingFileAppender 'com.databricks.MetricsLogging.appender': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:12:19.447752591Z metastoreChecker WARN RollingFileAppender 'dltExecution.rolling': The bufferSize is set to 8192 but bufferedIO is not true
2026-01-08T16:12:19.451450234Z metastoreChecker INFO Starting configuration XmlConfiguration[location=/databricks/spark/dbconf/log4j/driver/log4j2.xml, lastModified=2026-01-08T16:03:59.504Z]...
2026-01-08T16:12:19.451874964Z metastoreChecker INFO Start watching for changes to /databricks/spark/dbconf/log4j/driver/log4j2.xml every 0 seconds
2026-01-08T16:12:19.452065648Z metastoreChecker INFO Configuration XmlConfiguration[location=/databricks/spark/dbconf/log4j/driver/log4j2.xml, lastModified=2026-01-08T16:03:59.504Z] started.
2026-01-08T16:12:19.452208547Z metastoreChecker INFO Stopping configuration org.apache.logging.log4j.core.config.DefaultConfiguration@7a58a728...
2026-01-08T16:12:19.452411810Z metastoreChecker INFO Configuration org.apache.logging.log4j.core.config.DefaultConfiguration@7a58a728 stopped.
[2026-01-08T16:15:23.411+0000][528.616s][info][gc] GC(13) Pause Young (Allocation Failure) 2305M->369M(7581M) 59.568ms
NOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.

To exit, you will have to explicitly quit this process, by either sending
"quit" from a client, or using Ctrl-\ in UNIX-like environments.

To read more about this, see https://github.com/ipython/ipython/issues/2049


To connect another client to this kernel, use:
    --existing /databricks/kernel-connections/aef1ecd2c1e6320143ffecc0926a07b67af0f5973eb52c62f54ef9d596339b57.json
Configuration loaded from Databricks Secrets:
  Secret Scope: neo4j-uc-creds
  Neo4j Host: 3f1f827a.databases.neo4j.io
  Bolt URI: neo4j+s://3f1f827a.databases.neo4j.io
  JDBC URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j
  Connection Name: neo4j_connection
  JAR Path: /Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar
============================================================
ENVIRONMENT INFORMATION
============================================================

Spark Version: 4.0.0
Databricks Runtime: 17.3.x-cpu-ml-scala2.13
Python Version: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]
Neo4j Python Driver: 6.0.2

JDBC JAR Path: /Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar
JAR File Exists: True
============================================================
TEST: Network Connectivity (TCP)
============================================================
2026-01-08T16:16:45.390936356Z Thread-109 WARN RollingFileAppender 'com.databricks.LineageLogging.appender': The bufferSize is set to 128000 but bufferedIO is not true
[2026-01-08T16:16:46.132+0000][611.337s][info][gc] GC(14) Pause Young (Metadata GC Threshold) 1432M->382M(7578M) 63.889ms
[2026-01-08T16:16:46.529+0000][611.734s][info][gc] GC(15) Pause Full (Metadata GC Threshold) 382M->373M(7578M) 396.796ms

Result: FAILURE (return_code=1) | Cannot reach 3f1f827a.databases.neo4j.io:7687 - check firewall rules | Details: 3f1f827a.databases.neo4j.io: forward host lookup failed: Unknown host

Status: FAIL
============================================================
TEST: Neo4j Python Driver
============================================================

[FAIL] Connection failed: Failed to DNS resolve address 3f1f827a.databases.neo4j.io:7687: [Errno -2] Name or service not known

Status: FAIL
============================================================
TEST: Neo4j Spark Connector (org.neo4j.spark.DataSource)
============================================================

[FAIL] Spark Connector failed: An error occurred while calling o519.load.
: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io: Name or service not known
	at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)
	at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:89)
	at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
	at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
	at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
	at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
	at scala.Option.flatMap(Option.scala:283)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause
		at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
		at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:934)
		at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)
		at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)
		at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
		at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
		at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
		at org.neo4j.driver.internal.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:35)
		at org.neo4j.driver.internal.cluster.RediscoveryImpl.resolveAllByDomainName(RediscoveryImpl.java:380)
		at org.neo4j.driver.internal.cluster.RediscoveryImpl.resolve(RediscoveryImpl.java:337)
		at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.supportsMultiDb(LoadBalancer.java:176)
		at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.verifyConnectivity(LoadBalancer.java:150)
		at org.neo4j.driver.internal.SessionFactoryImpl.verifyConnectivity(SessionFactoryImpl.java:74)
		at org.neo4j.driver.internal.InternalDriver.verifyConnectivityAsync(InternalDriver.java:129)
		at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
		at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
		at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
		at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
		at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
		at scala.Option.flatMap(Option.scala:283)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)
		at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:89)
		at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
		at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
		at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
		at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
		at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
		at scala.Option.flatMap(Option.scala:283)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Direct JDBC - dbtable option (reads label as table)
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[FAIL] Direct JDBC dbtable failed: An error occurred while calling o529.load.
: java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more


Status: FAIL

Note: Ensure the label exists in Neo4j and JAR is installed as cluster library.
Also verify customSchema column names match your Neo4j node properties.
============================================================
TEST: Direct JDBC - SQL Translation
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[FAIL] Direct JDBC with SQL translation failed: An error occurred while calling o545.load.
: java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more


Status: FAIL
============================================================
TEST: Direct JDBC - SQL Aggregate (COUNT)
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[FAIL] Direct JDBC aggregate query failed: An error occurred while calling o561.load.
: java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more


Status: FAIL

Note: Ensure 'Flight' label exists in your Neo4j database, or change to a label that exists.
============================================================
TEST: Direct JDBC - SQL JOIN Translation
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[FAIL] Direct JDBC JOIN translation failed: An error occurred while calling o577.load.
: java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more


Status: FAIL

Note: Requires Flight-[:DEPARTS_FROM]->Airport pattern in Neo4j.
Adjust labels/relationship types to match your graph model.
============================================================
SETUP: Create Unity Catalog JDBC Connection
============================================================
Dropped existing connection (if any): neo4j_connection

[PASS] Connection created: neo4j_connection
============================================================
VERIFY: Connection Configuration
============================================================

Connection details:
+--------------------+--------------------------------------------------------------------------------------------------------------------+
|info_name           |info_value                                                                                                          |
+--------------------+--------------------------------------------------------------------------------------------------------------------+
|Connection Name     |neo4j_connection                                                                                                    |
|Type                |JDBC                                                                                                                |
|Owner               |ryan.knight@neo4j.com                                                                                               |
|Read-only           |true                                                                                                                |
|Options             |externalOptionsAllowList -> dbtable,query,partitionColumn,lowerBound,upperBound,numPartitions,fetchSize,customSchema|
|Environment Settings|Dependencies: "/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar"                    |
+--------------------+--------------------------------------------------------------------------------------------------------------------+

============================================================
TEST: Unity Catalog - Spark DataFrame API
============================================================

[FAIL] Unity Catalog Spark DataFrame API failed:

Error: An error occurred while calling o599.load.
: java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Unity Catalog - Native Cypher (FORCE_CYPHER)
============================================================

[FAIL] Unity Catalog with FORCE_CYPHER failed:

Error: An error occurred while calling o606.load.
: java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Unity Catalog - remote_query() Function
============================================================

[FAIL] Unity Catalog remote_query() failed:

Error: An error occurred while calling o441.sql.
: java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Unity Catalog - SQL Aggregate with Custom Schema
============================================================

[FAIL] Unity Catalog SQL aggregate query failed:

Error: An error occurred while calling o617.load.
: java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL

Note: Ensure 'Flight' label exists in your Neo4j database.
Adjust the label name to match your graph model if needed.
======================================================================
PHASE 1: JDBC DatabaseMetaData Schema Discovery
======================================================================

[FAIL] Schema discovery failed: An error occurred while calling o627.load.
: java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more

======================================================================
IN-MEMORY SCHEMA MODEL
======================================================================

[WARN] No schema discovered. Run the schema discovery cell first.
======================================================================
OPTION A: Views with Inferred Schema
======================================================================

[INFO] Creating view for label: Aircraft
[INFO] View name: neo4j_view_aircraft

[INFO] Approach: Create view via DataFrame registration

[FAIL] Option A failed: An error occurred while calling o639.load.
: java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

======================================================================
OPTION B: Tables with Explicit Schema
======================================================================

[INFO] Creating table for label: Airport
[INFO] Table name: neo4j_table_airport

[WARN] Could not build schema for Airport, using fallback

[FAIL] Option B failed: An error occurred while calling o646.load.
: java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

======================================================================
OPTION C: Hybrid Approach with Schema Registry
======================================================================

[INFO] Creating schema registry: neo4j_schema_registry
[INFO] Creating view for label: Flight

[WARN] No schema data to populate registry

[FAIL] Option C failed: An error occurred while calling o652.load.
: java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

======================================================================
VERIFICATION AND COMPARISON
======================================================================

======================================================================
APPROACH COMPARISON SUMMARY
======================================================================
+----------------------------------+------+---------+-------------+------+
|Approach                          |Object|Row Count|Schema Source|Status|
+----------------------------------+------+---------+-------------+------+
|Option A: Inferred Schema View    |N/A   |0        |N/A          |FAILED|
|Option B: Explicit Schema Table   |N/A   |0        |N/A          |FAILED|
|Option C: Hybrid (Registry + View)|N/A   |0        |N/A          |FAILED|
+----------------------------------+------+---------+-------------+------+


----------------------------------------------------------------------
PROS AND CONS
----------------------------------------------------------------------

OPTION A (Inferred Schema Views):
  Pros:
    - Simplest to implement
    - Always reflects current Neo4j schema
    - No schema maintenance needed
  Cons:
    - Schema inference may fail for some types
    - Less control over column types
    - Schema not visible until query time

OPTION B (Explicit Schema Tables):
  Pros:
    - Full control over column types
    - Predictable schema
    - Avoids inference issues
  Cons:
    - Must regenerate when Neo4j schema changes
    - Requires schema discovery step
    - More code to maintain

OPTION C (Hybrid with Registry):
  Pros:
    - Schema metadata is queryable
    - Good for documentation/discovery
    - Views provide live data access
    - Best of both worlds
  Cons:
    - Most complex to implement
    - Registry needs refresh mechanism
    - Two objects to manage per label


[INFO] All approaches use the UC JDBC connection for governance
[INFO] Choose based on your needs: simplicity (A), control (B), or visibility (C)
======================================================================
OPTIONAL: Relationship Traversal View Test
======================================================================

[INFO] No relationship patterns discovered to test
[INFO] Skipping relationship traversal test
======================================================================
CLEANUP
======================================================================

Temporary objects created during this demo:

Views/Tables:

  - neo4j_view_aircraft (Option A view)
  - neo4j_table_airport (Option B table)
  - neo4j_schema_registry (Option C registry)
  - neo4j_hybrid_flight (Option C view)

To clean up, uncomment and run:

# spark.sql('DROP VIEW IF EXISTS neo4j_view_aircraft')
# spark.sql('DROP VIEW IF EXISTS neo4j_table_airport')
# spark.sql('DROP VIEW IF EXISTS neo4j_schema_registry')
# spark.sql('DROP VIEW IF EXISTS neo4j_hybrid_flight')

Note: These are temporary views that will be automatically dropped 
when the Spark session ends. Uncomment above to drop manually.

Configuration loaded from Databricks Secrets:
  Secret Scope: neo4j-uc-creds
  Neo4j Host: 3f1f827a.databases.neo4j.io
  Bolt URI: neo4j+s://3f1f827a.databases.neo4j.io
  JDBC URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j
  Connection Name: neo4j_connection
  JAR Path: /Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar
============================================================
ENVIRONMENT INFORMATION
============================================================

Spark Version: 4.0.0
Databricks Runtime: 17.3.x-cpu-ml-scala2.13
Python Version: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]
Neo4j Python Driver: 6.0.2

JDBC JAR Path: /Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar
JAR File Exists: True
============================================================
TEST: Network Connectivity (TCP)
============================================================

Result: SUCCESS (return_code=0) | Network connectivity to 3f1f827a.databases.neo4j.io:7687 is OPEN | Details: Warning: inverse host lookup failed for 20.124.3.249: Unknown host
3f1f827a.databases.neo4j.io [20.124.3.249] 7687 (?) open

Status: PASS
============================================================
TEST: Neo4j Python Driver
============================================================

[FAIL] Connection failed: Unable to retrieve routing information

Status: FAIL
============================================================
TEST: Neo4j Spark Connector (org.neo4j.spark.DataSource)
============================================================
[2026-01-08T16:21:36.206+0000][901.411s][info][gc] GC(16) Pause Young (Allocation Failure) 2449M->410M(7587M) 36.544ms

[FAIL] Spark Connector failed: An error occurred while calling o720.load.
: org.neo4j.driver.exceptions.ServiceUnavailableException: Unable to connect to database management service, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)
	at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:89)
	at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
	at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
	at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
	at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
	at scala.Option.flatMap(Option.scala:283)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause
		at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.lambda$verifyConnectivity$3(LoadBalancer.java:156)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.driver.internal.util.Futures.lambda$asCompletionStage$0(Futures.java:75)
		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
		at io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:35)
		at io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:503)
		at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)
		at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:89)
		at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
		at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
		at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
		at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
		at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
		at scala.Option.flatMap(Option.scala:283)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.driver.exceptions.ServiceUnavailableException: Failed to perform multi-databases feature detection with the following servers: [3f1f827a.databases.neo4j.io(20.124.3.249):7687]
	at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.supportsMultiDb(LoadBalancer.java:181)
	at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.verifyConnectivity(LoadBalancer.java:150)
	at org.neo4j.driver.internal.SessionFactoryImpl.verifyConnectivity(SessionFactoryImpl.java:74)
	at org.neo4j.driver.internal.InternalDriver.verifyConnectivityAsync(InternalDriver.java:129)
	at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
	at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
	at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
	at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
	at scala.Option.flatMap(Option.scala:283)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.neo4j.driver.exceptions.ServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
		at org.neo4j.driver.internal.util.ErrorUtil.newConnectionTerminatedError(ErrorUtil.java:48)
		at org.neo4j.driver.internal.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:76)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:412)
		at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:377)
		at io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1192)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
		at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
		at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
		at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		... 1 more


Status: FAIL
============================================================
TEST: Direct JDBC - dbtable option (reads label as table)
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[FAIL] Direct JDBC dbtable failed: An error occurred while calling o733.load.
: java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.newConnectionTerminatedError(HandshakeHandler.java:126)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:120)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL

Note: Ensure the label exists in Neo4j and JAR is installed as cluster library.
Also verify customSchema column names match your Neo4j node properties.
============================================================
TEST: Direct JDBC - SQL Translation
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[FAIL] Direct JDBC with SQL translation failed: An error occurred while calling o746.load.
: java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.newConnectionTerminatedError(HandshakeHandler.java:126)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:120)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Direct JDBC - SQL Aggregate (COUNT)
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[FAIL] Direct JDBC aggregate query failed: An error occurred while calling o759.load.
: java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.newConnectionTerminatedError(HandshakeHandler.java:126)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:120)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL

Note: Ensure 'Flight' label exists in your Neo4j database, or change to a label that exists.
============================================================
TEST: Direct JDBC - SQL JOIN Translation
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[FAIL] Direct JDBC JOIN translation failed: An error occurred while calling o772.load.
: java.lang.RuntimeException: org.neo4j.jdbc.Neo4jException: general processing exception - An error occurred while handling request
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.nodeOrPattern(SqlToCypher.java:2081)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2048)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2038)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveJoin(SqlToCypher.java:2128)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2034)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.lambda$createOngoingReadingFromSources$0(SqlToCypher.java:529)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1003)
	at java.base/java.util.Collections$UnmodifiableCollection$1.forEachRemaining(Collections.java:1062)
	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:575)
	at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:616)
	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:622)
	at java.base/java.util.stream.ReferencePipeline.toList(ReferencePipeline.java:627)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.createOngoingReadingFromSources(SqlToCypher.java:529)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.statement(SqlToCypher.java:473)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.build(SqlToCypher.java:349)
	at org.neo4j.jdbc.translator.impl.SqlToCypher.translate0(SqlToCypher.java:255)
	at org.neo4j.jdbc.translator.impl.SqlToCypher.translate(SqlToCypher.java:250)
	at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1114)
	at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1085)
	at org.neo4j.jdbc.StatementImpl.processSQL(StatementImpl.java:706)
	at org.neo4j.jdbc.StatementImpl.lambda$executeQuery0$0(StatementImpl.java:156)
	at org.neo4j.jdbc.StatementImpl.recordEvent(StatementImpl.java:425)
	at org.neo4j.jdbc.StatementImpl.executeQuery0(StatementImpl.java:153)
	at org.neo4j.jdbc.PreparedStatementImpl.executeQuery(PreparedStatementImpl.java:168)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.nodeOrPattern(SqlToCypher.java:2081)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2048)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2038)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveJoin(SqlToCypher.java:2128)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2034)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.lambda$createOngoingReadingFromSources$0(SqlToCypher.java:529)
		at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
		at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1003)
		at java.base/java.util.Collections$UnmodifiableCollection$1.forEachRemaining(Collections.java:1062)
		at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845)
		at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
		at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
		at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:575)
		at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
		at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:616)
		at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:622)
		at java.base/java.util.stream.ReferencePipeline.toList(ReferencePipeline.java:627)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.createOngoingReadingFromSources(SqlToCypher.java:529)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.statement(SqlToCypher.java:473)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.build(SqlToCypher.java:349)
		at org.neo4j.jdbc.translator.impl.SqlToCypher.translate0(SqlToCypher.java:255)
		at org.neo4j.jdbc.translator.impl.SqlToCypher.translate(SqlToCypher.java:250)
		at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1114)
		at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1085)
		at org.neo4j.jdbc.StatementImpl.processSQL(StatementImpl.java:706)
		at org.neo4j.jdbc.StatementImpl.lambda$executeQuery0$0(StatementImpl.java:156)
		at org.neo4j.jdbc.StatementImpl.recordEvent(StatementImpl.java:425)
		at org.neo4j.jdbc.StatementImpl.executeQuery0(StatementImpl.java:153)
		at org.neo4j.jdbc.PreparedStatementImpl.executeQuery(PreparedStatementImpl.java:168)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.Neo4jException: general processing exception - An error occurred while handling request
	at org.neo4j.jdbc.DefaultTransactionImpl.execute(DefaultTransactionImpl.java:272)
	at org.neo4j.jdbc.DefaultTransactionImpl.runAndPull(DefaultTransactionImpl.java:130)
	at org.neo4j.jdbc.DatabaseMetadataImpl.doQuery(DatabaseMetadataImpl.java:1977)
	at org.neo4j.jdbc.DatabaseMetadataImpl.doQueryForResultSet(DatabaseMetadataImpl.java:1970)
	at org.neo4j.jdbc.DatabaseMetadataImpl.getTables0(DatabaseMetadataImpl.java:1038)
	at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1708)
	at org.neo4j.jdbc.DatabaseMetadataImpl.getTables(DatabaseMetadataImpl.java:1018)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.nodeOrPattern(SqlToCypher.java:2063)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2048)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2038)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveJoin(SqlToCypher.java:2128)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2034)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.lambda$createOngoingReadingFromSources$0(SqlToCypher.java:529)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1003)
	at java.base/java.util.Collections$UnmodifiableCollection$1.forEachRemaining(Collections.java:1062)
	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:575)
	at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:616)
	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:622)
	at java.base/java.util.stream.ReferencePipeline.toList(ReferencePipeline.java:627)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.createOngoingReadingFromSources(SqlToCypher.java:529)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.statement(SqlToCypher.java:473)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.build(SqlToCypher.java:349)
	at org.neo4j.jdbc.translator.impl.SqlToCypher.translate0(SqlToCypher.java:255)
	at org.neo4j.jdbc.translator.impl.SqlToCypher.translate(SqlToCypher.java:250)
	at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1114)
	at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1085)
	at org.neo4j.jdbc.StatementImpl.processSQL(StatementImpl.java:706)
	at org.neo4j.jdbc.StatementImpl.lambda$executeQuery0$0(StatementImpl.java:156)
	at org.neo4j.jdbc.StatementImpl.recordEvent(StatementImpl.java:425)
	at org.neo4j.jdbc.StatementImpl.executeQuery0(StatementImpl.java:153)
	at org.neo4j.jdbc.PreparedStatementImpl.executeQuery(PreparedStatementImpl.java:168)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection is closed
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.flush(BoltConnectionImpl.java:205)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.lambda$writeAndFlush$2(BoltConnectionImpl.java:144)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.lambda$executeInEventLoop$15(BoltConnectionImpl.java:604)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.lambda$executeInEventLoop$16(BoltConnectionImpl.java:613)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.executeInEventLoop(BoltConnectionImpl.java:619)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.executeInEventLoop(BoltConnectionImpl.java:603)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.writeAndFlush(BoltConnectionImpl.java:142)
	at org.neo4j.jdbc.DefaultTransactionImpl.lambda$runAndPull$0(DefaultTransactionImpl.java:125)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1150)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.lambda$executeInEventLoop$16(BoltConnectionImpl.java:613)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more


Status: FAIL

Note: Requires Flight-[:DEPARTS_FROM]->Airport pattern in Neo4j.
Adjust labels/relationship types to match your graph model.
============================================================
SETUP: Create Unity Catalog JDBC Connection
============================================================
Dropped existing connection (if any): neo4j_connection

[PASS] Connection created: neo4j_connection
============================================================
VERIFY: Connection Configuration
============================================================

Connection details:
+--------------------+--------------------------------------------------------------------------------------------------------------------+
|info_name           |info_value                                                                                                          |
+--------------------+--------------------------------------------------------------------------------------------------------------------+
|Connection Name     |neo4j_connection                                                                                                    |
|Type                |JDBC                                                                                                                |
|Owner               |ryan.knight@neo4j.com                                                                                               |
|Read-only           |true                                                                                                                |
|Options             |externalOptionsAllowList -> dbtable,query,partitionColumn,lowerBound,upperBound,numPartitions,fetchSize,customSchema|
|Environment Settings|Dependencies: "/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar"                    |
+--------------------+--------------------------------------------------------------------------------------------------------------------+

============================================================
TEST: Unity Catalog - Spark DataFrame API
============================================================

[FAIL] Unity Catalog Spark DataFrame API failed:

Error: An error occurred while calling o794.load.
: java.lang.RuntimeException: Connection was closed before the operation completed.
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
	at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
		at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Unity Catalog - Native Cypher (FORCE_CYPHER)
============================================================

[FAIL] Unity Catalog with FORCE_CYPHER failed:

Error: An error occurred while calling o801.load.
: org.apache.spark.SparkException: [JDBC_EXTERNAL_ENGINE_SYNTAX_ERROR.DURING_OUTPUT_SCHEMA_RESOLUTION] JDBC external engine syntax error. The error was caused by the query SELECT * FROM (/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test) SPARK_GEN_SUBQ_61 WHERE 1=0. error: syntax error or access rule violation - invalid syntax. The error occurred during output schema resolution. SQLSTATE: 42000
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.sql.SQLException: error: syntax error or access rule violation - invalid syntax
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:62)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:59)
	at scala.collection.IterableOnceOps.collectFirst(IterableOnce.scala:1256)
	at scala.collection.IterableOnceOps.collectFirst$(IterableOnce.scala:1248)
	at scala.collection.AbstractIterable.collectFirst(Iterable.scala:935)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$1(GrpcExceptionWrapper.scala:59)
	at scala.Option.flatMap(Option.scala:283)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:58)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcPrepareStatementClient$$anon$1.onError(JdbcPrepareStatementClient.scala:33)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Unity Catalog - remote_query() Function
============================================================

[FAIL] Unity Catalog remote_query() failed:

Error: An error occurred while calling o441.sql.
: java.lang.RuntimeException: Connection was closed before the operation completed.
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
	at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
	at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
		at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
		at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
		at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:796)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:762)
		at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:803)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Unity Catalog - SQL Aggregate with Custom Schema
============================================================
Configuration loaded from Databricks Secrets:
  Secret Scope: neo4j-uc-creds
  Neo4j Host: 3f1f827a.databases.neo4j.io
  Bolt URI: neo4j+s://3f1f827a.databases.neo4j.io
  JDBC URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j
  Connection Name: neo4j_connection
  JAR Path: /Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar
============================================================
ENVIRONMENT INFORMATION
============================================================

Spark Version: 4.0.0
Databricks Runtime: 17.3.x-cpu-ml-scala2.13
Python Version: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]
Neo4j Python Driver: 6.0.2

JDBC JAR Path: /Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar
JAR File Exists: True
============================================================
TEST: Network Connectivity (TCP)
============================================================
[2026-01-08T16:29:31.768+0000][1376.973s][info][gc] GC(17) Pause Young (Allocation Failure) 2464M->447M(7580M) 35.016ms

Result: SUCCESS (return_code=0) | Network connectivity to 3f1f827a.databases.neo4j.io:7687 is OPEN | Details: Warning: inverse host lookup failed for 20.124.3.249: Unknown host
3f1f827a.databases.neo4j.io [20.124.3.249] 7687 (?) open

Status: PASS
============================================================
TEST: Neo4j Python Driver
============================================================

[PASS] Driver connectivity verified
[PASS] Query executed: RETURN 1 = 1
[INFO] Connected to: Neo4j Kernel ['5.27-aura']
[INFO] Connected to: Cypher ['5', '25']

Status: PASS
============================================================
TEST: Neo4j Spark Connector (org.neo4j.spark.DataSource)
============================================================

[PASS] Spark Connector query executed successfully:
+----------------------+-----+
|message               |value|
+----------------------+-----+
|Spark Connector Works!|1    |
+----------------------+-----+


Status: PASS
============================================================
TEST: Direct JDBC - dbtable option (reads label as table)
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[PASS] Direct JDBC dbtable 'Aircraft' read successfully:
Schema: StructType([StructField('v$id', StringType(), True), StructField('aircraft_id', StringType(), True), StructField('tail_number', StringType(), True), StructField('icao24', StringType(), True), StructField('model', StringType(), True), StructField('operator', StringType(), True), StructField('manufacturer', StringType(), True)])
+----------------------------------------+-----------+-----------+------+--------+-----------+------------+
|v$id                                    |aircraft_id|tail_number|icao24|model   |operator   |manufacturer|
+----------------------------------------+-----------+-----------+------+--------+-----------+------------+
|4:327a1dec-cd17-4a20-b940-bd752ca20661:0|AC1001     |N95040A    |448367|B737-800|ExampleAir |Boeing      |
|4:327a1dec-cd17-4a20-b940-bd752ca20661:1|AC1002     |N30268B    |aee78a|A320-200|SkyWays    |Airbus      |
|4:327a1dec-cd17-4a20-b940-bd752ca20661:2|AC1003     |N54980C    |7c6b17|A321neo |RegionalCo |Airbus      |
|4:327a1dec-cd17-4a20-b940-bd752ca20661:3|AC1004     |N37272D    |fe5e91|E190    |NorthernJet|Embraer     |
|4:327a1dec-cd17-4a20-b940-bd752ca20661:4|AC1005     |N53032E    |232296|B737-800|ExampleAir |Boeing      |
+----------------------------------------+-----------+-----------+------+--------+-----------+------------+
only showing top 5 rows

Status: PASS
============================================================
TEST: Direct JDBC - SQL Translation
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[PASS] Direct JDBC (SQL translation) query executed:
+-----+
|value|
+-----+
|1    |
+-----+


Status: PASS
============================================================
TEST: Direct JDBC - SQL Aggregate (COUNT)
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[PASS] Direct JDBC SQL aggregate query executed:
+------------+
|flight_count|
+------------+
|2400        |
+------------+


Status: PASS
============================================================
TEST: Direct JDBC - SQL JOIN Translation
============================================================
URL: jdbc:neo4j+s://3f1f827a.databases.neo4j.io:7687/neo4j?enableSQLTranslation=true

[PASS] Direct JDBC SQL JOIN translation executed:
SQL JOINs translated to Cypher relationship pattern!
+-----+
|cnt  |
+-----+
|11200|
+-----+


Status: PASS
============================================================
SETUP: Create Unity Catalog JDBC Connection
============================================================
Dropped existing connection (if any): neo4j_connection

[PASS] Connection created: neo4j_connection
============================================================
VERIFY: Connection Configuration
============================================================

Connection details:
+--------------------+--------------------------------------------------------------------------------------------------------------------+
|info_name           |info_value                                                                                                          |
+--------------------+--------------------------------------------------------------------------------------------------------------------+
|Connection Name     |neo4j_connection                                                                                                    |
|Type                |JDBC                                                                                                                |
|Owner               |ryan.knight@neo4j.com                                                                                               |
|Read-only           |true                                                                                                                |
|Options             |externalOptionsAllowList -> dbtable,query,partitionColumn,lowerBound,upperBound,numPartitions,fetchSize,customSchema|
|Environment Settings|Dependencies: "/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar"                    |
+--------------------+--------------------------------------------------------------------------------------------------------------------+

============================================================
TEST: Unity Catalog - Spark DataFrame API
============================================================
============================================================
TEST: Unity Catalog - Native Cypher (FORCE_CYPHER)
============================================================

[FAIL] Unity Catalog with FORCE_CYPHER failed:

Error: An error occurred while calling o916.load.
: org.apache.spark.SparkException: [JDBC_EXTERNAL_ENGINE_SYNTAX_ERROR.DURING_OUTPUT_SCHEMA_RESOLUTION] JDBC external engine syntax error. The error was caused by the query SELECT * FROM (/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test) SPARK_GEN_SUBQ_131 WHERE 1=0. error: syntax error or access rule violation - invalid syntax. The error occurred during output schema resolution. SQLSTATE: 42000
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.sql.SQLException: error: syntax error or access rule violation - invalid syntax
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:62)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:59)
	at scala.collection.IterableOnceOps.collectFirst(IterableOnce.scala:1256)
	at scala.collection.IterableOnceOps.collectFirst$(IterableOnce.scala:1248)
	at scala.collection.AbstractIterable.collectFirst(Iterable.scala:935)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$1(GrpcExceptionWrapper.scala:59)
	at scala.Option.flatMap(Option.scala:283)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:58)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcPrepareStatementClient$$anon$1.onError(JdbcPrepareStatementClient.scala:33)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)


Status: FAIL
============================================================
TEST: Unity Catalog - remote_query() Function
============================================================
[2026-01-08T16:37:18.985+0000][1844.190s][info][gc] GC(18) Pause Young (Allocation Failure) 2531M->480M(7607M) 22.573ms
============================================================
TEST: Unity Catalog - SQL Aggregate with Custom Schema
============================================================
======================================================================
PHASE 1: JDBC DatabaseMetaData Schema Discovery
======================================================================

[INFO] Connected to: Neo4j Kernel-enterprise-5.27-aura 5.27-aura
[INFO] JDBC Driver: Neo4j JDBC Driver 6.10.3

[INFO] Discovering node labels via getTables(TABLE)...
[INFO] Found 9 labels

[INFO] Discovering properties via getColumns()...

[INFO] Discovering relationships via getTables(RELATIONSHIP)...
[INFO] Found 10 relationship patterns

[PASS] Schema discovery completed successfully
======================================================================
IN-MEMORY SCHEMA MODEL
======================================================================

NODE LABELS (9 discovered):
------------------------------------------------------------

  MaintenanceEvent:
    Primary Key: v$id
    Columns (9):
      - v$id: STRING [generated]
      - aircraft_id: STRING (nullable)
      - system_id: STRING (nullable)
      - component_id: STRING (nullable)
      - severity: STRING (nullable)
      - fault: STRING (nullable)
      - event_id: STRING (nullable)
      - reported_at: STRING (nullable)
      ... and 1 more columns

  Flight:
    Primary Key: v$id
    Columns (9):
      - v$id: STRING [generated]
      - aircraft_id: STRING (nullable)
      - operator: STRING (nullable)
      - scheduled_arrival: STRING (nullable)
      - flight_number: STRING (nullable)
      - origin: STRING (nullable)
      - destination: STRING (nullable)
      - scheduled_departure: STRING (nullable)
      ... and 1 more columns

  System:
    Primary Key: v$id
    Columns (5):
      - v$id: STRING [generated]
      - aircraft_id: STRING (nullable)
      - name: STRING (nullable)
      - type: STRING (nullable)
      - system_id: STRING (nullable)

  Delay:
    Primary Key: v$id
    Columns (5):
      - v$id: STRING [generated]
      - flight_id: STRING (nullable)
      - cause: STRING (nullable)
      - delay_id: STRING (nullable)
      - minutes: INTEGER (nullable)

  Reading:
    Primary Key: v$id
    Columns (5):
      - v$id: STRING [generated]
      - sensor_id: STRING (nullable)
      - value: FLOAT (nullable)
      - reading_id: STRING (nullable)
      - timestamp: STRING (nullable)

  Airport:
    Primary Key: v$id
    Columns (9):
      - v$id: STRING [generated]
      - name: STRING (nullable)
      - country: STRING (nullable)
      - iata: STRING (nullable)
      - airport_id: STRING (nullable)
      - city: STRING (nullable)
      - icao: STRING (nullable)
      - lon: FLOAT (nullable)
      ... and 1 more columns

  Component:
    Primary Key: v$id
    Columns (5):
      - v$id: STRING [generated]
      - name: STRING (nullable)
      - type: STRING (nullable)
      - system_id: STRING (nullable)
      - component_id: STRING (nullable)

  Aircraft:
    Primary Key: v$id
    Columns (7):
      - v$id: STRING [generated]
      - aircraft_id: STRING (nullable)
      - tail_number: STRING (nullable)
      - icao24: STRING (nullable)
      - model: STRING (nullable)
      - operator: STRING (nullable)
      - manufacturer: STRING (nullable)

  Sensor:
    Primary Key: v$id
    Columns (6):
      - v$id: STRING [generated]
      - name: STRING (nullable)
      - type: STRING (nullable)
      - system_id: STRING (nullable)
      - sensor_id: STRING (nullable)
      - unit: STRING (nullable)

RELATIONSHIP PATTERNS (10 discovered):
------------------------------------------------------------
  (:MaintenanceEvent)-[:AFFECTS_SYSTEM]->(:System)
  (:MaintenanceEvent)-[:AFFECTS_AIRCRAFT]->(:Aircraft)
  (:Flight)-[:ARRIVES_AT]->(:Airport)
  (:Flight)-[:DEPARTS_FROM]->(:Airport)
  (:Flight)-[:HAS_DELAY]->(:Delay)
  (:System)-[:HAS_COMPONENT]->(:Component)
  (:System)-[:HAS_SENSOR]->(:Sensor)
  (:Component)-[:HAS_EVENT]->(:MaintenanceEvent)
  (:Aircraft)-[:HAS_SYSTEM]->(:System)
  (:Aircraft)-[:OPERATES_FLIGHT]->(:Flight)

============================================================
SUMMARY: 9 labels, 60 total columns, 10 relationships
======================================================================
OPTION A: Views with Inferred Schema
======================================================================

[INFO] Creating view for label: MaintenanceEvent
[INFO] View name: neo4j_view_maintenanceevent

[INFO] Approach: Create view via DataFrame registration
======================================================================
OPTION B: Tables with Explicit Schema
======================================================================

[INFO] Creating table for label: Flight
[INFO] Table name: neo4j_table_flight

[INFO] Generated customSchema:
  `v$id` STRING, aircraft_id STRING, operator STRING, scheduled_arrival STRING, flight_number STRING, ...
[2026-01-08T16:48:20.436+0000][2505.641s][info][gc] GC(19) Pause Young (Allocation Failure) 2602M->489M(7598M) 19.470ms
======================================================================
OPTION C: Hybrid Approach with Schema Registry
======================================================================

[INFO] Creating schema registry: neo4j_schema_registry
[INFO] Creating view for label: System

[PASS] Schema registry 'neo4j_schema_registry' created with 70 entries

[INFO] Schema Registry Contents (sample):
+----------------+-----------------+-----------+--------------+
|label_name      |column_name      |column_type|is_primary_key|
+----------------+-----------------+-----------+--------------+
|MaintenanceEvent|v$id             |STRING     |true          |
|MaintenanceEvent|aircraft_id      |STRING     |false         |
|MaintenanceEvent|system_id        |STRING     |false         |
|MaintenanceEvent|component_id     |STRING     |false         |
|MaintenanceEvent|severity         |STRING     |false         |
|MaintenanceEvent|fault            |STRING     |false         |
|MaintenanceEvent|event_id         |STRING     |false         |
|MaintenanceEvent|reported_at      |STRING     |false         |
|MaintenanceEvent|corrective_action|STRING     |false         |
|Flight          |v$id             |STRING     |true          |
+----------------+-----------------+-----------+--------------+


[INFO] Relationship Patterns in Registry:
+----------------------+------------------------------+
|relationship          |pattern                       |
+----------------------+------------------------------+
|[REL] AFFECTS_SYSTEM  |(MaintenanceEvent)->(System)  |
|[REL] AFFECTS_AIRCRAFT|(MaintenanceEvent)->(Aircraft)|
|[REL] ARRIVES_AT      |(Flight)->(Airport)           |
|[REL] DEPARTS_FROM    |(Flight)->(Airport)           |
|[REL] HAS_DELAY       |(Flight)->(Delay)             |
+----------------------+------------------------------+

