26/01/08 16:06:41 INFO DriverDaemon$: Started Log4j2
26/01/08 16:06:42 INFO ShutdownHookManager$: Adding shutdown hook with priority=25, defined at com.databricks.DatabricksMain.setupLog4j2Conf
26/01/08 16:06:42 INFO DriverDaemon$: SafeFailCloseBehavior is not set, using the default behavior
26/01/08 16:06:44 WARN FrameworkReadinessSource$: info service or multi readiness probe is not enabled, disabling warmup
26/01/08 16:06:44 INFO OtelSdkInitializer$: OtelSdk registered for service: driver
26/01/08 16:06:44 INFO DriverDaemon$: Current JVM Version 17.0.16
26/01/08 16:06:44 INFO DriverDaemon$: Current JVM tzdata: 2025b
26/01/08 16:06:44 INFO DriverDaemon$: Fail to access field Promise.completeAllExceptions, could be due to not using Databricks' fork
java.lang.NoSuchFieldException: scala$concurrent$impl$Promise$$completeAllExceptions
	at java.base/java.lang.Class.getDeclaredField(Class.java:2610)
	at com.databricks.DatabricksMain.getPromiseCompleteAllExceptions(DatabricksMain.scala:414)
	at com.databricks.DatabricksMain.liftedTree1$1(DatabricksMain.scala:429)
	at com.databricks.DatabricksMain.initPromiseCompleteAllExceptions(DatabricksMain.scala:427)
	at com.databricks.DatabricksMain.initDatabricks(DatabricksMain.scala:487)
	at com.databricks.DatabricksMain.$anonfun$main$1(DatabricksMain.scala:229)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.DatabricksMain.main(DatabricksMain.scala:222)
	at com.databricks.backend.daemon.driver.DriverDaemon.main(DriverDaemon.scala)
26/01/08 16:06:44 INFO ShutdownHookManager$: Adding shutdown hook with priority=100, defined at com.databricks.DatabricksMain.initDatabricks, timeout=17000ms
26/01/08 16:06:44 INFO ShutdownHookManager$: Adding shutdown hook with priority=200, defined at com.databricks.DatabricksMain.initDatabricks
26/01/08 16:06:44 INFO DriverDaemon$: ========== driver starting up ==========
26/01/08 16:06:44 INFO DriverDaemon$: Java: Azul Systems, Inc. 17.0.16
26/01/08 16:06:44 INFO DriverDaemon$: OS: Linux/amd64 5.15.0-1091-azure
26/01/08 16:06:44 INFO DriverDaemon$: CWD: /databricks/driver
26/01/08 16:06:44 INFO DriverDaemon$: Mem: Max: 7.9G loaded GCs: PS MarkSweep, PS Scavenge
26/01/08 16:06:44 INFO DriverDaemon$: Logging multibyte characters: âœ“
26/01/08 16:06:44 INFO DriverDaemon$: 'publicFile.rolling.rewrite' appender in root logger: class org.apache.logging.log4j.core.appender.rewrite.RewriteAppender
26/01/08 16:06:44 INFO DriverDaemon$: == Modules:
26/01/08 16:06:45 INFO DriverDaemon$: Starting prometheus metrics log export timer
26/01/08 16:06:46 INFO SnapstartUtils$: Checkpoint host network info: CheckpointHostInfo(null,null,null,0104-134656-pvz3m00y-10-139-64-4/127.0.1.1,0104-134656-pvz3m00y-10-139-64-4)
26/01/08 16:06:46 INFO DriverDaemon$: Skipping class preloading because it is not enabled in conf
26/01/08 16:06:47 INFO DriverDaemon$: Loaded JDBC drivers in 590 ms
26/01/08 16:06:47 INFO DatabricksEdgeConfigs: serverlessEnabled : false
26/01/08 16:06:47 INFO DatabricksEdgeConfigs: perfPackEnabled : false
26/01/08 16:06:47 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
26/01/08 16:06:47 INFO DatabricksEdgeConfigs: spark.databricks.test.default.enabled : false
26/01/08 16:06:47 INFO DriverDaemon$: Universe Git Hash: 4d129bbb1ace069a115f85f2a992bd89113cd7f9
26/01/08 16:06:47 INFO DriverDaemon$: Spark Git Hash: 8921c73ea8b5b03dba62751ce2d04024288c870a
26/01/08 16:06:47 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path
26/01/08 16:06:47 WARN SparkConfUtils$: Skipping empty value for spark.hadoop.hive.server2.keystore.password
26/01/08 16:06:47 WARN SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete
26/01/08 16:06:47 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
26/01/08 16:06:47 INFO SecurityModeConfUtils$: Unsetting warmup security mode confs
26/01/08 16:06:47 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.,DATA_LABEL_SYSTEM_NOT_SENSITIVE,false,List(),)
26/01/08 16:06:47 INFO DriverDaemon$: Skipping container security setup in the warm pool
26/01/08 16:06:47 INFO DatabricksILoop$: Creating throwaway interpreter
26/01/08 16:06:47 INFO DriverDaemon$: Waiting for lazy Spark confs
26/01/08 16:06:47 INFO DriverDaemon$: Finished waiting for lazy Spark confs
26/01/08 16:06:47 INFO DriverDaemon$: Running post-lazy config delivery actions
26/01/08 16:06:47 INFO DriverDaemon$: Finished running post-lazy config delivery actions
26/01/08 16:06:47 INFO ClusterStartupStepsLogger: Finished driver warm-up step: handling lazy conf in 6 ms
26/01/08 16:06:49 INFO MetastoreMonitor$: Internal metastore configured
26/01/08 16:06:49 INFO DriverDaemon$: Starting metastore monitor
26/01/08 16:06:49 INFO DriverDaemon$: Started metastore monitor
26/01/08 16:06:49 INFO DriverDaemon$: Creating driver daemon
26/01/08 16:06:49 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:06:49 INFO DriverDaemon: Created corral client
26/01/08 16:06:49 INFO DriverDaemon: Creating driver corral backend
26/01/08 16:06:49 INFO DriverCorral: Creating the driver context
26/01/08 16:06:49 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-5894523486654786537-60b01ca8-2f57-4913-b37d-d5cdaa8b98f1
26/01/08 16:06:49 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path
26/01/08 16:06:49 WARN SparkConfUtils$: Skipping empty value for spark.hadoop.hive.server2.keystore.password
26/01/08 16:06:49 WARN SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete
26/01/08 16:06:49 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
26/01/08 16:06:49 INFO SecurityModeConfUtils$: Unsetting warmup security mode confs
26/01/08 16:06:49 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:06:49 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:06:50 INFO SparkContext: Running Spark version 4.0.0
26/01/08 16:06:50 INFO SparkContext: OS info Linux, 5.15.0-1091-azure, amd64
26/01/08 16:06:50 INFO SparkContext: Java version 17.0.16+8-LTS
26/01/08 16:06:50 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:06:50 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:06:50 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 1486 milliseconds)
26/01/08 16:06:50 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:06:50 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:06:50 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:06:50 INFO ResourceUtils: ==============================================================
26/01/08 16:06:50 INFO ResourceUtils: No custom resources configured for spark.driver.
26/01/08 16:06:50 INFO ResourceUtils: ==============================================================
26/01/08 16:06:50 INFO SparkContext: Submitted application: Databricks Shell
26/01/08 16:06:51 INFO JvmCrashLogger: Past JVM crashes: detected 0, logged 0, and skipped 0 already logged
26/01/08 16:06:51 INFO SecurityManager: Changing view acls to: root
26/01/08 16:06:51 INFO SecurityManager: Changing modify acls to: root
26/01/08 16:06:51 INFO SecurityManager: Changing view acls groups to: root
26/01/08 16:06:51 INFO SecurityManager: Changing modify acls groups to: root
26/01/08 16:06:51 INFO SecurityManager: SecurityManager: authentication is enabled: false; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL enabled: false
26/01/08 16:06:51 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:06:51 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:06:51 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 429 milliseconds)
26/01/08 16:06:51 INFO Utils: Successfully started service 'sparkDriver' on port 40407.
26/01/08 16:06:52 INFO SparkEnv: Registering MapOutputTracker
26/01/08 16:06:52 INFO SparkEnv: Registering BlockManagerMaster
26/01/08 16:06:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/01/08 16:06:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/01/08 16:06:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/01/08 16:06:52 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d10c88e2-df3e-4733-a1b0-86139cfdb0cb
26/01/08 16:06:52 INFO SparkEnv: Registering OutputCommitCoordinator
26/01/08 16:06:52 INFO ThreadDumpManager: ThreadDumpManager: started.
26/01/08 16:06:52 INFO HangingThreadDetector: HangingThreadDetector starting.
26/01/08 16:06:52 INFO HangingThreadDetector: Hanging thread detection task starting.
26/01/08 16:06:52 INFO SparkContext: Spark configuration:
databricks.data.unity.enabled=true
databricks.sqlgateway.history.queryParametersByteLimit=51200
libraryDownload.sleepIntervalSeconds=5
libraryDownload.timeoutSeconds=180
spark.aether.driver.id=driver
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.app.startTime=1767888410056
spark.cleaner.referenceTracking.blocking=false
spark.databricks.SC200982.blockBuiltInFunctionOverride=true
spark.databricks.SC200982.blockSecuritySensitiveFunctionOverride=false
spark.databricks.abTesting.client.httpClientClass=com.databricks.common.client.DreamcatcherRuntimeHttpClient
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.acl.scim.client=com.databricks.spark.sql.acl.client.DriverToWebappScimClient
spark.databricks.aether.fixedPort.enabled=true
spark.databricks.automl.serviceEnabled=true
spark.databricks.autotune.maintenance.client.classname=com.databricks.maintenanceautocompute.MACClientImpl
spark.databricks.autotune.maintenance.client.clusterScoped.asyncForcePush.enabled=true
spark.databricks.cloudProvider=Azure
spark.databricks.cloudfetch.hasRegionSupport=true
spark.databricks.cloudfetch.requestDownloadUrlsWithHeaders=*********(redacted)
spark.databricks.cloudfetch.requesterClassName=*********(redacted)
spark.databricks.cluster.profile=singleNode
spark.databricks.clusterSource=UI
spark.databricks.clusterUsageTags.attribute_tag_budget=
spark.databricks.clusterUsageTags.attribute_tag_dust_bazel_path=
spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env=
spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer=
spark.databricks.clusterUsageTags.attribute_tag_dust_resource_class=
spark.databricks.clusterUsageTags.attribute_tag_dust_runbot_id=
spark.databricks.clusterUsageTags.attribute_tag_dust_runner=
spark.databricks.clusterUsageTags.attribute_tag_dust_suite=
spark.databricks.clusterUsageTags.attribute_tag_platform_name=
spark.databricks.clusterUsageTags.attribute_tag_service=
spark.databricks.clusterUsageTags.autoTerminationMinutes=30
spark.databricks.clusterUsageTags.azureSubscriptionId=47fd4ce5-a912-480e-bb81-95fbd59bb6c5
spark.databricks.clusterUsageTags.cloudProvider=Azure
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"ResourceClass","value":"SingleNode"},{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"ryan.knight@neo4j.com"},{"key":"ClusterName","value":"Small Compute w/ Neo4j JDBC and Spark Connect"},{"key":"ClusterId","value":"0104-134656-pvz3m00y"},{"key":"DatabricksEnvironment","value":"workerenv-1098933906466604"}]
spark.databricks.clusterUsageTags.clusterAvailability=ON_DEMAND_AZURE
spark.databricks.clusterUsageTags.clusterCreator=Webapp
spark.databricks.clusterUsageTags.clusterFirstOnDemand=1
spark.databricks.clusterUsageTags.clusterGeneration=11
spark.databricks.clusterUsageTags.clusterId=0104-134656-pvz3m00y
spark.databricks.clusterUsageTags.clusterLastActivityTime=1767884372112
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=false
spark.databricks.clusterUsageTags.clusterLogDestination=
spark.databricks.clusterUsageTags.clusterLogDestinationType=
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterName=Small Compute w/ Neo4j JDBC and Spark Connect
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_D4ds_v5
spark.databricks.clusterUsageTags.clusterNodeTypeFlexibilityEnabled=false
spark.databricks.clusterUsageTags.clusterNumCustomTags=1
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=1098933906466604
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=3
spark.databricks.clusterUsageTags.clusterResourceClass=SingleNode
spark.databricks.clusterUsageTags.clusterScalingType=fixed_size
spark.databricks.clusterUsageTags.clusterSizeType=VM_CONTAINER
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice=-1.0
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=0
spark.databricks.clusterUsageTags.clusterUnityCatalogMode=*********(redacted)
spark.databricks.clusterUsageTags.clusterWorkers=0
spark.databricks.clusterUsageTags.computeKind=CLASSIC_PREVIEW
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.dataPlaneRegion=eastus
spark.databricks.clusterUsageTags.driverContainerId=a01714679e3249078c3f69d84732d4c7
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.139.64.4
spark.databricks.clusterUsageTags.driverInstanceId=d5e3ca8dd9cc4f7092c9b21da507458e
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.139.0.4
spark.databricks.clusterUsageTags.driverNodeType=Standard_D4ds_v5
spark.databricks.clusterUsageTags.effectiveSparkVersion=17.3.x-cpu-ml-scala2.13
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting=false
spark.databricks.clusterUsageTags.instanceWorkerEnvId=workerenv-1098933906466604
spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType=default
spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled=false
spark.databricks.clusterUsageTags.isGroupCluster=false
spark.databricks.clusterUsageTags.isIMv2Enabled=true
spark.databricks.clusterUsageTags.isServicePrincipalCluster=false
spark.databricks.clusterUsageTags.isSingleNode=true
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.managedResourceGroup=simple-demo
spark.databricks.clusterUsageTags.ngrokNpipEnabled=true
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes=0
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace=0
spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2=0
spark.databricks.clusterUsageTags.orgId=1098933906466604
spark.databricks.clusterUsageTags.privateLinkEnabled=false
spark.databricks.clusterUsageTags.region=eastus
spark.databricks.clusterUsageTags.runtimeEngine=STANDARD
spark.databricks.clusterUsageTags.shardName=az-eastus-c3
spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes=false
spark.databricks.clusterUsageTags.sparkImageLabel=release__17.3.x-snapshot-cpu-ml-scala2.13__databricks__17.3.3__4d129bb__8921c73__jenkins__43b99b4__format-3
spark.databricks.clusterUsageTags.sparkMasterUrlType=*********(redacted)
spark.databricks.clusterUsageTags.sparkVersion=17.3.x-cpu-ml-scala2.13
spark.databricks.clusterUsageTags.useMlRuntime=true
spark.databricks.clusterUsageTags.userId=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedSparkVersion=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-1098933906466604
spark.databricks.cmv1.fuseOnHostEnabled=false
spark.databricks.credential.aws.secretKey.redactor=*********(redacted)
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName=*********(redacted)
spark.databricks.credential.scope.fs.impl=*********(redacted)
spark.databricks.credential.scope.fs.onelake.tokenProviderClassName=*********(redacted)
spark.databricks.credential.scope.fs.r2.tokenProviderClassName=*********(redacted)
spark.databricks.credential.scope.fs.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.credential.scope.fs.wasbs.tokenProviderClassName=*********(redacted)
spark.databricks.dagScheduler.workItemMetricsEnabled=true
spark.databricks.dbrActivityRecorder.frameProfiler.enabled=false
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.deltaSharing.clientClassName=com.databricks.deltasharing.DataSharingClientImpl
spark.databricks.driver.classic.wsfsAsyncFlushEnabled=true
spark.databricks.driver.cleanUpSparkSessionsOnUCSharedClusters=true
spark.databricks.driver.enableDncOomMessage=false
spark.databricks.driver.preferredMavenCentralMirrorUrl=*********(redacted)
spark.databricks.driverNfs.clusterWidePythonLibsEnabled=true
spark.databricks.driverNfs.enabled=true
spark.databricks.driverNfs.pathSuffix=.ephemeral_nfs
spark.databricks.driverNodeTypeId=Standard_D4ds_v5
spark.databricks.enablePublicDbfsFuse=false
spark.databricks.eventLog.dir=eventlogs
spark.databricks.eventLog.enabled=true
spark.databricks.eventLog.listenerClassName=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.databricks.filesystem.lokiOverDbfs.enabled=true
spark.databricks.gdrive.fileSystem.enabled=false
spark.databricks.hangingThreadDetector.enabled=true
spark.databricks.hmr.nonQueryLogging.bypassRateLimitAllowList=AppStatusListener.scala
spark.databricks.instanceId=d5e3ca8dd9cc4f7092c9b21da507458e
spark.databricks.io.cache.autocompression.instance.overrides={}
spark.databricks.io.cache.initialDiskSize=161061273600
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.isShieldWorkspace=false
spark.databricks.lakehouseMonitoring.clientClassName=com.databricks.datamonitoring.clients.MonitorClientImpl
spark.databricks.logging.usage.migration.dualLogPercentProbabilitiesSparkCore={"driverMetastoreEvent":{"lC":0,"lP":100,"uP":100},"metastoreUsage":{"lC":100,"lP":100,"uP":0},"mlMlAlgorithm":{"lC":100,"lP":100,"uP":0},"pandasOnSparkImported":{"lC":100,"lP":100,"uP":0},"sparkConnectOperationDuration":{"lC":100,"lP":100,"uP":0},"sparkJobEnd":{"lC":100,"lP":100,"uP":0},"sparkOperationDuration":{"lC":100,"lP":100,"uP":0},"sparkStageCompleted":{"lC":100,"lP":100,"uP":0},"tahoeEvent":{"lC":0,"lP":100,"uP":100},"queryExecutionCost":{"lC":100,"lP":100,"uP":0}}
spark.databricks.managedCatalog.clientClassName=com.databricks.managedcatalog.ManagedCatalogClientImpl
spark.databricks.metricAggregation.internalMetricsDagAggregation.enabled=false
spark.databricks.metricAggregation.stageLevelMetricsAggregationOnTheDAGScheduler.enabled=false
spark.databricks.metrics.filesystem_io_metrics=true
spark.databricks.mlflow.autologging.enableGenAIFlavors=true
spark.databricks.mlflow.autologging.enabled=true
spark.databricks.nativeStackTraceInThreadDump.enabled=true
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.glue.credentialsProviderFactoryClassName=*********(redacted)
spark.databricks.passthrough.glue.executorServiceFactoryClassName=*********(redacted)
spark.databricks.passthrough.oauth.refresher.impl=*********(redacted)
spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class=com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.privateLinkEnabled=false
spark.databricks.proxyHadoopTraffic.host=storage-proxy.databricks.com
spark.databricks.proxyHadoopTraffic.port=9210
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.repl.enableClassFileCleanup=true
spark.databricks.safer.unifiedPath.applyFlag.enabled=true
spark.databricks.safespark.externalUDF.env.installerObjectName=com.databricks.backend.daemon.driver.UDFEnvUtils
spark.databricks.scheduler.executorConnectivityExclusion.enabled=false
spark.databricks.secret.envVar.keys.toRedact=*********(redacted)
spark.databricks.secret.sparkConf.keys.toRedact=*********(redacted)
spark.databricks.service.dbutils.repl.backend=com.databricks.dbconnect.ReplDBUtils
spark.databricks.service.dbutils.server.backend=com.databricks.dbconnect.SparkServerDBUtils
spark.databricks.session.share=false
spark.databricks.sharepoint.fileSystem.enabled=true
spark.databricks.singleuser.fuse.scala.enabled=*********(redacted)
spark.databricks.sparkContextId=5894523486654786537
spark.databricks.sql.configMapperClass=com.databricks.dbsql.config.SqlConfigMapperBridge
spark.databricks.sql.externalUDF.env.enabled=true
spark.databricks.sql.initial.catalog.name=partner_demo_workspace_v2
spark.databricks.sql.jdbc.enableLakeguardDriver=true
spark.databricks.sql.udf.connectionEnvironmentSettings.enabled=true
spark.databricks.sql.udf.routineEnvironmentSettings.enabled=true
spark.databricks.sqlservice.history.metricPolling.runIntervalMs=0
spark.databricks.sse.dmk.overrideMetadataUcEntityId.enabled=true
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.tahoe.logStore.gcp.class=com.databricks.tahoe.store.GCPLogStore
spark.databricks.tahoe.logStore.r2.class=com.databricks.tahoe.store.R2LogStore
spark.databricks.telemetry.prometheus.samplingRate=100
spark.databricks.unityCatalog.clientClassName=com.databricks.managedcatalog.ManagedCatalogClientImpl
spark.databricks.unityCatalog.connectionDfOptionInjection.enabled=true
spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName=*********(redacted)
spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled=*********(redacted)
spark.databricks.unityCatalog.enableServiceCredentials=*********(redacted)
spark.databricks.unityCatalog.enableUcSecretsDbUtils=*********(redacted)
spark.databricks.unityCatalog.enableUcSecretsDdl=*********(redacted)
spark.databricks.unityCatalog.enabled=true
spark.databricks.unityCatalog.enforce.permissions=false
spark.databricks.unityCatalog.externalLocationFallbackMode.enabled=false
spark.databricks.unityCatalog.fabricCrawler.enabled=false
spark.databricks.unityCatalog.glue.federation.enabled=false
spark.databricks.unityCatalog.hms.federation.enableDbfsSupport=true
spark.databricks.unityCatalog.hms.federation.enabled=true
spark.databricks.unityCatalog.lakehouseFederation.writes.enabled=true
spark.databricks.unityCatalog.legacy.enableCrossScopeCredCache=true
spark.databricks.unityCatalog.pathGovernance.dbfsOverUc.enabled=false
spark.databricks.unityCatalog.queryFederation.enabled=true
spark.databricks.unityCatalog.volumes.enabled=true
spark.databricks.unityCatalog.volumes.fuse.server.enabled=true
spark.databricks.unityCatalog.warmupEnabled=false
spark.databricks.universe.commandContextFactory.class=com.databricks.spark.util.UniverseCommandContextFactoryImpl
spark.databricks.workerNodeTypeId=Standard_D4ds_v5
spark.databricks.workspaceUrl=*********(redacted)
spark.databricks.wsfs.workspaceNotebookCwd=true
spark.databricks.wsfs.workspacePrivatePreview=true
spark.databricks.wsfsPublicPreview=true
spark.debugger.gcMonitor.shouldSendProductEventLog=true
spark.delta.sharing.profile.provider.class=*********(redacted)
spark.driver.allowMultipleContexts=false
spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED
spark.driver.host=10.139.64.4
spark.driver.maxResultSize=4g
spark.driver.port=40407
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=/databricks/hadoop-safety-jars/*:/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*
spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -Xlog:gc:stdout:time,uptime,level,tags -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -XX:+UseBiasedLocking -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal.consumer=ALL-UNNAMED --add-opens=jdk.jfr/jdk.jfr.internal=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED --add-opens=jdk.naming.dns/com.sun.jndi.dns=java.naming --add-opens=java.xml.crypto/com.sun.org.apache.xml.internal.security.utils=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Djava.security.manager=allow --enable-native-access=ALL-UNNAMED -XX:+UseParallelGC -Ddatabricks.serviceName=spark-executor-1
spark.executor.id=driver
spark.executor.memory=8874m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.fs.perfMetrics.enable=true
spark.hadoop.databricks.fs.s3a.dmk.slowPathSampler.class=com.databricks.unity.DBRUCSHandleWrapper
spark.hadoop.databricks.fs.s3a.dmk.slowPathSampling.probability.double=0.005
spark.hadoop.databricks.loki.fileStatusCache.enabled=true
spark.hadoop.databricks.loki.fileSystemCache.enabled=true
spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories=false
spark.hadoop.databricks.s3.verifyBucketExists.enabled=false
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.fs.AbstractFileSystem.gs.impl=shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
spark.hadoop.fs.abfs.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.adl.impl.disable.cache=true
spark.hadoop.fs.azure.authorization.caching.enable=false
spark.hadoop.fs.azure.cache.invalidator.type=com.databricks.encryption.utils.CacheInvalidatorImpl
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.azure.user.agent.prefix=*********(redacted)
spark.hadoop.fs.cpfs-abfss.impl=*********(redacted)
spark.hadoop.fs.cpfs-abfss.impl.disable.cache=true
spark.hadoop.fs.cpfs-adl.impl=*********(redacted)
spark.hadoop.fs.cpfs-adl.impl.disable.cache=true
spark.hadoop.fs.cpfs-s3.impl=*********(redacted)
spark.hadoop.fs.cpfs-s3a.impl=*********(redacted)
spark.hadoop.fs.cpfs-s3n.impl=*********(redacted)
spark.hadoop.fs.dbfs.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.dbfs.impl.disable.cache=true
spark.hadoop.fs.dbfsartifacts.impl=com.databricks.backend.daemon.data.client.DBFSV1
spark.hadoop.fs.fcfs-abfs.impl=*********(redacted)
spark.hadoop.fs.fcfs-abfs.impl.disable.cache=true
spark.hadoop.fs.fcfs-abfss.impl=*********(redacted)
spark.hadoop.fs.fcfs-abfss.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3a.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3a.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3n.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3n.impl.disable.cache=true
spark.hadoop.fs.fcfs-wasb.impl=*********(redacted)
spark.hadoop.fs.fcfs-wasb.impl.disable.cache=true
spark.hadoop.fs.fcfs-wasbs.impl=*********(redacted)
spark.hadoop.fs.fcfs-wasbs.impl.disable.cache=true
spark.hadoop.fs.file.impl=com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem
spark.hadoop.fs.gs.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.gs.impl.disable.cache=true
spark.hadoop.fs.gs.outputstream.upload.chunk.size=16777216
spark.hadoop.fs.idbfs.impl=com.databricks.io.idbfs.IdbfsFileSystem
spark.hadoop.fs.local-file.impl=com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem
spark.hadoop.fs.mlflowdbfs.impl=com.databricks.mlflowdbfs.MlflowdbfsFileSystem
spark.hadoop.fs.r2.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.r2.impl.disable.cache=true
spark.hadoop.fs.s3.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.s3.impl.disable.cache=true
spark.hadoop.fs.s3a.assumed.role.credentials.provider=*********(redacted)
spark.hadoop.fs.s3a.attempts.maximum=10
spark.hadoop.fs.s3a.block.size=67108864
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.connection.timeout=50000
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.active.blocks=32
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.s3a.impl.disable.cache=true
spark.hadoop.fs.s3a.max.total.tasks=1000
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.retry.interval=250ms
spark.hadoop.fs.s3a.retry.limit=6
spark.hadoop.fs.s3a.retry.throttle.interval=500ms
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3a.vectored.read.max.merged.size=2M
spark.hadoop.fs.s3a.vectored.read.min.seek.size=128K
spark.hadoop.fs.s3n.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.s3n.impl.disable.cache=true
spark.hadoop.fs.stage.impl=com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem
spark.hadoop.fs.stage.impl.disable.cache=true
spark.hadoop.fs.wasb.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=com.databricks.sql.io.LokiFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.hmshandler.retry.attempts=10
spark.hadoop.hive.hmshandler.retry.interval=2000
spark.hadoop.hive.server2.enable.doAs=false
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty_ssl_driver_keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.abfs.readahead.optimization.enabled=true
spark.hadoop.parquet.block.size.row.check.max=10
spark.hadoop.parquet.block.size.row.check.min=10
spark.hadoop.parquet.filter.columnindex.enabled=false
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.parquet.page.metadata.validation.enabled=true
spark.hadoop.parquet.page.size.check.estimate=false
spark.hadoop.parquet.page.verify-checksum.enabled=true
spark.hadoop.parquet.page.write-checksum.enabled=true
spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled=false
spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException=false
spark.hadoop.spark.databricks.metrics.filesystem_metrics=true
spark.hadoop.spark.driverproxy.customHeadersToProperties=*********(redacted)
spark.hadoop.spark.hadoop.aws.glue.cache.db.size=1000
spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins=30
spark.hadoop.spark.hadoop.aws.glue.cache.table.size=1000
spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins=30
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.home=/databricks/spark
spark.logConf=true
spark.master=local[*, 4]
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-5894523486654786537-60b01ca8-2f57-4913-b37d-d5cdaa8b98f1
spark.rpc.message.maxSize=256
spark.scheduler.listenerbus.eventqueue.capacity=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparklyr-backend.threads=1
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.disabledForLocalCheckpoint.enabled=true
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=/databricks/databricks-hive/*
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=0.13.0
spark.sql.hive.useDatabricksHive122=true
spark.sql.legacy.createHiveTableByDefault=false
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.sources.default=delta
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.streaming.stopTimeout=15s
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.reaper.enabled=true
spark.task.reaper.exitCausedByApp=false
spark.task.reaper.killTimeout=60s
spark.ui.port=40001
spark.ui.prometheus.enabled=true
spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass=com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient
spark.worker.aioaLazyConfig.iamReadinessCheckClientClass=com.databricks.backend.daemon.driver.NephosIamRoleCheckClient
spark.worker.cleanup.enabled=false
26/01/08 16:06:52 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
26/01/08 16:06:52 INFO ErrorEventListener: Configured monitoring unexpected Java module errors with a throttling threshold of 5 unique events per 10 minutes
26/01/08 16:06:52 INFO JfrStreamingManager: Started JFR stream JDK17 HMR
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3a. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3n. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfss. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme gs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme r2. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme wasb. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme wasbs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:06:52 ERROR SparkHadoopUtil: No filesystem implementation found for sharepoint scheme.
26/01/08 16:06:52 ERROR SparkHadoopUtil: No filesystem implementation found for gdrive scheme.
26/01/08 16:06:53 INFO log: Logging initialized @18533ms to org.eclipse.jetty.util.log.Slf4jLog
26/01/08 16:06:53 INFO DatabricksILoop$: Finished creating throwaway interpreter
26/01/08 16:06:54 INFO JettyUtils: Start Jetty 10.139.64.4:40001 for SparkUI
26/01/08 16:06:54 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 17.0.16+8-LTS
26/01/08 16:06:54 INFO Server: Started @19403ms
26/01/08 16:06:54 INFO AbstractConnector: Started ServerConnector@11bf44c0{HTTP/1.1, (http/1.1)}{10.139.64.4:40001}
26/01/08 16:06:54 INFO Utils: Successfully started service 'SparkUI' on port 40001.
26/01/08 16:06:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d251646{/,null,AVAILABLE,@Spark}
26/01/08 16:06:54 INFO DbrActivityRecorder: FrameProfilerExporter is disabled.
26/01/08 16:06:57 INFO Utils: Successfully started service 'org.apache.spark.sql.connect.service.SparkConnectService' on port 15002.
26/01/08 16:06:57 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
26/01/08 16:06:57 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
26/01/08 16:06:57 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
26/01/08 16:06:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 8874, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/01/08 16:06:57 INFO ResourceProfile: Limiting resource is cpu
26/01/08 16:06:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/01/08 16:06:57 INFO SecurityManager: Changing view acls to: root
26/01/08 16:06:57 INFO SecurityManager: Changing modify acls to: root
26/01/08 16:06:57 INFO SecurityManager: Changing view acls groups to: root
26/01/08 16:06:57 INFO SecurityManager: Changing modify acls groups to: root
26/01/08 16:06:57 INFO SecurityManager: SecurityManager: authentication is enabled: false; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL enabled: false
26/01/08 16:06:57 INFO UnifiedMemoryManager: Unmanaged memory polling started with interval 1000ms
26/01/08 16:06:58 INFO FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1
26/01/08 16:06:58 INFO DAGScheduler: Initialized stage-level aggregated metrics cache: maxSize=200, ttlMinutes=5 (currently caching QueryProfile aggregated metrics only)
26/01/08 16:06:58 INFO Executor: Starting executor ID driver on host 10.139.64.4
26/01/08 16:06:58 INFO Executor: OS info Linux, 5.15.0-1091-azure, amd64
26/01/08 16:06:58 INFO Executor: Java version 17.0.16+8-LTS
26/01/08 16:06:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/databricks/hadoop-safety-jars/*,file:/databricks/spark/dbconf/log4j/executor/,file:/databricks/spark/dbconf/jets3t/,file:/databricks/spark/dbconf/hadoop/,file:/databricks/hive/conf/,file:/databricks/jars/*,file:/databricks/driver/conf/,file:/databricks/driver/hadoop,file:/databricks/driver/executor,file:/databricks/driver/*,file:/databricks/driver/jets3t'
26/01/08 16:06:58 INFO Executor: Using REPL class URI: spark://10.139.64.4:40407/classes
26/01/08 16:06:58 INFO Executor: Created or updated repl class loader org.apache.spark.executor.ExecutorClassLoader@615a5152 for default.
26/01/08 16:06:58 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
26/01/08 16:06:58 INFO Utils: resolved command to be run: ArraySeq(getconf, PAGESIZE)
26/01/08 16:06:58 WARN NativeMemoryWatchdog: Native memory watchdog is disabled by conf.
26/01/08 16:06:58 INFO TaskSchedulerImpl: Task preemption enabled.
26/01/08 16:06:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34593.
26/01/08 16:06:58 INFO NettyBlockTransferService: Server created on 10.139.64.4:34593
26/01/08 16:06:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/01/08 16:06:59 INFO BlockManager: external shuffle service port = 4048
26/01/08 16:06:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 34593, None)
26/01/08 16:06:59 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:34593 with 4.2 GiB RAM, BlockManagerId(driver, 10.139.64.4, 34593, None)
26/01/08 16:06:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 34593, None)
26/01/08 16:06:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 34593, None)
26/01/08 16:06:59 INFO MetricsSystem: Starting driver MetricsSystem
26/01/08 16:06:59 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 17.0.16+8-LTS
26/01/08 16:06:59 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1af5eeba{/,null,AVAILABLE}
26/01/08 16:06:59 INFO SslContextFactory: x509=X509@62ba9ff2(1,h=[az-eastus-c3.workers.prod.ns.databricks.com],a=[],w=[]) for Server@28cb1b68[provider=null,keyStore=file:///databricks/keys/jetty_ssl_driver_keystore.jks,trustStore=file:///databricks/keys/jetty_ssl_driver_keystore.jks]
26/01/08 16:06:59 INFO AbstractConnector: Started ServerConnector@3b0e1faa{SSL, (ssl, http/1.1)}{0.0.0.0:1023}
26/01/08 16:06:59 INFO Server: Started @24382ms
26/01/08 16:06:59 INFO FuseDaemonServer: FuseDaemonServer started on 1023 with endpoint: '/get-unity-token'.
26/01/08 16:06:59 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener (compressionEnabled=true)
26/01/08 16:06:59 INFO DBCEventLoggingListener: Logging events to eventlogs/5894523486654786537/eventlog
26/01/08 16:06:59 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3a. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3n. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfss. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme gs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme r2. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme wasb. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme wasbs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:07:00 ERROR SparkHadoopUtil: No filesystem implementation found for sharepoint scheme.
26/01/08 16:07:00 ERROR SparkHadoopUtil: No filesystem implementation found for gdrive scheme.
26/01/08 16:07:00 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@4d251646{/,null,STOPPED,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43c1a2e{/jobs,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@273f247c{/jobs/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7c732d09{/jobs/job,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@54c497f2{/jobs/job/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1c290b82{/stages,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@622828ce{/stages/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@353cf6{/stages/stage,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4afafd11{/stages/stage/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5d730f36{/stages/pool,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@8da061b{/stages/pool/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7bd5f420{/stages/taskThreadDump,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d12d735{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2577f595{/storage,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6c6f9897{/storage/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3c68a85f{/storage/rdd,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@31800e86{/storage/rdd/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@27c73893{/environment,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6b4c6fa5{/environment/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9733465{/executors,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@16a67578{/executors/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@65463b83{/executors/threadDump,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@26154a00{/executors/threadDump/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@30559e92{/executors/heapHistogram,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@505806c8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@719edb69{/executors/heapHistogram,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@35356aef{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4468aa66{/static,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@512d8b67{/,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2907bf0b{/api,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@15764a15{/metrics,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@625a05e1{/jobs/job/kill,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2eafa259{/stages/stage/kill,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@58e53c27{/connect,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10216c09{/connect/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1942bc06{/connect/session,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3cf3a09e{/connect/session/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43aeca02{/metrics/json,null,AVAILABLE,@Spark}
26/01/08 16:07:00 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
26/01/08 16:07:00 INFO DatabricksILoop$: Successfully initialized SparkContext
26/01/08 16:07:02 INFO SharedState: Scheduler stats enabled.
26/01/08 16:07:03 INFO DriverResourceMonitor: Driver resource monitor is disabled.
26/01/08 16:07:03 INFO DriverCapacity: Starting DriverCapacity trackers
26/01/08 16:07:03 INFO MemoryUsageTracker: Starting MemoryUsageTracker
26/01/08 16:07:03 INFO DriverCapacity: Creating DriverMemoryCapacity
26/01/08 16:07:03 INFO DriverCapacity: Creating DriverCpuCapacity
26/01/08 16:07:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/01/08 16:07:03 INFO SharedState: Warehouse path is 'dbfs:/user/hive/warehouse'.
26/01/08 16:07:03 INFO UnifiedSoftstore: [UnifiedSoftstore]: Initializing unified softstore syncer and keylib asynchronously.
26/01/08 16:07:03 INFO ObservedStatsStore: ObservedStatsStoreSoftstoreInit thread started.
26/01/08 16:07:03 INFO UnifiedSoftstore: [UnifiedSoftstore]: Waiting for unified softstore initialization to complete.
26/01/08 16:07:03 INFO UnifiedSoftstore: [UnifiedSoftstore]: Initializing unified softstore syncer and keylib.Using remote softstore: false, using syncer: true
26/01/08 16:07:03 INFO UnifiedSoftstore: [UnifiedSoftstore]: Creating unified Keylib instance
26/01/08 16:07:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6038ed66{/storage/iocache,null,AVAILABLE,@Spark}
26/01/08 16:07:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6e0d9d4a{/storage/iocache/json,null,AVAILABLE,@Spark}
26/01/08 16:07:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49198701{/SQL,null,AVAILABLE,@Spark}
26/01/08 16:07:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b86cf94{/SQL/json,null,AVAILABLE,@Spark}
26/01/08 16:07:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9f2d904{/SQL/execution,null,AVAILABLE,@Spark}
26/01/08 16:07:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@50202c94{/SQL/execution/json,null,AVAILABLE,@Spark}
26/01/08 16:07:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6c82dfd3{/static/sql,null,AVAILABLE,@Spark}
26/01/08 16:07:03 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:07:03 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:07:03 WARN DatabricksEdge: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:07:05 INFO CacheSpace$[assigned-all-dummy-slicelet]:: [CacheSpace.scala:721] Creating HomogeneousCacheSpace unified-softstore-metadata with a cache byte limit of 5000000 bytes
26/01/08 16:07:05 INFO CellCache[]:: [CellCache.scala:578] Starting cache for namespace 'unified-softstore-metadata' with max cache bytes 4.77 MiB
26/01/08 16:07:05 INFO CacheSpace$[assigned-all-dummy-slicelet]:: [CacheSpace.scala:721] Creating HomogeneousCacheSpace observed-stats-store with a cache byte limit of 5000000 bytes
26/01/08 16:07:05 INFO CellCache[]:: [CellCache.scala:578] Starting cache for namespace 'observed-stats-store' with max cache bytes 4.77 MiB
26/01/08 16:07:05 INFO CacheSpace$[assigned-all-dummy-slicelet]:: [CacheSpace.scala:721] Creating HomogeneousCacheSpace history-prediction with a cache byte limit of 5000000 bytes
26/01/08 16:07:05 INFO CellCache[]:: [CellCache.scala:578] Starting cache for namespace 'history-prediction' with max cache bytes 4.77 MiB
26/01/08 16:07:05 INFO UnifiedSoftstore: [UnifiedSoftstore]: Unified Keylib instance created successfully.
26/01/08 16:07:05 INFO UnifiedSoftstore: [UnifiedSoftstore]: Unified softstore syncer is enabled. Initializing unified syncer.
26/01/08 16:07:05 INFO UnifiedSoftstore: [UnifiedSoftstore]: Creating unified softstore syncer instance
26/01/08 16:07:05 INFO SoftstoreSyncer: Created SoftstoreSyncer for cluster 0104-134656-pvz3m00y
26/01/08 16:07:05 INFO SoftstoreSyncer: Started sync thread.
26/01/08 16:07:05 INFO UnifiedSoftstore: [UnifiedSoftstore]: Current status of unified softstore after waitForInitialization -> Initialized.
26/01/08 16:07:05 INFO ObservedStatsStoreSoftstorePredictionBackend: [SoftstorePredictionBackend] Creating softstore with softSpaceName=observed-stats-store, softMapNamePrefix=v1_1098933906466604
26/01/08 16:07:05 INFO UnifiedSoftstore: [UnifiedSoftstore]: Current status of unified softstore after waitForInitialization -> Initialized.
26/01/08 16:07:05 INFO CacheSpace$[assigned-all-dummy-slicelet]:: [CacheSpace.scala:721] Creating HomogeneousCacheSpace observed-stats-store with a cache byte limit of 3333000 bytes
26/01/08 16:07:05 INFO CellCache[]:: [CellCache.scala:578] Starting cache for namespace 'observed-stats-store' with max cache bytes 3.18 MiB
26/01/08 16:07:05 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@e7a5543 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:07:05 INFO SoftMapImpl$[]:: [KeylibImpl.scala:931] [every=2s] Preload failed for softmap v1_1098933906466604 in namespace observed-stats-store. This is expected due to the use of purely local Softstore. Exception: java.lang.UnsupportedOperationException: scanEntries is not supported in LocalStubWrapper
26/01/08 16:07:05 INFO SoftstoreSyncer: Registered softmap for sync: v1_1098933906466604
26/01/08 16:07:05 INFO UnifiedSoftstore: [UnifiedSoftstore]: Current status of unified softstore after waitForInitialization -> Initialized.
26/01/08 16:07:05 INFO SoftstoreSyncer: Registered softmap for sync: v1_1098933906466604_insens
26/01/08 16:07:05 INFO UnifiedSoftstore: [UnifiedSoftstore]: Current status of unified softstore after waitForInitialization -> Initialized.
26/01/08 16:07:05 INFO SoftstoreSyncer: Registered softmap for sync: v1_1098933906466604_shape
26/01/08 16:07:05 INFO ObservedStatsStore: ObservedStatsStoreSoftstorePredictionBackend initialized using unified softstore for WorkspaceId: 1098933906466604
26/01/08 16:07:05 INFO ObservedStatsStore: ObservedStatsStoreSoftstoreInit thread completed in 2566 ms.
26/01/08 16:07:06 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 70 DFA states in the parser. Total cached DFA states: 70in the parser. Driver memory: 7888437248.
26/01/08 16:07:06 INFO QueryPlanningTracker: Query phase parsing took 3s before execution.
26/01/08 16:07:06 INFO SharedState: Initializing ManagedCatalogClient: com.databricks.managedcatalog.ManagedCatalogClientImpl
26/01/08 16:07:10 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 383.762088 ms.
26/01/08 16:07:10 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:07:11 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Metadata GC Threshold
 StartTime: 35984
 Duration: 36
26/01/08 16:07:11 INFO NativeAzureFileSystem: WASB Filesystem null is closed with isClosed = false
26/01/08 16:07:11 INFO MemoryUsageTracker: GC notification:
 Name: PS MarkSweep,
 Action: end of major GC,
 Cause: Metadata GC Threshold
 StartTime: 36020
 Duration: 157
26/01/08 16:07:11 INFO SyncState: Restarted sequence number tracking for: 0104-134656-pvz3m00y
26/01/08 16:07:11 INFO SyncState: Restarted sequence number tracking for: 0104-134656-pvz3m00y
26/01/08 16:07:11 INFO SyncState: Restarted sequence number tracking for: 0104-134656-pvz3m00y
26/01/08 16:07:11 INFO Flags: verboseExceptions: rate-limit=10 (default)
26/01/08 16:07:11 INFO Flags: useEpoll: true (default)
26/01/08 16:07:11 INFO Flags: annotatedServiceExceptionVerbosity: unhandled (default)
26/01/08 16:07:11 INFO Flags: allowSemicolonInPathComponent: true (DatabricksFlagsProvider)
26/01/08 16:07:12 INFO Flags: Using Tls engine: OpenSSL BoringSSL, 0x1010107f
26/01/08 16:07:12 INFO Flags: dumpOpenSslInfo: true (DatabricksFlagsProvider)
26/01/08 16:07:12 INFO Flags: All available SSL protocols: [SSLv2Hello, TLSv1, TLSv1.1, TLSv1.2, TLSv1.3]
26/01/08 16:07:12 INFO Flags: Default enabled SSL protocols: [TLSv1.3, TLSv1.2]
26/01/08 16:07:12 INFO Flags: All available SSL ciphers: [TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, SSL_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, SSL_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, SSL_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, SSL_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, SSL_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, SSL_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256, SSL_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, SSL_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, SSL_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA, SSL_ECDHE_PSK_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, SSL_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, SSL_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA, SSL_ECDHE_PSK_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, SSL_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_GCM_SHA384, SSL_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_AES_128_CBC_SHA, SSL_RSA_WITH_AES_128_CBC_SHA, TLS_PSK_WITH_AES_128_CBC_SHA, SSL_PSK_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA, SSL_RSA_WITH_AES_256_CBC_SHA, TLS_PSK_WITH_AES_256_CBC_SHA, SSL_PSK_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA, SSL_RSA_WITH_3DES_EDE_CBC_SHA, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256]
26/01/08 16:07:12 INFO Flags: Default enabled SSL ciphers: [TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256]
26/01/08 16:07:12 INFO SystemInfo: IPv6: disabled (no IPv6 network interface)
26/01/08 16:07:13 INFO NonBlockingCircuitBreaker: name:DBFS-SHARED state:CLOSED
26/01/08 16:07:13 INFO DatabricksMountsStore: Mount store initialization: Attempting to get the list of mounts from metadata manager of DBFS
26/01/08 16:07:17 INFO DatabricksMountsStore: Mount store initialization: Received a list of 9 mounts accessible from metadata manager of DBFS
26/01/08 16:07:17 INFO DatabricksMountsStore: Updated mounts cache. Changes: List((+,DbfsMountPoint(s3a://databricks-datasets-virginia/, /databricks-datasets)), (+,DbfsMountPoint(uc-volumes:/Volumes, /Volumes)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-tracking)), (+,DbfsMountPoint(abfss://REDACTED_CREDENTIALS(9a269212)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604, /databricks-results)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-registry)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /Volume)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /volumes)), (+,DbfsMountPoint(abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604, /)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /volume)))
26/01/08 16:07:17 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:07:17 INFO HiveConf: Found configuration file file:/databricks/hive/conf/hive-site.xml
26/01/08 16:07:18 INFO ThriftIdempotentExecutionHandler: Initializing ExecuteStatement Idempotency Manager with seenCacheTTLSeconds=21600, responseCacheTTLSeconds=30, responseCacheMaxWeightBytes=104857600, and sparkContextId=5894523486654786537.
26/01/08 16:07:18 INFO ThriftIdempotentExecutionHandler: Initializing FetchResults Idempotency Manager with seenCacheTTLSeconds=21600, responseCacheTTLSeconds=30, responseCacheMaxWeightBytes=419430400, and sparkContextId=5894523486654786537.
26/01/08 16:07:18 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
26/01/08 16:07:18 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 500
26/01/08 16:07:18 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10000 ms
26/01/08 16:07:18 INFO AbstractService: Service:OperationManager is inited.
26/01/08 16:07:18 INFO AbstractService: Service:SessionManager is inited.
26/01/08 16:07:18 INFO SparkSQLCLIService: Service: CLIService is inited.
26/01/08 16:07:18 INFO AbstractService: Service:ThriftHttpCLIService is inited.
26/01/08 16:07:18 INFO HiveThriftServer2: Service: JavaConverters$ is inited.
26/01/08 16:07:18 INFO AbstractService: Service:OperationManager is started.
26/01/08 16:07:18 INFO AbstractService: Service:SessionManager is started.
26/01/08 16:07:18 INFO SparkSQLCLIService: Service: CLIService is started.
26/01/08 16:07:18 INFO AbstractService: Service:ThriftHttpCLIService is started.
26/01/08 16:07:18 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
26/01/08 16:07:18 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
26/01/08 16:07:18 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 17.0.16+8-LTS
26/01/08 16:07:18 INFO session: DefaultSessionIdManager workerName=node0
26/01/08 16:07:18 INFO session: No SessionScavenger set, using defaults
26/01/08 16:07:18 INFO session: node0 Scavenging every 600000ms
26/01/08 16:07:18 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@10bf1c7f{/,null,STARTING} has uncovered http methods for path: /*
26/01/08 16:07:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10bf1c7f{/,null,AVAILABLE}
26/01/08 16:07:18 INFO SslContextFactory: x509=X509@2b6aca7f(1,h=[az-eastus-c3.workers.prod.ns.databricks.com],a=[],w=[]) for Server@48bb2279[provider=null,keyStore=file:///databricks/keys/jetty_ssl_driver_keystore.jks,trustStore=null]
26/01/08 16:07:18 INFO AbstractConnector: Started ServerConnector@2aa3c162{SSL, (ssl, http/1.1)}{0.0.0.0:10000}
26/01/08 16:07:18 INFO Server: Started @43643ms
26/01/08 16:07:18 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
26/01/08 16:07:18 INFO AbstractService: Service:JavaConverters$ is started.
26/01/08 16:07:18 INFO HiveThriftServer2: HiveThriftServer2 started
26/01/08 16:07:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@306465c8{/sqlserver,null,AVAILABLE,@Spark}
26/01/08 16:07:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@786f78c3{/sqlserver/json,null,AVAILABLE,@Spark}
26/01/08 16:07:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@582a7508{/sqlserver/session,null,AVAILABLE,@Spark}
26/01/08 16:07:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1ae04ca1{/sqlserver/session/json,null,AVAILABLE,@Spark}
26/01/08 16:07:18 INFO ClusterStartupStepsLogger: Finished driver warm-up step: thrift server startup in 629 ms
26/01/08 16:07:18 INFO DatabricksILoop$: Trying to load dynamic config on startup: /databricks/driver/conf/dynamicSparkConfig.conf
26/01/08 16:07:18 INFO DatabricksILoop$: Spent 0 ms checking for existence of file /databricks/driver/conf/dynamicSparkConfig.conf, exists: true
26/01/08 16:07:18 INFO DatabricksILoop$: Read the dynamic config: Vector(SaferConf(spark.databricks.remoteMetadataProvider.operationMetrics.enabled,false,1767866966,251107172413520,2,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.replaceAlias.maxExpressionNumNodes,100000,1755294131,250604175429933,2,None,None,List(),None), SaferConf(spark.databricks.remoteQueryTVFEnabled,true,1759928429,250620002151325,2,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.enableMultipleColumnOrPredicateTracking,false,1767809367,251020210800821,2,Some(true),None,List(),None), SaferConf(spark.databricks.unityCatalog.mcsc.initialCatalogParam.enabled,true,1767821862,251028054146996,1,None,None,List(),None), SaferConf(spark.databricks.delta.dbiManagedIcebergTable.enabled,true,1749235576,250407181953986,2,None,None,List(),None), SaferConf(spark.databricks.dataLineage.mergeIntoV2Enabled,false,1731558697,241113190524268,1,None,None,List(),None), SaferConf(spark.databricks.photon.deletedRecordCountsHistogramAgg.enabled,false,1763168071,250930231633843,2,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.aws.restrictSNSPolicyPrincipalAndAccount,false,1749573002,250606224725199,2,None,None,List(),None), SaferConf(spark.databricks.cloudfiles.state.allowStreamingTableNonOwnerAccess,true,1758039744,250729234817321,1,None,None,List(),None), SaferConf(spark.databricks.dynamicConf.demoFlag14,false,1754627510,240927225427317,2,None,None,List(),None), SaferConf(spark.databricks.variant.shreddingBeta.enabled,false,1756409202,250818190500481,1,None,None,List(),None), SaferConf(spark.databricks.optimizer.decorrelateQueriesWithVariantFields.enabled,false,1759441545,250731173245304,1,None,None,List(),None), SaferConf(spark.delta.sharing.client.useStructuralSchemaMatch,true,1747358959,250326212215677,3,None,None,List(),None), SaferConf(spark.databricks.scan.useRelativePathsDuringFileListing,true,1737572344,250118155344432,1,None,None,List(),None), SaferConf(spark.databricks.delta.snapshotHint.supportUcExternalTables,false,1759438110,251002184042520,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.enabled,true,1749084866,250116114847029,1,None,None,List(),None), SaferConf(spark.databricks.logging.queryProfile.captureAllAnalysisExceptions,true,1767659727,250930153432589,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.useStableResultMarkerForRuntimeFilters,true,1761037393,250915215400497,1,None,None,List(),None), SaferConf(spark.databricks.sql.internalHms.exponentialBackoff.enabled,true,1758638306,250912111946014,1,None,None,List(),None), SaferConf(spark.databricks.sql.metricViewV2.enabled,false,1766068919,250808013050988,2,None,None,List(),None), SaferConf(spark.databricks.rewriteViewCatalogInUcOnlyMode,true,1765401866,251109171934436,1,None,None,List(),None), SaferConf(spark.databricks.optimizer.useOldSelectivityDetector,true,1760043286,250903204956922,1,None,None,List(),None), SaferConf(spark.databricks.sql.ignoreRedactNonUTF8Binary,true,1740711077,250204013455621,1,None,None,List(),None), SaferConf(spark.databricks.sql.singlePassResolver.enableScriptingVariableResolution,true,1767651285,251029123748148,6,None,None,List(),None), SaferConf(spark.databricks.rowColumnControlsAllowlistRule.rule.enabled,false,1767721290,250802050035065,2,None,None,List(),None), SaferConf(spark.databricks.safespark.ucPythonUDF.sandboxReuse.enabled,true,1753197410,250627215939534,1,None,None,List(),None), SaferConf(spark.databricks.adaptive.dfp.explicitSchedulingOfBuildProbeSides,true,1764788215,250114152716224,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.multiModality.model.list,databricks-llama-4-maverick,databricks-claude-sonnet-4,databricks-claude-3-7-sonnet,databricks-gemma-3-12b,databricks-gpt-5,databricks-gpt-5-mini,databricks-gpt-5-nano,databricks-gpt-5-1,databricks-gpt-5-2,databricks-gemini-2-5-flash,databricks-gemini-2-5-pro,databricks-gemini-3-flash,databricks-gemini-3-pro,databricks-claude-haiku-4-5,1765233294,251013011721173,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.pathPermissionCacheDynamicTtlMs,10000,1767832323,251125031004338,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.scaleUpThresholdCurrentQpsIncreaseRatio,0.0,1749084866,241021170144915,1,None,None,List(),None), SaferConf(spark.databricks.dynamicConf.demoFlag10,false,1747116558,240921005742527,4,Some(true),None,List(),None), SaferConf(spark.databricks.sql.optimizer.statisticsOnLoadTruncateColumnsAtLimit.enabled,true,1765821023,250130182030788,1,None,None,List(),None), SaferConf(spark.databricks.adaptive.forceCancelSkewedShuffleStageInBroadcastJoin.enabled,false,1763576796,250806193308037,2,None,None,List(),None), SaferConf(spark.databricks.io.cache.parquet.crc,true,1762366027,250605073911737,1,None,None,List(),None), SaferConf(spark.databricks.delta.sqlSnapshotReconstruction.removeRepartition,true,1759898727,250730193950740,1,None,None,List(),None), SaferConf(spark.databricks.sqlservice.history.queryParametersEnabled,true,1749149701,250207220418590,1,None,None,List(),None), SaferConf(spark.databricks.sql.streamingTable.sharedResolution.enabled,false,1765996432,250805035853674,2,None,None,List(),None), SaferConf(spark.databricks.sql.jdbc.lakeguard.writes.enabled,true,1763650195,250925180456726,2,None,None,List(),None), SaferConf(spark.databricks.photon.fileSizeHistogramAgg.enabled,true,1759899673,250811212513942,1,None,None,List(),None), SaferConf(spark.databricks.deltaSharing.shareWithHistoryByDefault,false,1738187793,250124205011061,1,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.rocksDB.wal.ttlSeconds,-1,1763677060,251001203508474,1,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.workloadBasedMetricsForAlterTableSendFullDataSkippingColumns,true,1766081983,251208191323037,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.ignoreUnsupportedTypesInAnalyze.enabled,true,1744037924,250212021653157,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.statisticsOnLoadForPartitionColumns.enabled,true,1755811719,250722035947277,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.includeTemporaryCredentialsInBulkGetMetadata,true,1750108322,241016200607862,1,None,None,List(),None), SaferConf(spark.databricks.optimizer.decorrelateQueriesWithStructFields.enabled,false,1759440504,250627212814136,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.samDerivation.enabled,false,1755794591,250821021733719,9,None,None,List(),None), SaferConf(spark.sql.copyInto.timeout.threadDump.enabled,false,1762295103,250925210350919,3,None,None,List(),None), SaferConf(spark.databricks.hive.metastore.client.enableConnectionInfoLogging,true,1765448974,251021081936733,3,None,None,List(),None), SaferConf(spark.databricks.delta.liquid.eagerClustering.aqeLateStage.fastPath.enabled,false,1765834899,251023233007243,5,None,None,List(),None), SaferConf(spark.databricks.aether.dynamicScanDML.enabled,true,1753212095,250711200810902,2,None,None,List(),None), SaferConf(spark.databricks.sql.jdbc.enableLakeguardDriver,true,1759924431,250806143438304,2,None,None,List(),None), SaferConf(spark.databricks.acl.secureCatalogCheckUsageOnCatalog,true,1737511970,250118161713589,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.decimal.dataType.enabled,true,1749084866,241016192333959,1,None,None,List(),None), SaferConf(spark.databricks.sql.redactSecretsInOptimizer,true,1762203063,250408172844093,3,None,None,List(),None), SaferConf(spark.databricks.docingest.batchEval.size,32,1757955558,250801051128619,2,None,None,List(),None), SaferConf(spark.databricks.io.parquet.readAfterWrite.mode,OFF,1760132062,250716093151055,8,None,None,List(),None), SaferConf(spark.databricks.photon.photonRowToColumnarSnowflakePlan.enabled,false,1765829371,250122181201143,3,None,None,List(),None), SaferConf(spark.databricks.connector.teradata.connectionCharset,UTF8,1763658546,250620105756721,1,None,None,List(),None), SaferConf(spark.delta.sharing.query.includeEndStreamAction,true,1747764306,250319175409926,3,None,None,List(),None), SaferConf(spark.sql.cte.recursion.enabled,true,1748509680,250324144103512,2,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.clientWithCaller.enabled,true,1753369006,250606151049532,1,None,None,List(),None), SaferConf(spark.databricks.autotune.clusterByAuto.preview.enabled,true,1743457639,250122103732373,5,None,None,List(),None), SaferConf(spark.databricks.photon.cleanupResources.enabled,false,1765526745,250905214811826,1,None,None,List(),None), SaferConf(spark.databricks.sqlservice.history.sqlScriptCallStackLimit,65535,1758835746,250925202256765,2,None,None,List(),None), SaferConf(spark.databricks.safespark.ucPythonUDF.sandboxSharing.planning.enabled,true,1753196894,250627215957316,3,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiQuery.openAI.oSeries.model.list,o1-mini,o1,o3-mini,o3,o4-mini,gpt-5,gpt-5-mini,gpt-5-nano,GPT-5,GPT-5 Mini,GPT-5 Nano,1762229377,250808161003858,4,None,None,List(),None), SaferConf(spark.databricks.remoteFiltering.enableServerlessSessionLocalPropagation,true,1752089365,250509142801178,3,None,None,List(),None), SaferConf(spark.databricks.sqlservice.history.isSqlgatewayHistoryProxyClientEnabled,true,1748374558,250107162939870,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.scaleUpThresholdTotalQpsIncreaseRatio,0.0,1749084866,241021170115329,1,None,None,List(),None), SaferConf(spark.databricks.connector.bigquery.loadNamespaceFallbackEnabled,true,1765201664,250722144235673,2,None,None,List(),None), SaferConf(spark.databricks.sql.fsUtils.listShadowingSampleRate,0.2,1763652528,241118084446438,3,None,None,List(),None), SaferConf(spark.databricks.docingest.logging.maxContentLength,0,1761867742,251017083305279,1,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.enableColumnUsageTrackingIntegration,true,1756332507,241217173438622,2,None,None,List(),None), SaferConf(spark.databricks.delta.optimizeMetadataQuery.clusteredTable.enabled,true,1760569480,241022221533035,6,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.ensureRequirementsDP.tracing,false,1750712149,250616231837729,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.sftpConnection.enabled,true,1762325072,250306184629573,2,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.statisticsOnLoadWithSchemaEvolution.enabled,true,1754513159,250627181254441,1,None,None,List(),None), SaferConf(spark.databricks.dataLineage.columnOptimizationV2.enabled,true,1766092955,251125072045225,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.analyzeTruncateColumnsAtLimit.enabled,true,1744138653,250130182152981,1,None,None,List(),None), SaferConf(spark.databricks.deltaSharing.createShareWithBudgetPolicy,true,1758329049,250818232251561,3,None,None,List(),None), SaferConf(spark.databricks.delta.update.metrics.computationInFilters,false,1767886291,250916141657439,1,None,None,List(),None), SaferConf(spark.databricks.adaptive.dppExplicitScheduling.broadcastJoins.enabled,false,1767650851,250730213213737,2,None,None,List(),None), SaferConf(spark.databricks.dlt.mvAsTable.enabled,false,1767733945,250610185714210,2,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.mcsc.unityCatalogEnabledParam.enabled,true,1766109898,251110224334431,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.ucSecretsDdl.enabled,false,1759350369,250924185800840,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.statsOnDataLoadWithRowidTracking.enabled,true,1767837130,241011004901579,1,None,None,List(),None), SaferConf(spark.databricks.delta.unsetManaged.timeWindowInDays,14,1761157706,250701203526418,1,None,None,List(),None), SaferConf(spark.databricks.docingest.version.supported,2.0,1761331507,251010203205248,1,None,None,List(),None), SaferConf(spark.databricks.delta.dataskipping.supportToTimestampExprs.enabled,true,1761845955,250627004305923,1,None,None,List(),None), SaferConf(spark.databricks.sql.semanticMetadata.describe.enabled,true,1761152131,251006190718144,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.AnalyzeTableStalenessThreshold,0,1747859881,250521181612534,2,None,None,List(),None), SaferConf(spark.sql.analyzer.dontDeduplicateExpressionIfExprIdInOutput,false,1761608811,250805083235103,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.scaleUpThresholdSuccessRatio,0.95,1760670087,241011182833050,1,None,None,List(),None), SaferConf(spark.databricks.sqlservice.history.isWisErrorDiagnosticInfoTruncationEnabled,true,1748374558,241220010234395,2,None,None,List(),None), SaferConf(spark.databricks.pipelines.allowAlterNonDBSQLPipeline,true,1754084411,250325214705081,3,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.batch.execution.size,2048,1749084866,241021175030095,1,None,None,List(),None), SaferConf(spark.databricks.sqlgateway.history.profile.backgroundBuildEnabled,true,1758130594,250623230815142,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.http.client.minRetryIntervalMs,500,1757704533,250710181016773,2,None,None,List(),None), SaferConf(spark.databricks.adaptive.dfpExplicitSchedulingWithRBF.enabled,false,1764008607,250114151924018,3,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.purposeBuiltFunctions.batch.execution.enabled,true,1749084866,250220014454853,1,None,None,List(),None), SaferConf(spark.databricks.shuffle.cleanupResources.enabled,false,1762983840,250919174206479,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.safe.inference.enabled,true,1749084866,250122110753330,1,None,None,List(),None), SaferConf(spark.databricks.delta.liquid.lazyClustering.shuffle.preferDataFrameInterface,false,1761861637,250820151825882,2,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.yaml.v11.enabled,true,1759869543,250822195351023,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.connectionDfOptionInjection.enabled,true,1759924484,250708142859302,2,None,None,List(),None), SaferConf(spark.databricks.connector.sqldw.quoteIdentifiersLakehouseFederation,true,1755106101,250618154643267,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.vectorSearch.initialRetryIntervalMillis,1000,1754020297,250711210113443,3,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.rocksDB.wal.sizeLimitMB,0,1763677046,251001210528541,1,None,None,List(),None), SaferConf(spark.databricks.sql.view.legacy.schemaEvolution,false,1763756601,250627073108698,2,None,None,List(),None), SaferConf(spark.databricks.optimizer.rankFilterEarlyStop.unpartitioned.maxLimit,50000,1765404025,251011012636338,1,None,None,List(),None), SaferConf(spark.databricks.sql.udf.connectionEnvironmentSettings.enabled,true,1759924438,250915112742077,2,None,None,List(),None), SaferConf(spark.databricks.delta.incrementalSmallTable.cache.enabled,false,1765070394,250822055339449,2,None,None,List(),None), SaferConf(spark.databricks.optimizer.structSizeInBytesEstimateIncludeSchemaPruning.enabled,true,1760550908,250625222725835,1,None,None,List(),None), SaferConf(spark.databricks.dataLineage.clearColumnTableV2.enabled,true,1767746303,251125075122256,1,None,None,List(),None), SaferConf(spark.sql.insertIntoReplaceUsing.disallowMisalignedColumns.enabled,false,1767805379,250731214706209,7,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.batchSession.terminate.enabled,true,1749084866,250502042721888,4,None,None,List(),None), SaferConf(spark.databricks.photon.hllUnionAggGroupingNonGrouping.enabled,false,1763427104,250906013816206,2,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.enableMetricsForAllUCManagedTables,true,1756330524,250618183020206,1,None,None,List(),None), SaferConf(spark.databricks.sql.allowDroppingDLTMaterializedViewStreamingTable,true,1739914657,250204221211728,1,None,None,List(),None), SaferConf(spark.databricks.delta.uniform.ingress.enableOnFederation.sfdc,true,1749050893,250205200004155,6,None,None,List(),None), SaferConf(spark.databricks.sql.rowColumnAccess.castOnAbacColumnTypeMismatch.enabled,true,1761332151,250919183832472,2,None,None,List(),None), SaferConf(spark.databricks.delta.alterTableUnsetManaged.enabled,true,1758152350,250606210808196,3,None,None,List(),None), SaferConf(spark.databricks.sql.externalUDF.clientImage.enabled,false,1765403636,250529224001473,3,None,None,List(),None), SaferConf(spark.databricks.sql.tempTable.blockDatabricksJobs,false,1760552829,250602165113376,4,None,None,List(),None), SaferConf(spark.databricks.delta.dataskipping.supportTimestampDiff.enabled,false,1767820634,250916174651599,2,None,None,List(),None), SaferConf(spark.databricks.sql.externalUDF.clientImage.supportedMinVersion,3,1752888676,250718174741320,1,None,None,List(),None), SaferConf(spark.databricks.fs.azure.shared.threadpool.enabled,false,1737585215,250122110754651,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.vectorSearch.retryIntervalMultiplierFactor,2.0,1753838593,250711210251185,3,None,None,List(),None), SaferConf(spark.databricks.delta.usageLogging.latencyAccumulatorMetrics,true,1760468198,250602232641906,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.hms.federation.enableDbfsSupport,true,1738321987,250113135917844,2,None,None,List(),None), SaferConf(spark.databricks.io.parquet.rowBased.nativeFrameReader.taskSamplePercent,0,1761061704,241014072545893,13,None,None,List(),None), SaferConf(spark.databricks.optimizer.bloomFilter.disableIfReducingBhjOverApplicationSide,false,1762195555,250919201036066,2,Some(true),None,List(),None), SaferConf(spark.databricks.sqlservice.history.sqlScriptBatchEnabled,true,1764803298,250925010449957,1,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.ucPoNonAuto.scanMetric.sampleRate,0.2,1767730522,250822002312739,4,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.csms.preview.enabled,true,1758047992,250402235140871,5,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.temporary.enabled,true,1762976529,250811184456496,2,None,None,List(),None), SaferConf(spark.databricks.prefetchingReader.json.enabled,true,1748027916,241114034325821,2,None,None,List(),None), SaferConf(spark.databricks.photon.topk.offset.enabled,false,1761247772,250725232744295,1,None,None,List(),None), SaferConf(spark.databricks.sql.externalMetadata.postHook.incrementalUpdateOnly,false,1767824637,251113230217963,2,None,None,List(),None), SaferConf(spark.databricks.rowColumnControlsAllowlistLogs.enabled,false,1765206994,250713232053186,4,None,None,List(),None), SaferConf(spark.databricks.delta.versionedDeepClone.cloneInProgress.enabled,true,1767795202,251104142946081,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.batchInferenceApi.enabled,true,1749084866,241114194854239,1,None,None,List(),None), SaferConf(spark.databricks.delta.vacuum.entityStorageLocation.enabled,true,1762394115,250723162700878,5,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.pushCompleteWorkloadScanMetrics,true,1739914128,241120192053466,6,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.listingV2.enabled,false,1753838907,250729234731771,2,None,None,List(),None), SaferConf(spark.databricks.acl.secureCatalogBatchFilterDatabases,true,1737529326,250121205543302,2,None,None,List(),None), SaferConf(spark.databricks.pipelines.moveTableBetweenPipelines.materializedView.enabled,true,1751048036,250218231715065,1,None,None,List(),None), SaferConf(spark.databricks.io.native.s3.write.taskSamplePercent,0,1760136000,250113134122331,8,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.collectWorkloadBasedDataSkipping.enabled,true,1767646143,250930085804680,1,None,None,List(),None), SaferConf(spark.delta.sharing.network.useAsyncQuery,false,1747366646,250412002126809,4,None,None,List(),None), SaferConf(spark.databricks.optimizer.doNotMergeInvalidAutoStatistics.enabled,true,1758239235,250811170623507,1,None,None,List(),None), SaferConf(spark.databricks.sql.jdbcDialectForbidQueriesWithForbiddenKeywords,true,1765895712,250811103850235,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.maxPoolSize.aiParseDocument,64,1757562366,250910223555852,1,None,None,List(),None), SaferConf(spark.databricks.shuffle.service.native.client.migration.enabled,false,1767831117,250709094904815,4,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.client.UCSEWithMessageTemplateAndClass.enabled,true,1763685074,250930185930488,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.maxPoolSize,2048,1757616926,250118162959038,2,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.storeOptimizerStatsForStructField.enabled,false,1765992285,251124135358517,2,None,None,List(),None), SaferConf(spark.databricks.fs.wasb.writes.etag.enabled,true,1737511970,250118155345967,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.forceMetadataSnapshotBatchUpdate,false,1765202948,250910101039039,2,None,None,List(),None), SaferConf(spark.databricks.sqlservice.history.enableSessionScopedQueryStatusPublish,true,1762284484,250514173339712,1,None,None,List(),None), SaferConf(spark.databricks.prefetchingReader.compressedFiles.enabled,false,1742011471,241121212321445,3,None,None,List(),None), SaferConf(spark.databricks.reyden.saferSessionTest,2,1760136704,251010191719469,2,None,None,List(),None), SaferConf(spark.sql.optimizer.optimizeCsvJsonExprs.useSchemaField,false,1728499876,241008194704608,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.bloomFilterRuntimeValidation.enabled,true,1751908444,250122110754570,2,None,None,List(),None), SaferConf(spark.databricks.delta.writePartitionColumnsToParquet,true,1767827098,251029173022934,1,None,None,List(),None), SaferConf(spark.databricks.sqlservice.history.processExecutorMetricsUpdate,true,1741611920,250308001153572,1,None,None,List(),None), SaferConf(spark.databricks.docingest.stage2.descriptionModel,databricks-gemma-3-12b,1759493425,250805105141191,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.statisticsOnLoadWithColumnMapping.enabled,true,1767841335,250408163116078,1,None,None,List(),None), SaferConf(spark.connect.session.planCompression.threshold,10485760,1765881824,251010122812599,3,None,None,List(),None), SaferConf(spark.databricks.delta.changeDataFeed.readTimeCDCReader.enabled,false,1763120438,250820115540818,2,None,None,List(),None), SaferConf(spark.databricks.docingest.client.v2.enabled,true,1746725321,250430041957882,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.debugLogEnabled,true,1749084866,241009220807728,1,None,None,List(),None), SaferConf(spark.databricks.photon.bitmapAggregator.enabled,false,1764016228,250802010159557,2,None,None,List(),None), SaferConf(spark.databricks.thriftserver.enableComplexTypeNullParameters.enabled,false,1748467204,250328214849569,2,None,None,List(),None), SaferConf(spark.databricks.remoteFiltering.sendBillingInfo,true,1759313961,250818152930084,1,None,None,List(),None), SaferConf(spark.databricks.sql.singlePassResolver.enableTrustedPlanResolution,true,1767642053,251020111436781,6,None,None,List(),None), SaferConf(spark.databricks.sql.expression.aiFunctions.repartition,0,1731096473,241108162446775,1,None,None,List(),None), SaferConf(spark.databricks.sql.scripting.executionV2.enabled,true,1749141921,250311163439370,2,None,None,List(),None), SaferConf(spark.databricks.adaptive.dfpExplicitScheduling.broadcastJoins.enabled,true,1764788201,250618170320005,1,None,None,List(),None), SaferConf(spark.databricks.delta.partitionedTable.staticScan.allowed,false,1765314240,251015192058449,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.externalUDFs.trackedAsDependency,false,1747299881,250422085726336,10,None,None,List(),None), SaferConf(spark.delta.sharing.client.sparkParquetIOCache.enabled,true,1757966888,250521234626602,1,None,None,List(),None), SaferConf(spark.databricks.sql.ES_1355131,false,1752172473,250411091759412,2,None,None,List(),None), SaferConf(spark.databricks.photon.maxMinByNestedTypes.enabled,false,1763427073,250821071825669,2,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.discardColumnStatsWithTooLongKeys.enabled,true,1760954404,250912002310537,1,None,None,List(),None), SaferConf(spark.databricks.dlt.externalMetadata.refreshPolicy,ALL_HISTORY,1767824763,251104032506593,3,None,None,List(),None), SaferConf(spark.databricks.sql.functions.vectorSearch.use.indexIdentifierOnly,true,1748900129,250523161628825,2,None,None,List(),None), SaferConf(spark.databricks.sql.rowColumnAccess.implicitPoliciesNew.enabled,true,1748624394,250521174712203,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.hmsDisabledInfoLogging.enabled,true,1765907915,250902105514821,2,None,None,List(),None), SaferConf(spark.databricks.photon.outputHashCodeForBloomFilter.enabled,false,1759952675,250313205949165,5,None,None,List(),None), SaferConf(spark.databricks.catalog.hiveMetastorePlaceholderPathFixForDbfs.enabled,true,1748350692,250520132535231,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.useDynamicTaskQueueExecutor,false,1729531767,241011183212786,3,None,None,List(),None), SaferConf(spark.databricks.delta.commandInvariantChecksThrowInUpdate,false,1763484912,250408093848281,4,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.ucSecretsDbUtils.enabled,false,1763992331,250924192839066,1,None,None,List(),None), SaferConf(spark.databricks.delta.checkConstraints.validation.enabled,OFF,1767865977,251115003658334,3,None,None,List(),None), SaferConf(spark.databricks.delta.identifierExtraction.newCodePath.enabled,true,1747764696,250221192257794,3,None,None,List(),None), SaferConf(spark.databricks.sql.singlePassResolver.enableTempVariableResolution,true,1767629474,251029123547980,6,None,None,List(),None), SaferConf(spark.databricks.io.native.adlgen2.write.taskSamplePercent,100,1763024613,250122110753207,6,None,None,List(),None), SaferConf(spark.databricks.delta.spark.databricks.execution.resultCaching.versionChangeCheckEnabled,log-only,1767864342,250915150844362,2,Some(true),None,List(),None), SaferConf(spark.databricks.delta.dataskipping.supportConvertTimezone.enabled,true,1764893598,250815224922730,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.allowSetHmsCatalog,true,1767821870,251015232009726,1,None,None,List(),None), SaferConf(spark.databricks.delta.variant.forceUsePreviewTableFeature,true,1750459960,250617192929107,2,None,None,List(),None), SaferConf(spark.databricks.delta.deltaLogUpdate.useHeadForUcManagedTables,true,1760728579,250925200638690,1,None,None,List(),None), SaferConf(spark.databricks.delta.incrementalChecksumOnRead.numCommitsThreshold,20,1761251960,250929173403688,1,None,None,List(),None), SaferConf(spark.databricks.logging.queryProfile.skipSizeInBytes,true,1765398699,251014171558294,1,None,None,List(),None), SaferConf(spark.databricks.table.parquet.compression.codec.enabled,true,1762557246,241105233907422,2,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.hms.federation.enabled,true,1749585737,250121201349384,6,None,None,List(),None), SaferConf(spark.databricks.prefetchingReader.xml.enabled,true,1748027583,241114035005194,2,None,None,List(),None), SaferConf(spark.databricks.docingest.version.default,2.0,1758577669,250819121811044,3,None,None,List(),None), SaferConf(spark.databricks.safer.dimensionVerificationFlag,30,1749846506,250606013736517,3,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.lakehouseFederation.writes.enabled,true,1737511970,250118162025776,3,None,None,List(),None), SaferConf(spark.databricks.delta.deltaCommitSummary.refreshOnMiss.enabled,false,1758834338,250606155642974,5,None,None,List(),None), SaferConf(spark.databricks.dynamicConf.saca.killswitch-rampup-bool,true,1760472960,241003044953062,2,None,None,List(),None), SaferConf(spark.databricks.io.native.gcs.read.taskSamplePercent,100,1752742562,250118155345509,3,None,None,List(),None), SaferConf(spark.sql.insertIntoReplaceUsing.oldDPOCodePath.enabled,false,1767691503,251027184706875,3,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.requireStronglyConsistentStatsForDataSkipping.enabled,true,1761678289,250822225608349,1,None,None,List(),None), SaferConf(spark.databricks.abTesting.leadSampleRate,0.2,1761258738,250123114848723,4,None,None,List(),None), SaferConf(spark.databricks.docingest.pagination.concurrency,4,1757955543,250913122709804,2,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.includeAllDMLForPredictiveAnalyzeMetrics,true,1763410623,251014225531744,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.taskWaitTimeInSeconds,1000,1749084866,241011182924469,1,None,None,List(),None), SaferConf(spark.databricks.optimizer.doNotUseInvalidAutoStatistics.enabled,true,1755810548,250808185549774,1,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.materializations.enabled,true,1764800990,250508193652432,3,None,None,List(),None), SaferConf(spark.databricks.sql.functions.vectorSearch.enabled,true,1737686325,241031222835698,3,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.pushUnusedColumnInfo,true,1759851166,250423215157506,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiGen.endpointName,databricks-meta-llama-3-3-70b-instruct,1755905765,250122110754772,4,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.autoClusteringUpdate.scv.timeoutMs,300000,1767730492,250924065143796,2,None,None,List(),None), SaferConf(spark.databricks.default.table.parquet.compression.codec,zstd,1762126503,241114220844151,10,None,None,List(),None), SaferConf(spark.sql.optimizer.datasourceV2JoinPushdown,true,1756808980,250716120224140,1,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.client.pushIntervalMinutes,1,1752168673,250118155345061,2,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.statisticsOnLoadSelectDeltaColumnsWithPrecedence,true,1761162459,250624152718662,1,None,None,List(),None), SaferConf(spark.databricks.sql.rowColumnAccess.useEffectivePolicies.enabled,true,1760982861,250826030421698,3,None,None,List(),None), SaferConf(spark.databricks.sql.singlePassResolver.enableExtractValueResolution,true,1767629176,250918105117229,6,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.batchSizeForTableFetchForShowTablesExtended,100,1741214740,250118155344310,1,None,None,List(),None), SaferConf(spark.databricks.delta.alterTableSetManaged.validationMode,logonly,1767829402,251027055925954,2,None,None,List(),None), SaferConf(spark.databricks.streaming.foreachBatch.clonedSessionBehavior.enabled,false,1747848775,250519212615341,1,None,None,List(),None), SaferConf(spark.databricks.delta.alterTableSetManaged.skipVacuumableFiles,true,1760719272,250916224132356,2,None,None,List(),None), SaferConf(spark.databricks.delta.stats.localCache.maxNumFiles,12000,1765070445,250818035724793,5,None,None,List(),None), SaferConf(spark.databricks.adaptive.dfpExplicitScheduling.broadcastJoinsWithCostModel.enabled,true,1767721171,250717070503613,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.showStatsUsageInExplain.enabled,true,1738861170,250109173355824,1,None,None,List(),None), SaferConf(spark.databricks.photon.maxMinNestedTypes.enabled,false,1763427085,250917190143722,2,None,None,List(),None), SaferConf(spark.databricks.dynamicConf.demoFlag4,1,1748534164,250108214008473,2,None,None,List(),None), SaferConf(spark.databricks.delta.dataskipping.directAddFileMaterialization.enabled,false,1765832274,251023072056857,2,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.rocksDB.logLevel,ERROR,1762295108,250825202931633,2,None,None,List(),None), SaferConf(spark.databricks.sql.blockUnsupportedRemoteDataSourcesBeforeGatewayEdgeCheck,false,1766066368,251104211033274,2,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.alterWithFieldMasks.enabled,true,1761855805,251001223959258,1,None,None,List(),None), SaferConf(spark.databricks.prefetchingReader.csv.enabled,true,1748018184,241114032708192,2,None,None,List(),None), SaferConf(spark.databricks.optimizer.optimizeLimit.customPartitionSelection,false,1758822635,250805053412748,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.batch.aiQuery.embedding.request.size,40,1749084866,241023001803518,1,None,None,List(),None), SaferConf(spark.databricks.delta.snapshotHint.write.lengthThreshold,358400,1759951353,250808201009227,2,None,None,List(),None), SaferConf(spark.databricks.adaptive.bloomFilter.explicitScheduling.enableForCommands,true,1765311775,250121205543182,6,None,None,List(),None), SaferConf(spark.databricks.cloudfiles.inferPartitionSchemaInSingleVariantColumn.enabled,true,1758214232,250710093205511,2,None,None,List(),None), SaferConf(spark.databricks.io.parquet.vectorized.nativeFrameReader.taskSamplePercent,0,1762939400,241014072244945,17,None,None,List(),None), SaferConf(spark.databricks.delta.uniform.iceberg.v3.enabled,true,1765390123,250819220705932,3,None,None,List(),None), SaferConf(spark.databricks.pipelines.useCredentialHelperForDltClientApiToken,true,1766000061,250912021841308,1,None,None,List(),None), SaferConf(spark.databricks.delta.readclone.postCommit.skipFullUpdatesEnabled,MANAGED,MANAGED_SHALLOW_CLONE,1758050136,250813001116228,1,None,None,List(),None), SaferConf(spark.databricks.delta.logging.async.enabled,false,1758756013,250923023843678,2,None,None,List(),None), SaferConf(spark.databricks.delta.skipping.experimentalFilters.validationSelectivity,0.2,1764874651,250820223027702,9,None,None,List(),None), SaferConf(spark.databricks.adaptive.skewJoin.handlePushedDownAgg.enabled,false,1760072751,251008165417207,4,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.adjustBackfillBeginMs,1000ms,1767809885,251113190537964,5,Some(true),None,List(),None), SaferConf(spark.databricks.delta.typeWidening.allowAutomaticWidening,ALWAYS,1767692688,250702120413718,4,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.embeddingsEndpointName,databricks-gte-large-en,1749084866,250122105858955,7,None,None,List(),None), SaferConf(spark.databricks.acl.secureCatalogBatchFilterTables,true,1737511970,250118155345540,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.purposeBuiltFunctions.translate.instruction,### Instruction

Translate the provided text to %s, and output only the translated text
in the target language in this format: <DBSQLAI>translated text</DBSQLAI>.

If the text is already in the target language, output the provided text.

### Text

%s,1747340790,250513180737565,1,None,None,List(),None), SaferConf(spark.databricks.pyspark.udf.isolation.factory.idleWorkerMaxPoolSize,99999,1757330290,250814092126075,3,None,None,List(),None), SaferConf(spark.databricks.delta.alterTableSetManaged.enabled,true,1758150841,250401172122131,4,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.metadataCache.versioned.enabled,false,1765958283,250423095832618,4,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.batchSession.enabled,true,1749084866,250221233021036,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.client.newErrorClasses.enabled,false,1755882681,250403002450986,1,None,None,List(),None), SaferConf(spark.databricks.dataLineage.columnOptimization.enabled,false,1765397498,251012233407615,4,None,None,List(),None), SaferConf(spark.sql.scripting.enabled,true,1749141921,250122110754241,6,None,None,List(),None), SaferConf(spark.databricks.delta.optimizeMetadataQuery.shadowExecution.rate,0.0,1760985404,250603222911748,13,None,None,List(),None), SaferConf(spark.databricks.io.native.adlgen2.read.taskSamplePercent,100,1765541655,250118155345503,3,None,None,List(),None), SaferConf(spark.databricks.photon.oneRowRelationRowToColumnar.enabled,true,1767638562,250505190005206,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.connectionDfWritesOptionInjection.enabled,true,1763650201,250924112408533,2,None,None,List(),None), SaferConf(spark.databricks.photon.mapZipWith.enabled,false,1763511623,250811194343122,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.vectorSearch.maxRetryInterval,120000,1753838593,250708161129590,3,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.enabled,true,1753818673,241203171259183,5,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.clusterSizeBasedGlobalParallelism.scaleFactor,512.0,1749084866,241021220450892,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.remoteHttpClient.maxConnections,2048,1749084866,241011230804504,1,None,None,List(),None), SaferConf(spark.databricks.sql.showStatisticsPropertiesInDescribeExtended,false,1737529326,250121205543863,1,None,None,List(),None), SaferConf(spark.databricks.sql.jdbcDialectIgnoreQueriesWithForbiddenKeywords,false,1737662132,250123131150571,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.modelEndpointTypeParsing.enabled,true,1749084866,241011183358486,1,None,None,List(),None), SaferConf(spark.databricks.io.native.gcs.write.taskSamplePercent,100,1763024472,250114151924533,6,None,None,List(),None), SaferConf(spark.databricks.sqlservice.history.connect.commands.writeStreamOperationStartEnabled,true,1763145210,250418213540702,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.volume.sam.readAuthzEval.enabled,true,1766074346,251120171741584,1,None,None,List(),None), SaferConf(spark.databricks.sql.pythonVectorizedUDF.strictIsolationSyntax.enabled,true,1753197287,250702154456396,1,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.snowflake.join.enabled,true,1758141288,250617183520100,1,None,None,List(),None), SaferConf(spark.sql.functions.remoteHttpClient.retryOn400TimeoutError,true,1730827108,241011231241015,1,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.recordEventChanges,false,1742410140,250318071048458,3,None,None,List(),None), SaferConf(spark.databricks.analyzer.resolveRelationsCacheRelationsFromNestedViews,false,1760607622,250917122627811,1,None,None,List(),None), SaferConf(spark.databricks.delta.write.keepNullStructsWithNullType,true,1764777671,251001145634780,1,None,None,List(),None), SaferConf(spark.databricks.sql.singlePassResolver.enableParameterResolution,true,1767622336,251021110529669,6,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.materializations.windowMeasure.enabled,true,1764801064,250603185536342,3,None,None,List(),None), SaferConf(spark.databricks.prefetchingReader.text.enabled,false,1764002202,250128010708489,5,None,None,List(),None), SaferConf(spark.databricks.prefetchingReader.binary.enabled,true,1748018208,250128010535706,1,None,None,List(),None), SaferConf(spark.databricks.autotune.maintenance.enableWorkloadScanMetricsApi,true,1739915240,250122204750329,4,None,None,List(),None), SaferConf(spark.databricks.pipelines.moveTableBetweenPipelines.streamingTable.enabled,true,1751048242,250218232941921,1,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.semanticMetadataPropagation.enabled,true,1763751695,251025060805750,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.2hop.rpcAllowlist,MetadataAndPermissionsSnapshot,MetadataSnapshot,GetTable,GetSchema,GetCatalog,1767637795,250730210223338,6,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.viewMatching.enabled,true,1764800993,250603185109561,3,None,None,List(),None), SaferConf(spark.sql.optimizer.checkDynamicFilePruningExprEligibility,false,1765936229,250924004611832,2,None,None,List(),None), SaferConf(spark.databricks.adaptive.dppExplicitScheduling.broadcastJoinsWithCostModel.enabled,false,1765824635,250717070533834,2,None,None,List(),None), SaferConf(spark.databricks.delta.write.escapeBackticksWhenDroppingNullType,true,1764777721,251001152711855,3,None,None,List(),None), SaferConf(spark.databricks.adaptive.lightReplanning.enabled,false,1762384410,250305051422524,4,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.extract.tileMetadata.enabled,true,1753919161,250424044810134,3,None,None,List(),None), SaferConf(spark.databricks.dynamicConf.emily.testFlag,false,1761265000,250909015143825,2,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiForecast.enabled,false,1741133454,250123114848547,2,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.sftp.enabled,true,1762325072,250306185205037,2,None,None,List(),None), SaferConf(spark.databricks.geo.spatial.rangeShuffle.enabled,true,1765810350,250916084451778,1,None,None,List(),None), SaferConf(spark.databricks.delta.dataskipping.supportDateFormatExprs.enabled,false,1763167127,251114234652707,1,None,None,List(),None), SaferConf(spark.databricks.delta.liquid.fixInvalidStateAfterPhaseOut,true,1737511970,250118155345616,2,None,None,List(),None), SaferConf(spark.sql.legacy.codingErrorAction,true,1739382214,250203190102217,4,None,None,List(),None), SaferConf(spark.databricks.hive.hmshandler.retry.configs.enabled,true,1765474395,251006215957127,1,None,None,List(),None), SaferConf(spark.databricks.connector.mssqlserver.driverVersion,current,1755009514,241210223850803,7,None,None,List(),None), SaferConf(spark.databricks.sql.pythonUDTF.enabled,true,1756138948,250703004705547,4,None,None,List(),None), SaferConf(spark.sql.functions.remoteHttpClient.retryOnSocketTimeoutException,true,1760043565,241011231311833,1,None,None,List(),None), SaferConf(spark.databricks.delta.delete.metadataOnlyDelete.shadowMode.rate,0.0,1767827187,250826172228600,3,Some(true),None,List(),None), SaferConf(spark.databricks.unityCatalog.sftpConnection.disableStrictHostKeyCheck,false,1756326629,250603213714962,3,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.useDedicatedHttpClient,true,1749084866,241011231038332,1,None,None,List(),None), SaferConf(spark.databricks.delta.liquid.lazyClustering.postOptimizeCompaction.enabled,true,1758063980,250502185033030,5,None,None,List(),None), SaferConf(spark.databricks.delta.uniform.ingress.autoRefresh.enabled,true,1765390123,250121205542271,2,None,None,List(),None), SaferConf(spark.connect.session.connectML.enabled,true,1758637604,250430114526960,1,None,None,List(),None), SaferConf(spark.databricks.delta.skipping.experimentalFilters.validationRate,0.0,1765498539,250616143539557,33,None,None,List(),None), SaferConf(spark.databricks.sql.excel.enabled,true,1764879410,250818231927637,3,None,None,List(),None), SaferConf(fs.s3a.ifNoneMatch.star.enabled,false,1767865090,250917192713210,3,None,None,List(),None), SaferConf(spark.databricks.abTesting.supportNonDeltaFileFormat,false,1752692742,250716092042961,1,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.cleanSource.enabled,true,1753748247,250321174915957,6,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.remoteHttpClient.timeoutInSeconds,360,1749084866,241011231000692,4,None,None,List(),None), SaferConf(spark.databricks.sqlservice.history.multiBatchProfileEnabled,true,1745259919,250109211140045,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.createBatchSessionOnExecutor,false,1757620944,250910194519740,2,None,None,List(),None), SaferConf(spark.databricks.adaptive.dpp.explicitSchedulingOfBuildProbeSides,true,1764786215,241207003424486,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.statisticsOnLoadForStructWithString.enabled,true,1767841386,250124194306522,1,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.glue.federation.enabled,true,1737164978,250116114153565,1,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.runtimeFiltersMaterializedProbeAsShuffleBoundary,true,1764196878,250617204451345,1,None,None,List(),None), SaferConf(spark.databricks.delta.uniform.ingress.enableOnFederation,true,1765390123,250118155344763,2,None,None,List(),None), SaferConf(spark.databricks.optimizer.useStructFieldReferencesForLikelySelectivePred.enabled,true,1760550957,250702184227226,1,None,None,List(),None), SaferConf(spark.databricks.delta.merge.disableSourceMaterializationNotAllowed,true,1761939217,250429083126347,2,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.analyzeSupportForStruct.enabled,OFF,1764897614,250821174134904,2,None,None,List(),None), SaferConf(com.databricks.safe.ketao.testFlag3,false,1765587005,251211195528802,4,None,None,List(),None), SaferConf(spark.databricks.docingest.maxDocumentPages,5000,1762995634,250903130357149,1,None,None,List(),None), SaferConf(spark.databricks.delta.universalFormatCompatibility.postCommitRefresh.enabled,true,1759884584,250429173318898,3,None,None,List(),None), SaferConf(spark.databricks.sql.optimizer.collectTableStatisticsOnDataLoad.enabled,true,1767747411,230706062431994,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.threadKeepAliveTimeInSeconds,600,1749084866,241011183119413,1,None,None,List(),None), SaferConf(spark.databricks.internalHms.hmshandler.retry.attempts,3,1760454864,251006223017894,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.model.parameters.enabled,true,1749084866,250113140419887,1,None,None,List(),None), SaferConf(spark.databricks.sql.functions.aiFunctions.adaptiveThreadPool.dynamicPoolSizeEnabled,false,1749084866,250220043055429,2,None,None,List(),None), SaferConf(spark.sql.streaming.optimizer.viewFilterPushDown.enabled,true,1761930309,250730025149220,1,None,None,List(),None), SaferConf(spark.databricks.cloudFiles.rocksDB.wal.readerCoordination.enabled,false,1760397272,250902203352846,3,None,None,List(),None), SaferConf(spark.databricks.sql.metricView.materializations.nested.enabled,true,1764801009,250611214753671,3,None,None,List(),None), SaferConf(spark.databricks.photon.jniStringToDecimal,false,1752775522,241115171427019,2,None,None,List(),None), SaferConf(spark.databricks.unityCatalog.volumePathPermissionCache.dynamicTtl.millis,10000,1767828857,251124214724490,1,None,None,List(),None)) from /databricks/driver/conf/dynamicSparkConfig.conf, version 1767888326698
26/01/08 16:07:18 INFO LibraryResolutionManager: Preferred maven central mirror is configured to https://maven-central.storage-download.googleapis.com/maven2/
26/01/08 16:07:18 WARN OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-burst. Using default: 100000000000
26/01/08 16:07:18 WARN OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-steady-rate. Using default: 6000000000
26/01/08 16:07:18 INFO OutgoingDirectNotebookBufferRateLimiter: OutgoingDirectNotebookBufferRateLimiter initialized with clusterBurst=100000000000, clusterSteadyRate=6000000000 
26/01/08 16:07:18 WARN OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-warning-interval-sec. Using default: 60
26/01/08 16:07:18 INFO DriverCorral: Created the driver context
26/01/08 16:07:19 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
26/01/08 16:07:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4dc0094b{/StreamingQuery,null,AVAILABLE,@Spark}
26/01/08 16:07:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@69151aa2{/StreamingQuery/json,null,AVAILABLE,@Spark}
26/01/08 16:07:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@14ac020b{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
26/01/08 16:07:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b43dbf9{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
26/01/08 16:07:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@13652209{/static/sql,null,AVAILABLE,@Spark}
26/01/08 16:07:19 WARN StreamingQueryListenerBus: Failed to capture UCS handle for ListenerBus: org.apache.spark.sql.execution.streaming.runtime.StreamingQueryListenerBus, error: com.databricks.unity.error.MissingCredentialScopeException: [UNITY_CREDENTIAL_SCOPE_MISSING_SCOPE] Missing Credential Scope. Unity Credential Scope id not found in thread locals.. SQLSTATE: XXKUC
26/01/08 16:07:19 WARN StreamingQueryListenerBus: Failed to capture UCS handle for ListenerBus: org.apache.spark.sql.execution.streaming.runtime.StreamingQueryListenerBus, error: com.databricks.unity.error.MissingCredentialScopeException: [UNITY_CREDENTIAL_SCOPE_MISSING_SCOPE] Missing Credential Scope. Unity Credential Scope id not found in thread locals.. SQLSTATE: XXKUC
26/01/08 16:07:19 INFO DriverCorral: metastoreType: InternalMysqlMetastore(DbMetastoreConfig{host=consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com, port=9207, dbName=organization1098933906466604, user=[REDACTED], proxyHost=consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com, proxyPort=9207, proxyUser=[REDACTED]}), enableMetastoreHealthCheck: true
26/01/08 16:07:19 INFO ColdStartReplFactory: REPL pool is disabled.
26/01/08 16:07:19 INFO ShutdownHookManager$: Adding shutdown hook with priority=200, defined at com.databricks.backend.daemon.driver.DriverDaemon
26/01/08 16:07:19 INFO DriverDaemon: Created driver corral backend
26/01/08 16:07:19 INFO DriverDaemon: Creating driver corral server
26/01/08 16:07:19 INFO ReadinessAwareDriverCorral: Adding driverCorral to DriverDaemonRoutingServerBackend
26/01/08 16:07:19 INFO ArmeriaSslConfigurer$: Using these TLS settings for Armeria server listening on :6061
 mTLS enabled: true
 protocols: 
 ciphers: 

26/01/08 16:07:19 INFO DatabricksServerBuilder: Setting server max connection age to 3600000 ms
26/01/08 16:07:19 INFO DatabricksServerBuilder: Set http2MaxStreamsPerConnection to 100
26/01/08 16:07:19 INFO DatabricksServerBuilder: Adaptive Admission Control is disabled, using static request limits: maxConcurrentRequests: 100, maxPendingRequests: None.
26/01/08 16:07:19 INFO SystemInfo: hostname: 0104-134656-pvz3m00y-10-139-64-4 (from /proc/sys/kernel/hostname)
26/01/08 16:07:19 WARN ExecutorServiceMetrics: Failed to bind as com.databricks.threading.InstrumentedScheduledThreadPoolExecutor is unsupported.
26/01/08 16:07:20 INFO DatabricksServerBuilder: Armeria UnaryRpcService server is created:
framework: ARMERIA
port: 6061
rpc_services {
  backend_type: UNARY_RPC_SERVICE
  service_name: "com.databricks.backend.daemon.driver.DriverCorralCompatServerBackend"
  grpc_service: "databricks.chauffeur.DriverCorralService"
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.AutoCompleteInternalRequest"
    grpc_method: "AutoCompleteInternal"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetRuntimeServiceVersionsInternalRequest"
    grpc_method: "GetRuntimeServiceVersionsInternal"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.WorkersPendingRemoval"
    grpc_method: "WorkersPendingRemoval"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.SyncShortTermMemory"
    grpc_method: "SyncShortTermMemory"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.ListRunningExecutors"
    grpc_method: "ListRunningExecutors"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.InstallLibrariesOnExecutionContext"
    grpc_method: "InstallLibrariesOnExecutionContext"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetDriverExecutionStatusRequest"
    grpc_method: "GetDriverExecutionStatus"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.ListNodesRequest"
    grpc_method: "ListNodes"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetClusterLoadInfo"
    grpc_method: "GetClusterLoadInfo"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.StopReplRequest"
    grpc_method: "StopRepl"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetAutoscalingInfoInternalRequest"
    grpc_method: "GetAutoscalingInfo"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.AreQueriesAlive"
    grpc_method: "AreQueriesAlive"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GracefulDecommissionWorkersInPods"
    grpc_method: "GracefulDecommissionWorkersInPods"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetStartupInfo"
    grpc_method: "GetStartupInfo"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.ListSqlGatewayCommands"
    grpc_method: "ListSqlGatewayCommands"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetWorkloadRuntimeInfo"
    grpc_method: "GetWorkloadRuntimeInfo"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetInstallLibrariesOnExecutionContextStatus"
    grpc_method: "GetInstallLibrariesOnExecutionContextStatus"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.ServerlessOverspendExtensionRequest"
    grpc_method: "ServerlessOverspendExtension"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.tracing.ToggleServiceTracing"
    grpc_method: "ToggleServiceTracing"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.InputReplyInternalRequest"
    grpc_method: "InputReplyInternal"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetUsageInfo"
    grpc_method: "GetUsageInfo"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.CancelCommandRequest"
    grpc_method: "CancelCommand"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.SubscribeToLiveQuery"
    grpc_method: "SubscribeToLiveQuery"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.AttachLibrariesRequest"
    grpc_method: "AttachLibraries"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.ExecuteCommandRequest"
    grpc_method: "ExecuteCommand"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.InspectInternalRequest"
    grpc_method: "InspectInternal"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetEnvironmentStatus"
    grpc_method: "GetEnvironmentStatus"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.ReplayWorkloadRequest"
    grpc_method: "ReplayWorkload"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.driver.webterminal.GetWebTerminalConfig"
    grpc_method: "GetWebTerminalConfig"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetDriverHealthRequest"
    grpc_method: "GetDriverHealth"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.SetAdmissionStatus"
    grpc_method: "SetAdmissionStatus"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetSparkActiveJobInfoRequest"
    grpc_method: "GetSparkActiveJobInfo"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.StartReplWarmupRequest"
    grpc_method: "StartReplWarmup"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.StartReplRequest"
    grpc_method: "StartRepl"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.SnapshotReplsRequest"
    grpc_method: "SnapshotRepls"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.WriteDbfsCommandResultInternalRequest"
    grpc_method: "WriteDbfsCommandResultInternal"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.TerminateServerlessSparkSession"
    grpc_method: "TerminateServerlessSparkSession"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.libraries.GetAllLibraryStatusesRequest"
    grpc_method: "GetAllLibraryStatuses"
  }
  handlers {
    jetty_request_type: "com.databricks.api.proto.chauffeur.GetReplStatusRequest"
    grpc_method: "GetReplStatus"
  }
}
concurrency_conf {
  num_threads: 50
  max_concurrent_requests: 100
}
connection_conf {
  max_connection_age_ms: 3600000
}
request_conf {
  timeout_ms: 3600000
  max_request_body_size_bytes: 1073741824
}
framework_version: "1.30.3"

26/01/08 16:07:20 INFO DriverDaemon: Created driver corral server
26/01/08 16:07:20 INFO ArmeriaSslConfigurer$: Using these TLS settings for Armeria server listening on :6062
 KeyStore certificate (1):
 	 serial-no: 7c:aa:20:96:61:ae:19:ed:19:5f:8a:77:2b:df:65:57:2a:e5:d1:18
 	 rsa-public-key-bit-length: 2048
 	 signature-algorithm: SHA256withRSA
 	 valid: Fri Dec 19 01:24:44 UTC 2025 - Tue Jun 16 21:23:00 UTC 2026
 	 issuer: CN=Databricks Prod Worker Services CA, OU=Services, O=Databricks, L=San Francisco, ST=California, C=US
 	 issuer alt-names: n/a
 	 subject: CN=az-eastus-c3.workers.prod.ns.databricks.com
 	 subject alt-names: az-eastus-c3.workers.prod.ns.databricks.com
 TrustStore certificate (cert-0):
 	 serial-no: 14:45:85:52:c8:4a:a1:4a:28:9c:7e:04:b1:92:2a:60:95:a2:d8:9a
 	 rsa-public-key-bit-length: 4096
 	 signature-algorithm: SHA512withRSA
 	 valid: Sat Jun 18 21:23:00 UTC 2016 - Tue Jun 16 21:23:00 UTC 2026
 	 issuer: CN=Databricks Prod Root Signing CA, OU=Services, O=Databricks, L=San Francisco, ST=California, C=US
 	 issuer alt-names: n/a
 	 subject: CN=Databricks Prod Worker Services CA, OU=Services, O=Databricks, L=San Francisco, ST=California, C=US
 	 subject alt-names: n/a
 TrustStore certificate (cert-1):
 	 serial-no: 38:16:8c:2e:39:a6:19:e6:d2:b9:b0:20:7b:f6:90:96:7d:20:ca:f8
 	 rsa-public-key-bit-length: 4096
 	 signature-algorithm: SHA512withRSA
 	 valid: Sat Jun 18 21:23:00 UTC 2016 - Tue Jun 16 21:23:00 UTC 2026
 	 issuer: CN=Databricks Prod Managed Services CA, OU=Services, O=Databricks, L=San Francisco, ST=California, C=US
 	 issuer alt-names: n/a
 	 subject: CN=Databricks Prod Shard Services CA, OU=Services, O=Databricks, L=San Francisco, ST=California, C=US
 	 subject alt-names: n/a
 mTLS enabled: true
 protocols: TLSv1.3,TLSv1.2
 ciphers: TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_256_CBC_SHA256,TLS_EMPTY_RENEGOTIATION_INFO_SCSV

26/01/08 16:07:20 INFO DatabricksServerBuilder: Setting server max connection age to 3600000 ms
26/01/08 16:07:20 INFO DatabricksServerBuilder: Set http2MaxStreamsPerConnection to 100
26/01/08 16:07:20 INFO DatabricksServerBuilder: No standard error handler is set for the server, using DatabricksDefaultServerErrorHandler instead.
26/01/08 16:07:20 INFO DatabricksServerBuilder: Adaptive Admission Control is disabled, using static request limits: maxConcurrentRequests: 50, maxPendingRequests: None.
26/01/08 16:07:20 WARN SslContextUtil: Attempted to configure TLS with a bad cipher suite (TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA). Do not use any cipher suites listed in https://datatracker.ietf.org/doc/html/rfc7540#appendix-A
26/01/08 16:07:20 WARN ExecutorServiceMetrics: Failed to bind as com.databricks.threading.InstrumentedScheduledThreadPoolExecutor is unsupported.
26/01/08 16:07:20 INFO DatabricksServerBuilder: Armeria server (ServerBackend, a.k.a. Phase 1) is created:
framework: ARMERIA
port: 6062
concurrency_conf {
  num_threads: 25
  max_concurrent_requests: 50
}
connection_conf {
  max_connection_age_ms: 3600000
}
request_conf {
  timeout_ms: 55000
  max_request_body_size_bytes: 1048576
}
framework_version: "1.30.3"

26/01/08 16:07:20 INFO DriverDaemon: Created Armeria comm channel server
26/01/08 16:07:20 INFO DriverDaemon$: Finished creating driver daemon
26/01/08 16:07:20 INFO DriverDaemon: Starting driver daemon...
26/01/08 16:07:20 INFO DriverDaemon$: Attempting to run: 'Kill all orphaned Scala kernel JVMs at driver startup'
26/01/08 16:07:20 WARN SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path
26/01/08 16:07:20 WARN SparkConfUtils$: Skipping empty value for spark.hadoop.hive.server2.keystore.password
26/01/08 16:07:20 WARN SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete
26/01/08 16:07:20 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
26/01/08 16:07:20 INFO SecurityModeConfUtils$: Unsetting warmup security mode confs
26/01/08 16:07:20 INFO DriverDaemon: Start refreshing on-creation SAFEr flags for Driver
26/01/08 16:07:20 INFO DriverDaemon: Successfully finished refreshing on-creation SAFEr flags for Driver
26/01/08 16:07:20 INFO DriverDaemon: Setting up driver daemon
26/01/08 16:07:20 INFO DriverDaemon$: Creating repl logs base directory: /databricks/repl-logs
26/01/08 16:07:20 INFO DriverDaemon$: Setting up container security, isIptablesEnabled: false, shouldRunAsLowPrivilegeUser: false, isUnityCatalogEnabled: true
26/01/08 16:07:20 INFO DriverDaemon$: Skipping iptables setup
26/01/08 16:07:20 INFO DriverDaemon$: Finished setting up filesystem permissions for single-user cluster
26/01/08 16:07:20 INFO DriverDaemon$: Finished setting up container security, took 5 ms
26/01/08 16:07:20 INFO DriverDaemon$: Attempting to run: 'set up ttyd daemon'
26/01/08 16:07:20 INFO DriverDaemon$: Attempting to run: 'set up git_agent daemon'
26/01/08 16:07:20 INFO LocalSparkConnectService: Adjusted permissions on /databricks/sparkconnect/grpc.sock: chown:1 chmod:0
26/01/08 16:07:20 INFO DriverDaemon$: No user ID counter detected. Proceeding to set one up.
26/01/08 16:07:20 INFO DriverDaemon$: Finished setting up the user ID tracker.
26/01/08 16:07:20 INFO DriverDaemon: Finished setting up driver daemon
26/01/08 16:07:20 INFO DriverDaemon$: Setting up Python root env
26/01/08 16:07:20 INFO DriverDaemon$: Resetting the default python executable
26/01/08 16:07:20 INFO VirtualenvCloneHelper: Creating Python cluster virtualenv
26/01/08 16:07:20 INFO VirtualenvCloneHelper: Completed configuration of file permissions and ACL entries for environment root directories
26/01/08 16:07:20 INFO Utils: resolved command to be run: List(virtualenv, /local_disk0/.ephemeral_nfs/cluster_libraries/python, -p, /databricks/python/bin/python, --no-download, --no-setuptools, --no-wheel)
26/01/08 16:07:21 INFO PythonEnvCloneHelper$: Created python virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:07:21 INFO Utils: resolved command to be run: List(/databricks/python/bin/python, -c, import sys;dirs=[p for p in sys.path if 'package' in p];print('__SITE_DELIMITER__'.join([f'import site;site.addsitedir(\"\"\"{path}\"\"\")' for path in dirs])))
26/01/08 16:07:21 INFO Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, -c, from sysconfig import get_path; print(get_path('purelib')))
26/01/08 16:07:21 INFO PythonEnvCloneHelper$: Created sites.pth at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/sites.pth
26/01/08 16:07:21 INFO ClusterWidePythonEnvManager: Time spent creating Python cluster virtualenv is 1258 ms
26/01/08 16:07:21 INFO DriverDaemon$: Attempting to run: 'Update root virtualenv'
26/01/08 16:07:21 INFO ClusterWidePythonEnvManager: Registered /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages with the WatchService sun.nio.fs.LinuxWatchService$LinuxWatchKey@64cf24ba (1 current watcher(s)).
26/01/08 16:07:21 INFO DriverDaemon$: Finished updating /etc/environment
26/01/08 16:07:21 INFO DriverDaemon$: Finished setting up Python root env
26/01/08 16:07:21 INFO ReplPoolManagerImpl: Initializing REPL factories Set(generic)
26/01/08 16:07:21 INFO DriverDaemon$$anon$1: Thread to send message is ready
26/01/08 16:07:22 INFO DatabricksTraceExporter$: Tracing: Spark logging predicate set. Initial value: false
26/01/08 16:07:22 INFO DriverDaemon: Tracing: SAFEr driverSpanLogsEnabled flag predicate is set.
26/01/08 16:07:22 INFO DriverDaemon: Tracing: Driver tracing is disabled via SAFEr flag.
26/01/08 16:07:22 INFO DatabricksTraceExporter$: Tracing: Spark trace enabled predicate set. Initial value: false
26/01/08 16:07:22 INFO DriverDaemon: Tracing: SAFEr kill switch predicate is set based on tracingEnabled flag.
26/01/08 16:07:22 INFO DatabricksSparkTracingConfig$: Tracing: Trace generation disabled predicate set. Initial value: false
26/01/08 16:07:22 INFO DriverDaemon: Tracing: SAFEr driverTraceGenerationDisabled flag predicate is set.
26/01/08 16:07:22 INFO NetstatUtil$: netstat -lnpt
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:7681            0.0.0.0:*               LISTEN      937/ttyd            
tcp        0      0 127.0.0.54:53           0.0.0.0:*               LISTEN      87/systemd-resolved 
tcp        0      0 0.0.0.0:2812            0.0.0.0:*               LISTEN      191/monit           
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      87/systemd-resolved 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1/init              
tcp6       0      0 :::8001                 :::*                    LISTEN      334/goofys-dbr      
tcp6       0      0 :::15002                :::*                    LISTEN      656/java            
tcp6       0      0 :::7071                 :::*                    LISTEN      460/java            
tcp6       0      0 10.139.64.4:34593       :::*                    LISTEN      656/java            
tcp6       0      0 :::6059                 :::*                    LISTEN      460/java            
tcp6       0      0 :::6060                 :::*                    LISTEN      460/java            
tcp6       0      0 :::2812                 :::*                    LISTEN      191/monit           
tcp6       0      0 :::10000                :::*                    LISTEN      656/java            
tcp6       0      0 10.139.64.4:40407       :::*                    LISTEN      656/java            
tcp6       0      0 10.139.64.4:40001       :::*                    LISTEN      656/java            
tcp6       0      0 :::1017                 :::*                    LISTEN      319/wsfs            
tcp6       0      0 :::1021                 :::*                    LISTEN      319/wsfs            
tcp6       0      0 :::1023                 :::*                    LISTEN      656/java            
tcp6       0      0 :::1015                 :::*                    LISTEN      334/goofys-dbr      
tcp6       0      0 :::22                   :::*                    LISTEN      1/init              

26/01/08 16:07:22 INFO Server: Serving HTTP at /[0:0:0:0:0:0:0:0]:6061 - http://127.0.0.1:6061/
26/01/08 16:07:22 INFO DriverDaemon: Setup private endpoints
26/01/08 16:07:22 INFO DriverDaemon: Finished setting up private endpoints
26/01/08 16:07:22 INFO DriverDaemon: Marking driver as assigned.
26/01/08 16:07:22 INFO NetstatUtil$: netstat -lnpt
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:7681            0.0.0.0:*               LISTEN      937/ttyd            
tcp        0      0 127.0.0.54:53           0.0.0.0:*               LISTEN      87/systemd-resolved 
tcp        0      0 0.0.0.0:2812            0.0.0.0:*               LISTEN      191/monit           
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      87/systemd-resolved 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1/init              
tcp6       0      0 :::8001                 :::*                    LISTEN      334/goofys-dbr      
tcp6       0      0 :::15002                :::*                    LISTEN      656/java            
tcp6       0      0 :::7071                 :::*                    LISTEN      460/java            
tcp6       0      0 10.139.64.4:34593       :::*                    LISTEN      656/java            
tcp6       0      0 :::6059                 :::*                    LISTEN      460/java            
tcp6       0      0 :::6060                 :::*                    LISTEN      460/java            
tcp6       0      0 :::6061                 :::*                    LISTEN      656/java            
tcp6       0      0 :::2812                 :::*                    LISTEN      191/monit           
tcp6       0      0 :::10000                :::*                    LISTEN      656/java            
tcp6       0      0 10.139.64.4:40407       :::*                    LISTEN      656/java            
tcp6       0      0 10.139.64.4:40001       :::*                    LISTEN      656/java            
tcp6       0      0 :::1017                 :::*                    LISTEN      319/wsfs            
tcp6       0      0 :::1021                 :::*                    LISTEN      319/wsfs            
tcp6       0      0 :::1023                 :::*                    LISTEN      656/java            
tcp6       0      0 :::1015                 :::*                    LISTEN      334/goofys-dbr      
tcp6       0      0 :::22                   :::*                    LISTEN      1/init              

26/01/08 16:07:22 INFO NetstatUtil$: netstat -lnpt output unchanged from previous invocation, skipping detailed output
26/01/08 16:07:22 INFO Server: Serving HTTPS at /[0:0:0:0:0:0:0:0]:6062 - https://127.0.0.1:6062/
26/01/08 16:07:22 INFO Server: Serving HTTP at /[0:0:0:0:0:0:0:0]:6062 - http://127.0.0.1:6062/
26/01/08 16:07:22 INFO DriverDaemon: Started comm channel server
26/01/08 16:07:22 INFO DriverDaemon: Driver daemon started.
26/01/08 16:07:22 INFO NetstatUtil$: netstat -lnpt
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:7681            0.0.0.0:*               LISTEN      937/ttyd            
tcp        0      0 127.0.0.54:53           0.0.0.0:*               LISTEN      87/systemd-resolved 
tcp        0      0 0.0.0.0:2812            0.0.0.0:*               LISTEN      191/monit           
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      87/systemd-resolved 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1/init              
tcp6       0      0 :::8001                 :::*                    LISTEN      334/goofys-dbr      
tcp6       0      0 :::15002                :::*                    LISTEN      656/java            
tcp6       0      0 :::7071                 :::*                    LISTEN      460/java            
tcp6       0      0 10.139.64.4:34593       :::*                    LISTEN      656/java            
tcp6       0      0 :::6059                 :::*                    LISTEN      460/java            
tcp6       0      0 :::6060                 :::*                    LISTEN      460/java            
tcp6       0      0 :::6061                 :::*                    LISTEN      656/java            
tcp6       0      0 :::6062                 :::*                    LISTEN      656/java            
tcp6       0      0 :::2812                 :::*                    LISTEN      191/monit           
tcp6       0      0 :::10000                :::*                    LISTEN      656/java            
tcp6       0      0 10.139.64.4:40407       :::*                    LISTEN      656/java            
tcp6       0      0 10.139.64.4:40001       :::*                    LISTEN      656/java            
tcp6       0      0 :::1017                 :::*                    LISTEN      319/wsfs            
tcp6       0      0 :::1021                 :::*                    LISTEN      319/wsfs            
tcp6       0      0 :::1023                 :::*                    LISTEN      656/java            
tcp6       0      0 :::1015                 :::*                    LISTEN      334/goofys-dbr      
tcp6       0      0 :::22                   :::*                    LISTEN      1/init              

26/01/08 16:07:22 INFO ClusterStartupStepsLogger: Finished driver warm-up step: daemon start in 2198 ms
26/01/08 16:07:22 INFO DriverDaemon$: AppCDS archive is memory mapped: false
26/01/08 16:07:22 INFO DriverDaemon$: Logged AppCDS status in 3 ms
26/01/08 16:07:23 WARN LoggingService: [sreqId=43cc2307, chanId=4ee0758d, raddr=10.139.64.4:56568, laddr=10.139.64.4:6061][h1c://0104-134656-pvz3m00y-10-139-64-4/#POST] Request: {startTime=2026-01-08T16:07:23.074Z(1767888443074000), length=50B, duration=5157Âµs(5157654ns), scheme=ws+h1c, name=POST, headers=[:method=POST, :path=/?type="com.databricks.api.proto.chauffeur.GetSparkActiveJobInfoRequest", x-request-id=80393917-062c-48d1-9e76-1d6e5cb900aa, content-length=50, content-type=application/octet-stream, traceparent=00-07351eea18c8f07333c03de6112e87f8-7ec7c9d98c2beca4-00]}
26/01/08 16:07:23 WARN LoggingService: [sreqId=43cc2307, chanId=4ee0758d, raddr=10.139.64.4:56568, laddr=10.139.64.4:6061][h1c://0104-134656-pvz3m00y-10-139-64-4/#POST] Response: {startTime=2026-01-08T16:07:23.124Z(1767888443124000), length=228B, duration=0ns, totalDuration=49436Âµs(49436175ns), cause=com.databricks.api.base.DatabricksServiceException: BAD_REQUEST: [SHOULD_USE_AUTOSCALING_INFO] , headers=[:status=500, content-length=228, content-type=application/octet-stream]}
com.databricks.api.base.DatabricksServiceException: BAD_REQUEST: [SHOULD_USE_AUTOSCALING_INFO] 
	at com.databricks.api.base.DatabricksServiceException$.apply(DatabricksServiceException.scala:458)
	at com.databricks.common.chauffeur.exception.ExceptionType.toDatabricksServiceException(ChauffeurException.scala:254)
	at com.databricks.common.chauffeur.exception.ChauffeurException$.shouldUseAutoscalingInfoException(ChauffeurException.scala:76)
	at com.databricks.backend.daemon.driver.DriverCorral.getSparkActiveJobInfo(DriverCorral.scala:1999)
	at com.databricks.backend.daemon.driver.DriverCorralCompatServerBackend.$anonfun$handlers$15(DriverCorralCompatServerBackend.scala:111)
	at com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:470)
	at com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:413)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:618)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:596)
	at com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)
	at com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:594)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)
	at grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)
	at com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)
	at com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
internalStackTrace: 
26/01/08 16:07:24 INFO ReplManagerImpl: REPL ReplId-19b9e-5c1b4-a finished addReplToExecutionContext (1)
26/01/08 16:07:24 INFO ColdStartReplFactory: Cold-start repl creation for: sql, ReplInfo(driverReplId=ReplId-19b9e-5c1b4-a, chauffeurReplId=ReplId-19b9e-5c1b4-a,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:07:24 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:07:24 WARN StreamingQueryListenerBus: Failed to capture UCS handle for ListenerBus: org.apache.spark.sql.execution.streaming.runtime.StreamingQueryListenerBus, error: com.databricks.unity.error.MissingCredentialScopeException: [UNITY_CREDENTIAL_SCOPE_MISSING_SCOPE] Missing Credential Scope. Failed to find Unity Credential Scope.. SQLSTATE: XXKUC
26/01/08 16:07:24 WARN StreamingQueryListenerBus: Failed to capture UCS handle for ListenerBus: org.apache.spark.sql.execution.streaming.runtime.StreamingQueryListenerBus, error: com.databricks.unity.error.MissingCredentialScopeException: [UNITY_CREDENTIAL_SCOPE_MISSING_SCOPE] Missing Credential Scope. Failed to find Unity Credential Scope.. SQLSTATE: XXKUC
26/01/08 16:07:24 INFO DriverCorral: Loading the root classloader
26/01/08 16:07:24 INFO ReplOuterWrapper: DriverWrapper created for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b4-a, chauffeurReplId=ReplId-19b9e-5c1b4-a,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplManagerImpl: Adding repl ReplId-19b9e-5c1b4-a to execution context 0 (2)
26/01/08 16:07:24 INFO ReplManagerImpl: StartReplOptions: 
26/01/08 16:07:24 INFO ReplManagerImpl: Skipping snapshot restore for REPL ReplId-19b9e-5c1b4-a because language is sql (only Python REPLs support snapshot restore)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL status transitioned to Starting ReplId-19b9e-5c1b4-a (3)
26/01/08 16:07:24 INFO SQLDriverWrapper: Starting ReplId-19b9e-5c1b4-a - driverStatus transitioned to Starting (4)
26/01/08 16:07:24 INFO ReplManagerImpl: REPL ReplId-19b9e-5c1b4-b finished addReplToExecutionContext (1)
26/01/08 16:07:24 INFO ColdStartReplFactory: Cold-start repl creation for: sql, ReplInfo(driverReplId=ReplId-19b9e-5c1b4-b, chauffeurReplId=ReplId-19b9e-5c1b4-b,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplOuterWrapper: DriverWrapper created for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b4-b, chauffeurReplId=ReplId-19b9e-5c1b4-b,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplManagerImpl: Adding repl ReplId-19b9e-5c1b4-b to execution context 0 (2)
26/01/08 16:07:24 INFO ReplManagerImpl: StartReplOptions: 
26/01/08 16:07:24 INFO ReplManagerImpl: Skipping snapshot restore for REPL ReplId-19b9e-5c1b4-b because language is sql (only Python REPLs support snapshot restore)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL status transitioned to Starting ReplId-19b9e-5c1b4-b (3)
26/01/08 16:07:24 INFO SQLDriverWrapper: Starting ReplId-19b9e-5c1b4-b - driverStatus transitioned to Starting (4)
26/01/08 16:07:24 INFO SQLDriverWrapper: Driver instantiated for ReplId-19b9e-5c1b4-b (6)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL started but is idle: ReplInfo(driverReplId=ReplId-19b9e-5c1b4-b, chauffeurReplId=ReplId-19b9e-5c1b4-b,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true) (7)
26/01/08 16:07:24 INFO ReplManagerImpl: REPL ReplId-19b9e-5c1b4-c finished addReplToExecutionContext (1)
26/01/08 16:07:24 INFO ColdStartReplFactory: Cold-start repl creation for: sql, ReplInfo(driverReplId=ReplId-19b9e-5c1b4-c, chauffeurReplId=ReplId-19b9e-5c1b4-c,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplOuterWrapper: DriverWrapper created for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b4-c, chauffeurReplId=ReplId-19b9e-5c1b4-c,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO SQLDriverWrapper: Driver instantiated for ReplId-19b9e-5c1b4-a (6)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL started but is idle: ReplInfo(driverReplId=ReplId-19b9e-5c1b4-a, chauffeurReplId=ReplId-19b9e-5c1b4-a,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true) (7)
26/01/08 16:07:24 INFO ReplManagerImpl: Adding repl ReplId-19b9e-5c1b4-c to execution context 0 (2)
26/01/08 16:07:24 INFO ReplManagerImpl: StartReplOptions: 
26/01/08 16:07:24 INFO ReplManagerImpl: Skipping snapshot restore for REPL ReplId-19b9e-5c1b4-c because language is sql (only Python REPLs support snapshot restore)
26/01/08 16:07:24 INFO SQLDriverWrapper: setupRepl:ReplInfo(driverReplId=ReplId-19b9e-5c1b4-a, chauffeurReplId=ReplId-19b9e-5c1b4-a,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true): finished to load
26/01/08 16:07:24 INFO SQLDriverWrapper: Finished setting-up REPL ReplId-19b9e-5c1b4-a, accepting commands (8)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL status transitioned to Starting ReplId-19b9e-5c1b4-c (3)
26/01/08 16:07:24 INFO SQLDriverWrapper: setupRepl:ReplInfo(driverReplId=ReplId-19b9e-5c1b4-b, chauffeurReplId=ReplId-19b9e-5c1b4-b,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true): finished to load
26/01/08 16:07:24 INFO SQLDriverWrapper: Finished setting-up REPL ReplId-19b9e-5c1b4-b, accepting commands (8)
26/01/08 16:07:24 INFO SQLDriverWrapper: Starting ReplId-19b9e-5c1b4-c - driverStatus transitioned to Starting (4)
26/01/08 16:07:24 INFO SQLDriverWrapper: Driver instantiated for ReplId-19b9e-5c1b4-c (6)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL started but is idle: ReplInfo(driverReplId=ReplId-19b9e-5c1b4-c, chauffeurReplId=ReplId-19b9e-5c1b4-c,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true) (7)
26/01/08 16:07:24 INFO SQLDriverWrapper: setupRepl:ReplInfo(driverReplId=ReplId-19b9e-5c1b4-c, chauffeurReplId=ReplId-19b9e-5c1b4-c,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true): finished to load
26/01/08 16:07:24 INFO SQLDriverWrapper: Finished setting-up REPL ReplId-19b9e-5c1b4-c, accepting commands (8)
26/01/08 16:07:24 INFO ReplManagerImpl: REPL ReplId-19b9e-5c1b4-d finished addReplToExecutionContext (1)
26/01/08 16:07:24 INFO ColdStartReplFactory: Cold-start repl creation for: sql, ReplInfo(driverReplId=ReplId-19b9e-5c1b4-d, chauffeurReplId=ReplId-19b9e-5c1b4-d,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplOuterWrapper: DriverWrapper created for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b4-d, chauffeurReplId=ReplId-19b9e-5c1b4-d,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplManagerImpl: Adding repl ReplId-19b9e-5c1b4-d to execution context 0 (2)
26/01/08 16:07:24 INFO ReplManagerImpl: StartReplOptions: 
26/01/08 16:07:24 INFO ReplManagerImpl: Skipping snapshot restore for REPL ReplId-19b9e-5c1b4-d because language is sql (only Python REPLs support snapshot restore)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL status transitioned to Starting ReplId-19b9e-5c1b4-d (3)
26/01/08 16:07:24 INFO SQLDriverWrapper: Starting ReplId-19b9e-5c1b4-d - driverStatus transitioned to Starting (4)
26/01/08 16:07:24 INFO SQLDriverWrapper: Driver instantiated for ReplId-19b9e-5c1b4-d (6)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL started but is idle: ReplInfo(driverReplId=ReplId-19b9e-5c1b4-d, chauffeurReplId=ReplId-19b9e-5c1b4-d,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true) (7)
26/01/08 16:07:24 INFO SQLDriverWrapper: setupRepl:ReplInfo(driverReplId=ReplId-19b9e-5c1b4-d, chauffeurReplId=ReplId-19b9e-5c1b4-d,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true): finished to load
26/01/08 16:07:24 INFO SQLDriverWrapper: Finished setting-up REPL ReplId-19b9e-5c1b4-d, accepting commands (8)
26/01/08 16:07:24 INFO ReplManagerImpl: REPL ReplId-19b9e-5c1b4-e finished addReplToExecutionContext (1)
26/01/08 16:07:24 INFO ColdStartReplFactory: Cold-start repl creation for: sql, ReplInfo(driverReplId=ReplId-19b9e-5c1b4-e, chauffeurReplId=ReplId-19b9e-5c1b4-e,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplOuterWrapper: DriverWrapper created for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b4-e, chauffeurReplId=ReplId-19b9e-5c1b4-e,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplManagerImpl: Adding repl ReplId-19b9e-5c1b4-e to execution context 0 (2)
26/01/08 16:07:24 INFO ReplManagerImpl: StartReplOptions: 
26/01/08 16:07:24 INFO ReplManagerImpl: Skipping snapshot restore for REPL ReplId-19b9e-5c1b4-e because language is sql (only Python REPLs support snapshot restore)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL status transitioned to Starting ReplId-19b9e-5c1b4-e (3)
26/01/08 16:07:24 INFO SQLDriverWrapper: Starting ReplId-19b9e-5c1b4-e - driverStatus transitioned to Starting (4)
26/01/08 16:07:24 INFO SQLDriverWrapper: Driver instantiated for ReplId-19b9e-5c1b4-e (6)
26/01/08 16:07:24 INFO SQLDriverWrapper: REPL started but is idle: ReplInfo(driverReplId=ReplId-19b9e-5c1b4-e, chauffeurReplId=ReplId-19b9e-5c1b4-e,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true) (7)
26/01/08 16:07:24 INFO SQLDriverWrapper: setupRepl:ReplInfo(driverReplId=ReplId-19b9e-5c1b4-e, chauffeurReplId=ReplId-19b9e-5c1b4-e,
 executionContextId=Some(ExecutionContextIdV2(0)), lazyInfoInitialized=true): finished to load
26/01/08 16:07:24 INFO SQLDriverWrapper: Finished setting-up REPL ReplId-19b9e-5c1b4-e, accepting commands (8)
26/01/08 16:07:24 INFO ReplManagerImpl: REPL ReplId-19b9e-5c1b4-f finished addReplToExecutionContext (1)
26/01/08 16:07:24 INFO ColdStartReplFactory: Cold-start repl creation for: r, ReplInfo(driverReplId=ReplId-19b9e-5c1b4-f, chauffeurReplId=ReplId-19b9e-5c1b4-f,
 executionContextId=Some(ExecutionContextIdV2(4)), lazyInfoInitialized=true)
26/01/08 16:07:24 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:07:24 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:07:24 INFO log: Logging initialized @49570ms to shaded.v9_4.org.eclipse.jetty.util.log.Slf4jLog
26/01/08 16:07:24 WARN StreamingQueryListenerBus: Failed to capture UCS handle for ListenerBus: org.apache.spark.sql.execution.streaming.runtime.StreamingQueryListenerBus, error: com.databricks.unity.error.MissingCredentialScopeException: [UNITY_CREDENTIAL_SCOPE_MISSING_SCOPE] Missing Credential Scope. Failed to find Unity Credential Scope.. SQLSTATE: XXKUC
26/01/08 16:07:24 WARN StreamingQueryListenerBus: Failed to capture UCS handle for ListenerBus: org.apache.spark.sql.execution.streaming.runtime.StreamingQueryListenerBus, error: com.databricks.unity.error.MissingCredentialScopeException: [UNITY_CREDENTIAL_SCOPE_MISSING_SCOPE] Missing Credential Scope. Failed to find Unity Credential Scope.. SQLSTATE: XXKUC
26/01/08 16:07:24 INFO ReplOuterWrapper: DriverWrapper created for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b4-f, chauffeurReplId=ReplId-19b9e-5c1b4-f,
 executionContextId=Some(ExecutionContextIdV2(4)), lazyInfoInitialized=true)
26/01/08 16:07:24 INFO ReplManagerImpl: Adding repl ReplId-19b9e-5c1b4-f to execution context 4 (2)
26/01/08 16:07:24 INFO ReplManagerImpl: StartReplOptions: 
26/01/08 16:07:24 INFO ReplManagerImpl: Skipping snapshot restore for REPL ReplId-19b9e-5c1b4-f because language is r (only Python REPLs support snapshot restore)
26/01/08 16:07:24 INFO RDriverWrapper: REPL status transitioned to Starting ReplId-19b9e-5c1b4-f (3)
26/01/08 16:07:24 INFO RDriverWrapper: Starting ReplId-19b9e-5c1b4-f - driverStatus transitioned to Starting (4)
26/01/08 16:07:24 INFO ROutputStreamHandler: Connection succeeded on port 41631
26/01/08 16:07:24 INFO ROutputStreamHandler: Connection succeeded on port 40097
26/01/08 16:07:24 INFO RDriverLocal: 1. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: object created with for ReplInfo(driverReplId=ReplId-19b9e-5c1b4-f, chauffeurReplId=ReplId-19b9e-5c1b4-f,
 executionContextId=Some(ExecutionContextIdV2(4)), lazyInfoInitialized=true).
26/01/08 16:07:24 INFO RDriverLocal: 2. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: initializing ...
26/01/08 16:07:24 INFO RDriverLocal: 3. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: started RBackend thread on port 43661
26/01/08 16:07:24 INFO RDriverLocal: 4. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: waiting for SparkR to be installed ...
26/01/08 16:07:33 INFO RDriverLocal$: SparkR installation completed.
26/01/08 16:07:33 INFO RDriverLocal: 5. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: launching R process ...
26/01/08 16:07:33 INFO RDriverLocal: 6. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: cgroup isolation disabled, not placing R process in REPL cgroup.
26/01/08 16:07:33 INFO RDriverLocal: 7. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: starting R process on port 1100 (attempt 1) ...
26/01/08 16:07:33 INFO RDriverLocal$: Debugging command for R process builder: LD_LIBRARY_PATH=/opt/simba/sparkodbc/lib/64/ SPARKR_BACKEND_CONNECTION_TIMEOUT=604800 DB_STREAM_BEACON_STRING_START=DATABRICKS_STREAM_START-ReplId-19b9e-5c1b4-f DB_STDOUT_STREAM_PORT=41631 SPARKR_BACKEND_AUTH_SECRET=[REDACTED] DB_STREAM_BEACON_STRING_END=DATABRICKS_STREAM_END-ReplId-19b9e-5c1b4-f EXISTING_SPARKR_BACKEND_PORT=43661 ODBCINI=/etc/odbc.ini DB_STDERR_STREAM_PORT=40097 SIMBASPARKINI=/etc/simba.sparkodbc.ini R_LIBS=/local_disk0/.ephemeral_nfs/envs/rEnv-0a0c8c9a-a5d0-4f88-9bd8-f3043f4a4053:/databricks/spark/R/lib:/local_disk0/.ephemeral_nfs/cluster_libraries/r /bin/bash /local_disk0/tmp/_startR.sh6116011382814747166resource.r /local_disk0/tmp/_rServeScript.r18419261329944250478resource.r 1100 None
26/01/08 16:07:33 INFO RDriverLocal: 8. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: setting up BufferedStreamThread with bufferSize: 1000.
26/01/08 16:07:35 INFO RDriverLocal: 9. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: R process started with RServe listening on port 1100.
26/01/08 16:07:35 INFO RDriverLocal: 10. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: starting interpreter to talk to R process ...
26/01/08 16:07:36 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
26/01/08 16:07:36 INFO ROutputStreamHandler: Successfully connected to stdout in the RShell.
26/01/08 16:07:36 INFO ROutputStreamHandler: Successfully connected to stderr in the RShell.
26/01/08 16:07:36 INFO RDriverLocal: 11. RDriverLocal.68b49d29-5abe-4619-9fdc-bf50cb9ff785: R interpreter is connected.
26/01/08 16:07:36 INFO RDriverWrapper: Driver instantiated for ReplId-19b9e-5c1b4-f (6)
26/01/08 16:07:36 INFO RDriverWrapper: REPL started but is idle: ReplInfo(driverReplId=ReplId-19b9e-5c1b4-f, chauffeurReplId=ReplId-19b9e-5c1b4-f,
 executionContextId=Some(ExecutionContextIdV2(4)), lazyInfoInitialized=true) (7)
26/01/08 16:07:36 INFO RDriverWrapper: setupRepl:ReplInfo(driverReplId=ReplId-19b9e-5c1b4-f, chauffeurReplId=ReplId-19b9e-5c1b4-f,
 executionContextId=Some(ExecutionContextIdV2(4)), lazyInfoInitialized=true): finished to load
26/01/08 16:07:36 INFO RDriverWrapper: Finished setting-up REPL ReplId-19b9e-5c1b4-f, accepting commands (8)
26/01/08 16:07:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2900fccc size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:07:40 INFO DriverCorral: [Thread 294] AttachLibraries - candidate libraries: List(org/neo4j/neo4j-connector-apache-spark_2.13#5.3.10_for_spark_3.jar, dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar, python-pypi;neo4j;;6.0.2;, python-pypi;python-dotenv;;NONE;)
26/01/08 16:07:40 INFO SparkContext: Library installation started
26/01/08 16:07:40 INFO LibraryResolutionManager: Resolving with preferred maven central mirror https://maven-central.storage-download.googleapis.com/maven2/
26/01/08 16:07:51 INFO SharedDriverContext: Register resolved dependencies: Map(org/neo4j/neo4j-connector-apache-spark_2.13#5.3.10_for_spark_3.jar -> List(file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar))
26/01/08 16:07:51 INFO DriverCorral: [Thread 294] AttachLibraries - new libraries to install (including resolved dependencies): List(org/neo4j/neo4j-connector-apache-spark_2.13#5.3.10_for_spark_3.jar, dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar, python-pypi;neo4j;;6.0.2;, python-pypi;python-dotenv;;NONE;, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar, file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar)
26/01/08 16:07:51 INFO SharedDriverContext: [Thread 294] attachLibrariesToSpark org/neo4j/neo4j-connector-apache-spark_2.13#5.3.10_for_spark_3.jar
26/01/08 16:07:51 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar to Spark
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar
26/01/08 16:07:51 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar is downloaded
26/01/08 16:07:51 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar as local file /local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar in 107 milliseconds
26/01/08 16:07:51 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar to local file /local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar
26/01/08 16:07:51 INFO SparkContext: Added file /local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar at file:/local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar with timestamp 1767888471559
26/01/08 16:07:51 INFO Utils: Copying /local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar
26/01/08 16:07:51 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar at (spark://10.139.64.4:40407/jars/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar,Some(/local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar)) with timestamp 1767888471591
26/01/08 16:07:51 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar to Spark
26/01/08 16:07:51 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-cypher-dsl-2022.11.0.jar
26/01/08 16:07:51 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar to Spark
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar
26/01/08 16:07:51 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar is downloaded
26/01/08 16:07:51 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar as local file /local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar in 5 milliseconds
26/01/08 16:07:51 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar to local file /local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar
26/01/08 16:07:51 INFO SparkContext: Added file /local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar at file:/local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar with timestamp 1767888471658
26/01/08 16:07:51 INFO Utils: Copying /local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar
26/01/08 16:07:51 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar at (spark://10.139.64.4:40407/jars/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar,Some(/local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar)) with timestamp 1767888471686
26/01/08 16:07:51 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar to Spark
26/01/08 16:07:51 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains.kotlin_kotlin-stdlib-2.1.20.jar
26/01/08 16:07:51 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar to Spark
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar
26/01/08 16:07:51 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar is downloaded
26/01/08 16:07:51 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar as local file /local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar in 2 milliseconds
26/01/08 16:07:51 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar to local file /local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar
26/01/08 16:07:51 INFO SparkContext: Added file /local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar at file:/local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar with timestamp 1767888471765
26/01/08 16:07:51 INFO Utils: Copying /local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_resolver_4_1_127_Final.jar
26/01/08 16:07:51 INFO SparkContext: Added JAR /local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_resolver_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar)) with timestamp 1767888471821
26/01/08 16:07:51 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar to Spark
26/01/08 16:07:51 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-resolver-4.1.127.Final.jar
26/01/08 16:07:51 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar to Spark
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar
26/01/08 16:07:51 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar is downloaded
26/01/08 16:07:51 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar as local file /local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar in 6 milliseconds
26/01/08 16:07:51 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar to local file /local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar
26/01/08 16:07:51 INFO SparkContext: Added file /local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar at file:/local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar with timestamp 1767888471901
26/01/08 16:07:51 INFO Utils: Copying /local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_common_4_1_127_Final.jar
26/01/08 16:07:51 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_common_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar)) with timestamp 1767888471969
26/01/08 16:07:51 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar to Spark
26/01/08 16:07:51 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-common-4.1.127.Final.jar
26/01/08 16:07:51 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar to Spark
26/01/08 16:07:51 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar is downloaded
26/01/08 16:07:52 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar as local file /local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar in 4 milliseconds
26/01/08 16:07:52 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar to local file /local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar
26/01/08 16:07:52 INFO SparkContext: Added file /local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar at file:/local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar with timestamp 1767888472041
26/01/08 16:07:52 INFO Utils: Copying /local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_slf4j_slf4j_api_2_0_17.jar
26/01/08 16:07:52 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar at (spark://10.139.64.4:40407/jars/org_slf4j_slf4j_api_2_0_17.jar,Some(/local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar)) with timestamp 1767888472095
26/01/08 16:07:52 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar to Spark
26/01/08 16:07:52 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.slf4j_slf4j-api-2.0.17.jar
26/01/08 16:07:52 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar to Spark
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar is downloaded
26/01/08 16:07:52 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar as local file /local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar in 11 milliseconds
26/01/08 16:07:52 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar to local file /local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar
26/01/08 16:07:52 INFO SparkContext: Added file /local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar at file:/local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar with timestamp 1767888472210
26/01/08 16:07:52 INFO Utils: Copying /local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar
26/01/08 16:07:52 INFO SparkContext: Added JAR /local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar at (spark://10.139.64.4:40407/jars/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar,Some(/local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar)) with timestamp 1767888472261
26/01/08 16:07:52 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar to Spark
26/01/08 16:07:52 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13-5.3.10_for_spark_3.jar
26/01/08 16:07:52 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar to Spark
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar is downloaded
26/01/08 16:07:52 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar as local file /local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar in 3 milliseconds
26/01/08 16:07:52 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar to local file /local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar
26/01/08 16:07:52 INFO SparkContext: Added file /local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar at file:/local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar with timestamp 1767888472360
26/01/08 16:07:52 INFO Utils: Copying /local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar
26/01/08 16:07:52 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar at (spark://10.139.64.4:40407/jars/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar,Some(/local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar)) with timestamp 1767888472412
26/01/08 16:07:52 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar to Spark
26/01/08 16:07:52 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.driver_neo4j-java-driver-slim-4.4.21.jar
26/01/08 16:07:52 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar to Spark
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar is downloaded
26/01/08 16:07:52 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar as local file /local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar in 3 milliseconds
26/01/08 16:07:52 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar to local file /local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar
26/01/08 16:07:52 INFO SparkContext: Added file /local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar at file:/local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar with timestamp 1767888472528
26/01/08 16:07:52 INFO Utils: Copying /local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_codec_4_1_127_Final.jar
26/01/08 16:07:52 INFO SparkContext: Added JAR /local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_codec_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar)) with timestamp 1767888472581
26/01/08 16:07:52 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar to Spark
26/01/08 16:07:52 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-codec-4.1.127.Final.jar
26/01/08 16:07:52 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar to Spark
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar is downloaded
26/01/08 16:07:52 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar as local file /local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar in 5 milliseconds
26/01/08 16:07:52 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar to local file /local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar
26/01/08 16:07:52 INFO SparkContext: Added file /local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar at file:/local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar with timestamp 1767888472690
26/01/08 16:07:52 INFO Utils: Copying /local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_neo4j_detection_1_3_0.jar
26/01/08 16:07:52 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar at (spark://10.139.64.4:40407/jars/org_neo4j_caniuse_neo4j_detection_1_3_0.jar,Some(/local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar)) with timestamp 1767888472743
26/01/08 16:07:52 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar to Spark
26/01/08 16:07:52 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-neo4j-detection-1.3.0.jar
26/01/08 16:07:52 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar to Spark
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar is downloaded
26/01/08 16:07:52 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar as local file /local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar in 3 milliseconds
26/01/08 16:07:52 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar to local file /local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar
26/01/08 16:07:52 INFO SparkContext: Added file /local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar at file:/local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar with timestamp 1767888472856
26/01/08 16:07:52 INFO Utils: Copying /local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_transport_4_1_127_Final.jar
26/01/08 16:07:52 INFO SparkContext: Added JAR /local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_transport_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar)) with timestamp 1767888472913
26/01/08 16:07:52 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar to Spark
26/01/08 16:07:52 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-4.1.127.Final.jar
26/01/08 16:07:52 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar to Spark
26/01/08 16:07:52 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar
26/01/08 16:07:52 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar is downloaded
26/01/08 16:07:52 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar as local file /local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar in 4 milliseconds
26/01/08 16:07:53 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar to local file /local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar
26/01/08 16:07:53 INFO SparkContext: Added file /local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar at file:/local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar with timestamp 1767888473002
26/01/08 16:07:53 INFO Utils: Copying /local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_handler_4_1_127_Final.jar
26/01/08 16:07:53 INFO SparkContext: Added JAR /local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_handler_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar)) with timestamp 1767888473031
26/01/08 16:07:53 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar to Spark
26/01/08 16:07:53 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-handler-4.1.127.Final.jar
26/01/08 16:07:53 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar to Spark
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar is downloaded
26/01/08 16:07:53 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar as local file /local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar in 4 milliseconds
26/01/08 16:07:53 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar to local file /local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar
26/01/08 16:07:53 INFO SparkContext: Added file /local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar at file:/local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar with timestamp 1767888473146
26/01/08 16:07:53 INFO Utils: Copying /local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar
26/01/08 16:07:53 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar at (spark://10.139.64.4:40407/jars/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar,Some(/local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar)) with timestamp 1767888473189
26/01/08 16:07:53 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar to Spark
26/01/08 16:07:53 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_neo4j-connector-apache-spark_2.13_common-5.3.10_for_spark_3.jar
26/01/08 16:07:53 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar to Spark
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar is downloaded
26/01/08 16:07:53 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar as local file /local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar in 2 milliseconds
26/01/08 16:07:53 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar to local file /local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar
26/01/08 16:07:53 INFO SparkContext: Added file /local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar at file:/local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar with timestamp 1767888473287
26/01/08 16:07:53 INFO Utils: Copying /local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar
26/01/08 16:07:53 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar at (spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar,Some(/local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar)) with timestamp 1767888473343
26/01/08 16:07:53 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar to Spark
26/01/08 16:07:53 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-spi-1.0.0-rc2.jar
26/01/08 16:07:53 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar to Spark
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar is downloaded
26/01/08 16:07:53 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar as local file /local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar in 4 milliseconds
26/01/08 16:07:53 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar to local file /local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar
26/01/08 16:07:53 INFO SparkContext: Added file /local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar at file:/local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar with timestamp 1767888473459
26/01/08 16:07:53 INFO Utils: Copying /local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_apiguardian_apiguardian_api_1_1_2.jar
26/01/08 16:07:53 INFO SparkContext: Added JAR /local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar at (spark://10.139.64.4:40407/jars/org_apiguardian_apiguardian_api_1_1_2.jar,Some(/local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar)) with timestamp 1767888473518
26/01/08 16:07:53 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar to Spark
26/01/08 16:07:53 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.apiguardian_apiguardian-api-1.1.2.jar
26/01/08 16:07:53 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar to Spark
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar is downloaded
26/01/08 16:07:53 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar as local file /local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar in 3 milliseconds
26/01/08 16:07:53 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar to local file /local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar
26/01/08 16:07:53 INFO SparkContext: Added file /local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar at file:/local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar with timestamp 1767888473619
26/01/08 16:07:53 INFO Utils: Copying /local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_reactivestreams_reactive_streams_1_0_4.jar
26/01/08 16:07:53 INFO SparkContext: Added JAR /local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar at (spark://10.139.64.4:40407/jars/org_reactivestreams_reactive_streams_1_0_4.jar,Some(/local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar)) with timestamp 1767888473682
26/01/08 16:07:53 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar to Spark
26/01/08 16:07:53 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.reactivestreams_reactive-streams-1.0.4.jar
26/01/08 16:07:53 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar to Spark
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar is downloaded
26/01/08 16:07:53 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar as local file /local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar in 4 milliseconds
26/01/08 16:07:53 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar to local file /local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar
26/01/08 16:07:53 INFO SparkContext: Added file /local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar at file:/local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar with timestamp 1767888473757
26/01/08 16:07:53 INFO Utils: Copying /local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_buffer_4_1_127_Final.jar
26/01/08 16:07:53 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_buffer_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar)) with timestamp 1767888473828
26/01/08 16:07:53 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar to Spark
26/01/08 16:07:53 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-buffer-4.1.127.Final.jar
26/01/08 16:07:53 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar to Spark
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar is downloaded
26/01/08 16:07:53 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar
26/01/08 16:07:53 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar as local file /local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar in 11 milliseconds
26/01/08 16:07:53 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar to local file /local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar
26/01/08 16:07:53 INFO SparkContext: Added file /local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar at file:/local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar with timestamp 1767888473942
26/01/08 16:07:53 INFO Utils: Copying /local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_projectreactor_reactor_core_3_6_11.jar
26/01/08 16:07:54 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar at (spark://10.139.64.4:40407/jars/io_projectreactor_reactor_core_3_6_11.jar,Some(/local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar)) with timestamp 1767888474005
26/01/08 16:07:54 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar to Spark
26/01/08 16:07:54 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.projectreactor_reactor-core-3.6.11.jar
26/01/08 16:07:54 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar to Spark
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar is downloaded
26/01/08 16:07:54 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar as local file /local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar in 8 milliseconds
26/01/08 16:07:54 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar to local file /local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar
26/01/08 16:07:54 INFO SparkContext: Added file /local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar at file:/local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar with timestamp 1767888474093
26/01/08 16:07:54 INFO Utils: Copying /local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar
26/01/08 16:07:54 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar)) with timestamp 1767888474155
26/01/08 16:07:54 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar to Spark
26/01/08 16:07:54 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-transport-native-unix-common-4.1.127.Final.jar
26/01/08 16:07:54 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar to Spark
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar is downloaded
26/01/08 16:07:54 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar as local file /local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar in 3 milliseconds
26/01/08 16:07:54 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar to local file /local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar
26/01/08 16:07:54 INFO SparkContext: Added file /local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar at file:/local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar with timestamp 1767888474235
26/01/08 16:07:54 INFO Utils: Copying /local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar
26/01/08 16:07:54 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar at (spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar,Some(/local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar)) with timestamp 1767888474307
26/01/08 16:07:54 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar to Spark
26/01/08 16:07:54 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-reauth-driver-1.0.0-rc2.jar
26/01/08 16:07:54 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar to Spark
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar is downloaded
26/01/08 16:07:54 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar as local file /local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar in 3 milliseconds
26/01/08 16:07:54 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar to local file /local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar
26/01/08 16:07:54 INFO SparkContext: Added file /local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar at file:/local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar with timestamp 1767888474396
26/01/08 16:07:54 INFO Utils: Copying /local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar
26/01/08 16:07:54 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar at (spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar,Some(/local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar)) with timestamp 1767888474441
26/01/08 16:07:54 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar to Spark
26/01/08 16:07:54 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j.connectors_commons-authn-provided-1.0.0-rc2.jar
26/01/08 16:07:54 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar to Spark
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar
26/01/08 16:07:54 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 79596
 Duration: 33
26/01/08 16:07:54 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar is downloaded
26/01/08 16:07:54 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar as local file /local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar in 3 milliseconds
26/01/08 16:07:54 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar to local file /local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar
26/01/08 16:07:54 INFO SparkContext: Added file /local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar at file:/local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar with timestamp 1767888474563
26/01/08 16:07:54 INFO Utils: Copying /local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_api_1_3_0.jar
26/01/08 16:07:54 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar at (spark://10.139.64.4:40407/jars/org_neo4j_caniuse_api_1_3_0.jar,Some(/local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar)) with timestamp 1767888474623
26/01/08 16:07:54 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar to Spark
26/01/08 16:07:54 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-api-1.3.0.jar
26/01/08 16:07:54 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar to Spark
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar is downloaded
26/01/08 16:07:54 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar as local file /local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar in 4 milliseconds
26/01/08 16:07:54 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar to local file /local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar
26/01/08 16:07:54 INFO SparkContext: Added file /local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar at file:/local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar with timestamp 1767888474705
26/01/08 16:07:54 INFO Utils: Copying /local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_tcnative_classes_2_0_73_Final.jar
26/01/08 16:07:54 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_tcnative_classes_2_0_73_Final.jar,Some(/local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar)) with timestamp 1767888474758
26/01/08 16:07:54 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar to Spark
26/01/08 16:07:54 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/io.netty_netty-tcnative-classes-2.0.73.Final.jar
26/01/08 16:07:54 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar to Spark
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar is downloaded
26/01/08 16:07:54 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar as local file /local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar in 3 milliseconds
26/01/08 16:07:54 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar to local file /local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar
26/01/08 16:07:54 INFO SparkContext: Added file /local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar at file:/local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar with timestamp 1767888474824
26/01/08 16:07:54 INFO Utils: Copying /local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_jetbrains_annotations_13_0.jar
26/01/08 16:07:54 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar at (spark://10.139.64.4:40407/jars/org_jetbrains_annotations_13_0.jar,Some(/local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar)) with timestamp 1767888474885
26/01/08 16:07:54 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar to Spark
26/01/08 16:07:54 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.jetbrains_annotations-13.0.jar
26/01/08 16:07:54 INFO SharedDriverContext: Attaching lib: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar to Spark
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloading a library that was not in the cache: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Attempt 1: wait until library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar is downloaded
26/01/08 16:07:54 INFO LibraryDownloadManager: Preparing to download library file using the default access mechanism: file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar
26/01/08 16:07:54 INFO LibraryDownloadManager: Downloaded library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar as local file /local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar in 2 milliseconds
26/01/08 16:07:54 INFO SharedDriverContext: Successfully saved library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar to local file /local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar
26/01/08 16:07:54 INFO SparkContext: Added file /local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar at file:/local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar with timestamp 1767888474971
26/01/08 16:07:54 INFO Utils: Copying /local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_core_1_3_0.jar
26/01/08 16:07:55 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar at (spark://10.139.64.4:40407/jars/org_neo4j_caniuse_core_1_3_0.jar,Some(/local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar)) with timestamp 1767888475029
26/01/08 16:07:55 INFO SharedDriverContext: Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar to Spark
26/01/08 16:07:55 INFO LibraryState: [Thread 294] Successfully attached library file:/local_disk0/tmp/clusterWideResolutionDir/maven/ivy/jars/org.neo4j_caniuse-core-1.3.0.jar
26/01/08 16:07:55 INFO LibraryState: [Thread 294] Successfully attached library maven;neo4j-connector-apache-spark_2.13;org.neo4j;5.3.10_for_spark_3;;
26/01/08 16:07:55 INFO SharedDriverContext: [Thread 294] attachLibrariesToSpark dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar
26/01/08 16:07:55 INFO SharedDriverContext: Attaching lib: dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar to Spark
26/01/08 16:07:55 INFO LibraryDownloadManager: Downloading a library that was not in the cache: dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar
26/01/08 16:07:55 INFO LibraryDownloadManager: Attempt 1: wait until library dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar is downloaded
26/01/08 16:07:55 INFO LibraryDownloadManager: Preparing to download library file from UC Volume path: dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar
26/01/08 16:07:56 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
26/01/08 16:07:57 ERROR AbfsClient: HttpRequest: 403,err=,appendpos=,cid=0104-134656-pvz3m00y------:17fdd8ef-7e35-4ef1-8370-49ae7abeda04:1a7528be-127f-4a38-a1fb-066e4446a797:::GF:0,rid=21785996-901f-0037-23b8-80222a000000,connMs=2,sendMs=0,recvMs=39,sent=0,recv=0,method=HEAD,https://dbstorageolrfaaff6x6nu.dfs.core.windows.net/unity-catalog-storage/_encryption_meta/manifest.json?upn=false&action=getStatus&timeout=90&st=2026-01-08T15:57:56Z&sv=2020-02-10&ske=2026-01-08T17:57:56Z&sig=XXXXX&sktid=54e85725-ed2a-49a4-a19e-11c8d29f9a0f&se=2026-01-08T17:07:56Z&sdd=7&skoid=b0a1dcb0-4ca9-4b42XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2026-01-08T15:57:56Z&sp=rl&skv=2025-01-05&sr=d
26/01/08 16:07:57 WARN FallbackEncryptionContextProvider: Accessing the manifest file failed with 403.
26/01/08 16:07:57 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(686ca54d)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604/jdbc_drivers/__unitystorage/schemas/b8494da4-84bb-491e-8f20-8be304588f99/volumes/dca69f58-0089-490e-8659-549c43d0ccc6 with credential = CredentialScopeADLSTokenProvider with jvmId = 656
26/01/08 16:07:57 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
26/01/08 16:07:57 INFO NativeADLGen2RequestComparisonHandler: Native - hadoop request mismatch: Request type: FetchRange$
Extra params in native request: [st=2026-01-08t15%3a57%3a56z&sv=2020-02-10&ske=2026-01-08t17%3a57%3a56z&sig=REDACTED_POSSIBLE_SECRET_ACCESS_KEY%2fey%3d&sktid=54e85725-ed2a-49a4-a19e-11c8d29f9a0f&se=2026-01-08t17%3a07%3a56z&sdd=7&skoid=b0a1dcb0-4ca9-4b42-995f-82e66e87d647&spr=https&sks=b&skt=2026-01-08t15%3a57%3a56z&sp=rl&skv=2025-01-05&sr=d]
Extra params in java request: [st,sv,ske,sig,sktid,sdd,se,skoid,spr,sks,skt,sp,sr,skv]

26/01/08 16:07:58 INFO LibraryDownloadManager: Downloaded library dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar as local file /local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar in 2774 milliseconds
26/01/08 16:07:58 INFO SharedDriverContext: Successfully saved library dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar to local file /local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar
26/01/08 16:07:58 INFO SparkContext: Added file /local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar at file:/local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar with timestamp 1767888478251
26/01/08 16:07:58 INFO Utils: Copying /local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/neo4j_jdbc_full_bundle.jar
26/01/08 16:07:58 INFO SparkContext: Added JAR /local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar at (spark://10.139.64.4:40407/jars/neo4j_jdbc_full_bundle.jar,Some(/local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar)) with timestamp 1767888478283
26/01/08 16:07:58 INFO SharedDriverContext: Successfully attached library dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar to Spark
26/01/08 16:07:58 INFO LibraryState: [Thread 294] Successfully attached library dbfs:/Volumes/uc-w-neo4j/jdbc_drivers/jars/neo4j-jdbc-full-bundle.jar
26/01/08 16:07:58 INFO SparkContext: Library installation completed
26/01/08 16:07:58 INFO SharedDriverContext: [Thread 294] attachLibrariesToSpark python-pypi;neo4j;;6.0.2;
26/01/08 16:07:58 INFO SharedDriverContext: Attaching Python lib: python-pypi;neo4j;;6.0.2; to clusterwide nfs path
26/01/08 16:07:58 INFO Utils: resolved command to be run: List(/bin/su, libraries, -c, bash /local_disk0/.ephemeral_nfs/cluster_libraries/python/python_start_clusterwide.sh /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/pip install 'neo4j==6.0.2' --disable-pip-version-check)
26/01/08 16:08:00 INFO LibraryUtils$: Successfully installed library neo4j==6.0.2 on cluster with output:
Collecting neo4j==6.0.2
  Downloading neo4j-6.0.2-py3-none-any.whl.metadata (5.2 kB)
Requirement already satisfied: pytz in /databricks/python/lib/python3.12/site-packages (from neo4j==6.0.2) (2024.1)
Downloading neo4j-6.0.2-py3-none-any.whl (325 kB)
Installing collected packages: neo4j
Successfully installed neo4j-6.0.2

26/01/08 16:08:00 ERROR CommandLineHelper$: Command [REDACTED] failed with exit code 1 out: err:
java.lang.RuntimeException: CommandLineHelper exception - stack trace
	at com.databricks.backend.common.util.CommandLineHelper$.runJavaProcessBuilderSafe(CommandLineHelper.scala:485)
	at com.databricks.backend.daemon.driver.SharedDriverContext.cleanLowPrivilegedUserProcesses(SharedDriverContext.scala:858)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withLowPrivilegedProcessCleanup(SharedDriverContext.scala:834)
	at com.databricks.backend.daemon.driver.SharedDriverContext.installLibraryOnDriverNfs(SharedDriverContext.scala:806)
	at com.databricks.backend.daemon.driver.SharedDriverContext.installLibrary(SharedDriverContext.scala:716)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibraryToSpark$1(SharedDriverContext.scala:737)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.threading.TimeLimiter$.runInCallingThreadWithTimeout(TimeLimiter.scala:110)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibraryToSpark(SharedDriverContext.scala:736)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$3(SharedDriverContext.scala:662)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:133)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:133)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperationWithResultTags(SharedDriverContext.scala:133)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:133)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:651)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:639)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:639)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$attachLibraries$4(DriverCorral.scala:906)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$attachLibraries$3(DriverCorral.scala:905)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$attachLibraries$3$adapted(DriverCorral.scala:905)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibraries(DriverCorral.scala:905)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibraries(DriverCorral.scala:2165)
	at com.databricks.backend.daemon.driver.DriverCorralCompatServerBackend.$anonfun$handlers$33(DriverCorralCompatServerBackend.scala:150)
	at com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:470)
	at com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:413)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:618)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:596)
	at com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)
	at com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:594)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)
	at grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)
	at com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)
	at com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:08:00 INFO SharedDriverContext: Successfully attached library python-pypi;neo4j;;6.0.2; to Spark
26/01/08 16:08:00 INFO LibraryState: [Thread 294] Successfully attached library python-pypi;neo4j;;6.0.2;
26/01/08 16:08:00 INFO SharedDriverContext: [Thread 294] attachLibrariesToSpark python-pypi;python-dotenv;;NONE;
26/01/08 16:08:00 INFO SharedDriverContext: Attaching Python lib: python-pypi;python-dotenv;;NONE; to clusterwide nfs path
26/01/08 16:08:00 INFO Utils: resolved command to be run: List(/bin/su, libraries, -c, bash /local_disk0/.ephemeral_nfs/cluster_libraries/python/python_start_clusterwide.sh /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/pip install 'python-dotenv' --disable-pip-version-check)
26/01/08 16:08:02 INFO LibraryUtils$: Successfully installed library python-dotenv on cluster with output:
Collecting python-dotenv
  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)
Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)
Installing collected packages: python-dotenv
Successfully installed python-dotenv-1.2.1

26/01/08 16:08:02 ERROR CommandLineHelper$: Command [REDACTED] failed with exit code 1 out: err:
java.lang.RuntimeException: CommandLineHelper exception - stack trace
	at com.databricks.backend.common.util.CommandLineHelper$.runJavaProcessBuilderSafe(CommandLineHelper.scala:485)
	at com.databricks.backend.daemon.driver.SharedDriverContext.cleanLowPrivilegedUserProcesses(SharedDriverContext.scala:858)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withLowPrivilegedProcessCleanup(SharedDriverContext.scala:834)
	at com.databricks.backend.daemon.driver.SharedDriverContext.installLibraryOnDriverNfs(SharedDriverContext.scala:806)
	at com.databricks.backend.daemon.driver.SharedDriverContext.installLibrary(SharedDriverContext.scala:716)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibraryToSpark$1(SharedDriverContext.scala:737)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.threading.TimeLimiter$.runInCallingThreadWithTimeout(TimeLimiter.scala:110)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibraryToSpark(SharedDriverContext.scala:736)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$3(SharedDriverContext.scala:662)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionContext(SharedDriverContext.scala:133)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.backend.daemon.driver.SharedDriverContext.withAttributionTags(SharedDriverContext.scala:133)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperationWithResultTags(SharedDriverContext.scala:133)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.backend.daemon.driver.SharedDriverContext.recordOperation(SharedDriverContext.scala:133)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1(SharedDriverContext.scala:651)
	at com.databricks.backend.daemon.driver.SharedDriverContext.$anonfun$attachLibrariesToSpark$1$adapted(SharedDriverContext.scala:639)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at com.databricks.backend.daemon.driver.SharedDriverContext.attachLibrariesToSpark(SharedDriverContext.scala:639)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$attachLibraries$4(DriverCorral.scala:906)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$attachLibraries$3(DriverCorral.scala:905)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$attachLibraries$3$adapted(DriverCorral.scala:905)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibraries(DriverCorral.scala:905)
	at com.databricks.backend.daemon.driver.DriverCorral.attachLibraries(DriverCorral.scala:2165)
	at com.databricks.backend.daemon.driver.DriverCorralCompatServerBackend.$anonfun$handlers$33(DriverCorralCompatServerBackend.scala:150)
	at com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:470)
	at com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:413)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:618)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:596)
	at com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)
	at com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:594)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)
	at grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)
	at com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)
	at com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:08:02 INFO SharedDriverContext: Successfully attached library python-pypi;python-dotenv;;NONE; to Spark
26/01/08 16:08:02 INFO LibraryState: [Thread 294] Successfully attached library python-pypi;python-dotenv;;NONE;
26/01/08 16:08:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@439df449 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:08:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:08:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3ebdbff1 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:09:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@72ab9198 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:09:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:09:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@5ce3e59d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:10:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@70f07df4 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:10:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:10:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@23e1162c size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:11:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@de87499 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:11:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:11:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4ee4a7b4 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:11:51 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:11:51 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:11:51 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:11:51 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:11:51 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:11:51 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 269 milliseconds)
26/01/08 16:11:51 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:11:51 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:11:51 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:11:51 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:11:51 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:11:51 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 237 milliseconds)
26/01/08 16:11:58 WARN SparkContext: Requesting executors is not supported by current scheduler.
26/01/08 16:11:58 INFO DeadlockDetector: Requested deadlock detection caused by: DAG_SCHEDULER_NO_ACTIVE_JOB
26/01/08 16:11:58 INFO HangingThreadDetector: Requested hanging thread detection caused by: DAG_SCHEDULER_NO_ACTIVE_JOB
26/01/08 16:12:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3d886c41 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:12:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:12:18 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:12:19 INFO InstanceMetadataServiceHelper$: Failed to read AWS instance metadata service document. Assuming not in AWS.
com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/dynamic/instance-identity/document
	at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:108)
	at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:70)
	at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:81)
	at com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:62)
	at com.databricks.s3a.logging.InstanceMetadataServiceHelper$.$anonfun$isAws$1(InstanceMetadataServiceHelper.scala:24)
	at scala.util.Try$.apply(Try.scala:217)
	at com.databricks.s3a.logging.InstanceMetadataServiceHelper$.isAws$lzycompute(InstanceMetadataServiceHelper.scala:22)
	at com.databricks.s3a.logging.InstanceMetadataServiceHelper$.isAws(InstanceMetadataServiceHelper.scala:18)
	at com.databricks.backend.common.util.HadoopFSUtil$.setDefaultS3Configuration(HadoopFSUtil.scala:165)
	at com.databricks.backend.common.util.HadoopFSUtil$.createConfiguration(HadoopFSUtil.scala:138)
	at com.databricks.backend.common.util.HadoopFSUtil$.createConfiguration(HadoopFSUtil.scala:55)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2Factory.getHadoopConfiguration(DatabricksFileSystemV2Factory.scala:184)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2Factory.createFileSystem(DatabricksFileSystemV2Factory.scala:54)
	at com.databricks.backend.daemon.data.filesystem.MountEntryResolver.$anonfun$getOrCreateFileSystem$1(MountEntryResolver.scala:109)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.common.util.locks.LoggedLock$.withAttributionContext(LoggedLock.scala:89)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.common.util.locks.LoggedLock$.withAttributionTags(LoggedLock.scala:89)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.common.util.locks.LoggedLock$.recordOperationWithResultTags(LoggedLock.scala:89)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.common.util.locks.LoggedLock$.recordOperation(LoggedLock.scala:89)
	at com.databricks.common.util.locks.LoggedLock$.withLock(LoggedLock.scala:162)
	at com.databricks.common.util.locks.PerKeyLock.withLock(PerKeyLock.scala:42)
	at com.databricks.backend.daemon.data.filesystem.MountEntryResolver.getOrCreateFileSystem(MountEntryResolver.scala:106)
	at com.databricks.backend.daemon.data.filesystem.MountEntryResolver.createFileSystem(MountEntryResolver.scala:133)
	at com.databricks.backend.daemon.data.client.DBFSV2.resolveAndGetFileSystem(DatabricksFileSystemV2.scala:158)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.resolve(DatabricksFileSystemV2.scala:798)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1225)
	at com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)
	at com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$4(LokiFileSystem.scala:317)
	at com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:297)
	at com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$1(LokiFileSystem.scala:317)
	at com.databricks.sql.io.LokiFileSystem$.withWrappedExceptions(LokiFileSystem.scala:156)
	at com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:317)
	at com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:323)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$2(DriverCorral.scala:345)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$3(NamedTimer.scala:139)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:127)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$2(NamedTimer.scala:136)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$1(NamedTimer.scala:134)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:109)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:134)
	at java.base/java.util.TimerThread.mainLoop(Timer.java:566)
	at java.base/java.util.TimerThread.run(Timer.java:516)
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:12:19 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 16:12:19 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 656
26/01/08 16:12:19 INFO DriverCorral: DBFS health check ok
26/01/08 16:12:19 INFO SecuredHiveExternalCatalog: creating hiveClient from java.lang.Throwable
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:87)
	at org.apache.spark.sql.hive.HiveExternalCatalog.maybeSynchronized(HiveExternalCatalog.scala:127)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$1(HiveExternalCatalog.scala:186)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:457)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:186)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:420)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:426)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:416)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$13(DriverCorral.scala:581)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$12(DriverCorral.scala:581)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$12$adapted(DriverCorral.scala:580)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$11(DriverCorral.scala:580)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$3(NamedTimer.scala:139)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:127)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$2(NamedTimer.scala:136)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$1(NamedTimer.scala:134)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:109)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:134)
	at java.base/java.util.TimerThread.mainLoop(Timer.java:566)
	at java.base/java.util.TimerThread.run(Timer.java:516)

26/01/08 16:12:19 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:12:19 INFO HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using paths: file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__4.1.19.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--javax.transaction--jta--javax.transaction__jta__1.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.logging.log4j--log4j-api--org.apache.logging.log4j__log4j-api__2.19.0.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--com.zaxxer--HikariCP--com.zaxxer__HikariCP__2.5.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.0.5.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--com.google.guava--guava--com.google.guava__guava__11.0.2.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--commons-io--commons-io--commons-io__commons-io__2.5.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-common-secure--org.apache.hive.shims__hive-shims-common-secure__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-service--org.apache.hive__hive-service__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-exec--org.apache.hive__hive-exec__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--jline--jline--jline__jline__0.9.94.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-cli--org.apache.hive__hive-cli__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--javax.jdo--org.datanucleus__javax.jdo__3.2.0-m3.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.avro--avro--org.apache.avro__avro__1.7.5.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.reflectasm--reflectasm--com.esotericsoftware.reflectasm__reflectasm__1.07-shaded.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--junit--junit--junit__junit__3.8.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__1.3.9.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-common--org.apache.hive__hive-common__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.logging.log4j--log4j-1.2-api--org.apache.logging.log4j__log4j-1.2-api__2.19.0.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.20--org.apache.hive.shims__hive-shims-0.20__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--antlr--antlr--antlr__antlr__2.7.7.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--stax--stax-api--stax__stax-api__1.0.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.ant--ant--org.apache.ant__ant__1.9.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.slf4j--slf4j-api--org.slf4j__slf4j-api__2.0.6.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--oro--oro--oro__oro__2.0.8.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-ant--org.apache.hive__hive-ant__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--commons-lang--commons-lang--commons-lang__commons-lang__2.4.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-metastore--org.apache.hive__hive-metastore__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-serde--org.apache.hive__hive-serde__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.logging.log4j--log4j-slf4j2-impl--org.apache.logging.log4j__log4j-slf4j2-impl__2.19.0.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.antlr--ST4--org.antlr__ST4__4.0.4.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.5.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.20S--org.apache.hive.shims__hive-shims-0.20S__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.logging.log4j--log4j-core--org.apache.logging.log4j__log4j-core__2.19.0.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-shims--org.apache.hive__hive-shims__0.13.1-databricks-10.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.0.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar, file:/databricks/databricks-hive/manifest.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.1.jar, file:/databricks/databricks-hive/----ws_4_0--maven-trees--hive-metastore-databricks-log4j2--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar, file:/databricks/databricks-hive/bonecp-configs.jar
26/01/08 16:12:19 INFO PoolingHiveClient: Hive metastore connection pool implementation is HikariCP
26/01/08 16:12:19 INFO LocalHiveClientsPool: Create Hive Metastore client pool of size 20
26/01/08 16:12:19 INFO HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is dbfs:/user/hive/warehouse
26/01/08 16:12:19 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
26/01/08 16:12:19 INFO ObjectStore: ObjectStore, initialize called
26/01/08 16:12:19 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored
26/01/08 16:12:19 INFO Persistence: Property datanucleus.connectionPool.idleTimeout unknown - will be ignored
26/01/08 16:12:19 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
26/01/08 16:12:19 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
26/01/08 16:12:19 INFO HikariDataSource: HikariPool-1 - Started.
26/01/08 16:12:20 INFO HikariDataSource: HikariPool-2 - Started.
26/01/08 16:12:20 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
26/01/08 16:12:22 INFO ObjectStore: Initialized ObjectStore
26/01/08 16:12:22 INFO HiveMetaStore: Added admin role in metastore
26/01/08 16:12:22 INFO HiveMetaStore: Added public role in metastore
26/01/08 16:12:22 INFO HiveMetaStore: No user is added in admin role, since config is empty
26/01/08 16:12:22 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:12:22 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:12:22 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:12:22 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:12:22 INFO DriverCorral: Metastore health check ok
26/01/08 16:12:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@16734daa size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:13:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@58f1af41 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:13:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:13:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@33437aec size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:14:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4dd7f2a1 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:14:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:14:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@41c5f8ac size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:15:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@149bdc72 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:15:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:15:23 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 528451
 Duration: 60
26/01/08 16:15:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3c274017 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:16:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4e97efc8 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:16:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:16:26 INFO ReplManagerImpl: REPL ReplId-19b9e-5c1b5-0 finished addReplToExecutionContext (1)
26/01/08 16:16:26 INFO ColdStartReplFactory: Cold-start repl creation for: python, ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true)
26/01/08 16:16:26 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:16:26 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
26/01/08 16:16:26 WARN StreamingQueryListenerBus: Failed to capture UCS handle for ListenerBus: org.apache.spark.sql.execution.streaming.runtime.StreamingQueryListenerBus, error: com.databricks.unity.error.MissingCredentialScopeException: [UNITY_CREDENTIAL_SCOPE_MISSING_SCOPE] Missing Credential Scope. Failed to find Unity Credential Scope.. SQLSTATE: XXKUC
26/01/08 16:16:26 WARN StreamingQueryListenerBus: Failed to capture UCS handle for ListenerBus: org.apache.spark.sql.execution.streaming.runtime.StreamingQueryListenerBus, error: com.databricks.unity.error.MissingCredentialScopeException: [UNITY_CREDENTIAL_SCOPE_MISSING_SCOPE] Missing Credential Scope. Failed to find Unity Credential Scope.. SQLSTATE: XXKUC
26/01/08 16:16:26 INFO ReplOuterWrapper: DriverWrapper created for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true)
26/01/08 16:16:26 INFO ReplManagerImpl: Adding repl ReplId-19b9e-5c1b5-0 to execution context 8925385330113782073 (2)
26/01/08 16:16:26 INFO ReplManagerImpl: StartReplOptions: 
26/01/08 16:16:26 INFO PythonDriverWrapper: REPL status transitioned to Starting ReplId-19b9e-5c1b5-0 (3)
26/01/08 16:16:26 INFO PythonDriverWrapper: Starting ReplId-19b9e-5c1b5-0 - driverStatus transitioned to Starting (4)
26/01/08 16:16:26 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 10 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:26 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:26 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:26 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO ArtifactManager: Added cluster scoped jar /local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar to session f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:26 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:26 WARN DriverDaemon: Unexpected exception: java.lang.NullPointerException: Cannot invoke "com.databricks.backend.daemon.driver.DriverLocal.inspectRequest(String, long, long)" because "this.repl" is null
java.lang.NullPointerException: Cannot invoke "com.databricks.backend.daemon.driver.DriverLocal.inspectRequest(String, long, long)" because "this.repl" is null
	at com.databricks.backend.daemon.driver.DriverWrapper.inspectRequest(DriverWrapper.scala:440)
	at com.databricks.backend.daemon.driver.DriverCorralRPCRequestHandlerHelper$.$anonfun$handleInspectRequest$1(DriverCorralRPCRequestHandlerHelper.scala:30)
	at scala.Option.map(Option.scala:242)
	at com.databricks.backend.daemon.driver.DriverCorralRPCRequestHandlerHelper$.handleInspectRequest(DriverCorralRPCRequestHandlerHelper.scala:30)
	at com.databricks.backend.daemon.driver.DriverCorral.inspectInternal(DriverCorral.scala:2180)
	at com.databricks.backend.daemon.driver.DriverCorralCompatServerBackend.$anonfun$handlers$35(DriverCorralCompatServerBackend.scala:156)
	at com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:470)
	at com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:413)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:618)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:596)
	at com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)
	at com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:594)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)
	at grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)
	at com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)
	at com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:16:26 INFO ClusterLoadMonitor: Added query with execution ID:0. Current active queries:1
26/01/08 16:16:26 ERROR LoggingService: [sreqId=98b3c65b, chanId=4ee0758d, raddr=10.139.64.4:56568, laddr=10.139.64.4:6061][h1c://0104-134656-pvz3m00y-10-139-64-4/#POST] Request: {startTime=2026-01-08T16:16:26.541Z(1767888986541000), length=171B, duration=2543Âµs(2543905ns), scheme=ws+h1c, name=POST, headers=[:method=POST, :path=/?type="com.databricks.api.proto.chauffeur.InspectInternalRequest", x-request-id=89f1042b-6129-9d9c-8d9e-be3caca95832, content-length=171, content-type=application/octet-stream, traceparent=00-a12417586523734eb4561bbc959959d6-6cd3054905bec6e9-00]}
26/01/08 16:16:26 ERROR LoggingService: [sreqId=98b3c65b, chanId=4ee0758d, raddr=10.139.64.4:56568, laddr=10.139.64.4:6061][h1c://0104-134656-pvz3m00y-10-139-64-4/#POST] Response: {startTime=2026-01-08T16:16:26.570Z(1767888986570000), length=9023B, duration=0ns, totalDuration=28418Âµs(28418398ns), cause=java.lang.NullPointerException: Cannot invoke "com.databricks.backend.daemon.driver.DriverLocal.inspectRequest(String, long, long)" because "this.repl" is null, headers=[:status=500, content-length=9023, content-type=application/octet-stream]}
java.lang.NullPointerException: Cannot invoke "com.databricks.backend.daemon.driver.DriverLocal.inspectRequest(String, long, long)" because "this.repl" is null
	at com.databricks.backend.daemon.driver.DriverWrapper.inspectRequest(DriverWrapper.scala:440)
	at com.databricks.backend.daemon.driver.DriverCorralRPCRequestHandlerHelper$.$anonfun$handleInspectRequest$1(DriverCorralRPCRequestHandlerHelper.scala:30)
	at scala.Option.map(Option.scala:242)
	at com.databricks.backend.daemon.driver.DriverCorralRPCRequestHandlerHelper$.handleInspectRequest(DriverCorralRPCRequestHandlerHelper.scala:30)
	at com.databricks.backend.daemon.driver.DriverCorral.inspectInternal(DriverCorral.scala:2180)
	at com.databricks.backend.daemon.driver.DriverCorralCompatServerBackend.$anonfun$handlers$35(DriverCorralCompatServerBackend.scala:156)
	at com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:470)
	at com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:413)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:618)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:596)
	at com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)
	at com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)
	at com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:594)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)
	at com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)
	at com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)
	at com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)
	at com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)
	at grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)
	at com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)
	at com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)
	at com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)
	at com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)
	at com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:16:26 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 4 ms.
26/01/08 16:16:26 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:26 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
26/01/08 16:16:27 INFO ArmeriaCommChannelWebSocketHandler: [session: 618750256] onWebSocketSubscribe with headers: Map(db-outgoing-buffer-throttler-burst -> List(60000000), db-outgoing-buffer-throttler-steady-rate -> List(6000000), db-outgoing-buffer-throttler-warning-interval-sec -> List(60))
26/01/08 16:16:27 INFO OutgoingDirectNotebookBufferRateLimiter: OutgoingDirectNotebookBufferRateLimiter initialized with clusterBurst=60000000, clusterSteadyRate=6000000 
26/01/08 16:16:27 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 618750256] Start ArmeriaMessageSendTask
26/01/08 16:16:27 ERROR DriverCorral$: Could not get repl, python repl for context Some(8925385330113782073) is not ready or in a bad state.
26/01/08 16:16:27 ERROR DriverCorral$: Invalid state, unknown type for python repl: scala.runtime.BoxedUnit
26/01/08 16:16:27 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:27 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:27 INFO QueryPlanningTracker: Query phase optimization took 1s before execution.
26/01/08 16:16:28 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
26/01/08 16:16:28 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:28 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:28 INFO DBCEventLoggingListener: Rolling event log; numTimesRolledOver = 1
26/01/08 16:16:28 INFO DBCEventLoggingListener: Rolled active log file /databricks/driver/eventlogs/5894523486654786537/eventlog to /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-10, size = 35210696
26/01/08 16:16:28 INFO DBCEventLoggingListener: Logging events to eventlogs/5894523486654786537/eventlog
26/01/08 16:16:28 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar at (spark://10.139.64.4:40407/jars/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar,Some(/local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:28 INFO ClusterLoadMonitor: Removed query with execution ID:0. Current active queries:0
26/01/08 16:16:28 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:28 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:28 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:28 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:28 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:28 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:28 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:28 INFO ClusterLoadMonitor: Added query with execution ID:1. Current active queries:1
26/01/08 16:16:28 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:28 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar at (spark://10.139.64.4:40407/jars/org_apiguardian_apiguardian_api_1_1_2.jar,Some(/local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:1. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:2. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_buffer_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:2. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:3. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar at (spark://10.139.64.4:40407/jars/org_neo4j_caniuse_neo4j_detection_1_3_0.jar,Some(/local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:3. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:4. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar at (spark://10.139.64.4:40407/jars/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar,Some(/local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:4. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:5. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_tcnative_classes_2_0_73_Final.jar,Some(/local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:5. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:6. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_codec_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:6. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:7. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar at (spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar,Some(/local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:7. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:8. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar at (spark://10.139.64.4:40407/jars/org_jetbrains_annotations_13_0.jar,Some(/local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:8. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:9. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar at (spark://10.139.64.4:40407/jars/org_neo4j_caniuse_core_1_3_0.jar,Some(/local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:9. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:10. Current active queries:1
26/01/08 16:16:29 INFO DBCEventLoggingListener: Compressed rolled file /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-10 to /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-10.gz in 1004ms, size = 5286234
26/01/08 16:16:29 INFO DBCEventLoggingListener: Deleted rolled file eventlogs/5894523486654786537/eventlog-2026-01-08--16-10
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_transport_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:10. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:11. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar at (spark://10.139.64.4:40407/jars/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar,Some(/local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:11. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:12. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar at (spark://10.139.64.4:40407/jars/org_slf4j_slf4j_api_2_0_17.jar,Some(/local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:12. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:13. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar at (spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar,Some(/local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:13. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:29 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Added query with execution ID:14. Current active queries:1
26/01/08 16:16:29 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:29 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:29 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_handler_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:29 INFO ClusterLoadMonitor: Removed query with execution ID:14. Current active queries:0
26/01/08 16:16:29 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:29 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:15. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:15. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:16. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar at (spark://10.139.64.4:40407/jars/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar,Some(/local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:16. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:17. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar at (spark://10.139.64.4:40407/jars/io_projectreactor_reactor_core_3_6_11.jar,Some(/local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:17. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:18. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar at (spark://10.139.64.4:40407/jars/org_neo4j_caniuse_api_1_3_0.jar,Some(/local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:18. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:19. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar at (spark://10.139.64.4:40407/jars/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar,Some(/local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:19. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:20. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_common_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:20. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:21. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar at (spark://10.139.64.4:40407/jars/neo4j_jdbc_full_bundle.jar,Some(/local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:21. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:22. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar at (spark://10.139.64.4:40407/jars/io_netty_netty_resolver_4_1_127_Final.jar,Some(/local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:22. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:23. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar at (spark://10.139.64.4:40407/jars/org_reactivestreams_reactive_streams_1_0_4.jar,Some(/local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:23. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 80in the parser. Driver memory: 7888437248.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand).
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Added query with execution ID:24. Current active queries:1
26/01/08 16:16:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:30 WARN SparkContext: The JAR file:/local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar at (spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar,Some(/local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar)) has been added already. Overwriting of added jar is not supported in the current version.
26/01/08 16:16:30 INFO ClusterLoadMonitor: Removed query with execution ID:24. Current active queries:0
26/01/08 16:16:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:30 INFO CurrentQueryContext: Thread Thread[WRAPPER-ReplId-19b9e-5c1b5-0,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:30 INFO PyPIPkgInfo$$anon$1: create pypi file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00000_767ec6342382e4f6e61eb6b309a77ee7.pypi for project neo4j
26/01/08 16:16:30 INFO PyPIPkgInfo$$anon$1: create pypi file /local_disk0/tmp/clusterWideLibDir/__clusterWide_lib_00001_4c2c3c21c8f70af91418a98326dcda48.pypi for project python-dotenv
26/01/08 16:16:30 INFO JupyterDriverLocal: Setting up Python REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true), initializing JupyterDriverLocal (5 - 0)
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 0, app id: local-1767888418069
26/01/08 16:16:30 INFO AsyncEventQueue: Process of event SparkListenerQueryProfileParamsReady(executionId=0, ...) bylistener org.apache.spark.sql.QueryProfileListener took 1825.545319ms.
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 1, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 2, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 3, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 4, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 5, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 6, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 7, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 8, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 9, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 10, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 11, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 12, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 13, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 14, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 15, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 16, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 17, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 18, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 19, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 20, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 21, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 22, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 23, app id: local-1767888418069
26/01/08 16:16:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 24, app id: local-1767888418069
26/01/08 16:16:30 INFO JupyterDriverLocal: Starting Python for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true)
26/01/08 16:16:30 INFO JupyterDriverLocal: Starting gateway server for repl ReplId-19b9e-5c1b5-0
26/01/08 16:16:30 INFO PythonPy4JUtil: Using pinned thread mode in Py4J
26/01/08 16:16:30 INFO JupyterDriverLocal: Starting Python for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true): started Py4J Gateway Server (5 - 1)
26/01/08 16:16:30 INFO VirtualenvCloneHelper: Creating Python notebook virtualenv for f53ae975-90ca-450a-9977-b7bcfa997cc0 with user root
26/01/08 16:16:30 INFO Utils: resolved command to be run: List(virtualenv, /local_disk0/.ephemeral_nfs/envs/pythonEnv-f53ae975-90ca-450a-9977-b7bcfa997cc0, -p, /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, --no-download, --no-setuptools, --no-wheel)
26/01/08 16:16:31 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
26/01/08 16:16:31 INFO PythonEnvCloneHelper$: Created python virtualenv: /local_disk0/.ephemeral_nfs/envs/pythonEnv-f53ae975-90ca-450a-9977-b7bcfa997cc0
26/01/08 16:16:31 INFO Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, -c, import sys;dirs=[p for p in sys.path if 'package' in p];print('__SITE_DELIMITER__'.join([f'import site;site.addsitedir(\"\"\"{path}\"\"\")' for path in dirs])))
26/01/08 16:16:31 INFO Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/envs/pythonEnv-f53ae975-90ca-450a-9977-b7bcfa997cc0/bin/python, -c, from sysconfig import get_path; print(get_path('purelib')))
26/01/08 16:16:31 INFO PythonEnvCloneHelper$: Created sites.pth at /local_disk0/.ephemeral_nfs/envs/pythonEnv-f53ae975-90ca-450a-9977-b7bcfa997cc0/lib/python3.12/site-packages/sites.pth
26/01/08 16:16:31 INFO NotebookScopedPythonEnvManager: Time spent creating Python notebook virtualenv /local_disk0/.ephemeral_nfs/envs/pythonEnv-f53ae975-90ca-450a-9977-b7bcfa997cc0 is 530 ms
26/01/08 16:16:31 INFO JupyterDriverLocal: Created Python Environment Manager for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true) (5 - 2)
26/01/08 16:16:31 INFO NotebookScopedPythonEnvManager: Registered /local_disk0/.ephemeral_nfs/envs/pythonEnv-f53ae975-90ca-450a-9977-b7bcfa997cc0/lib/python3.12/site-packages with the WatchService sun.nio.fs.LinuxWatchService$LinuxWatchKey@5ef40641 (2 current watcher(s)).
26/01/08 16:16:31 INFO JupyterDriverLocal: Created LSP backend symlink for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true) (5 - 3)
26/01/08 16:16:31 INFO JupyterDriverLocal: Sandbox API is not used in REPL
26/01/08 16:16:31 INFO IpykernelUtils$: Python process builder: [bash, /local_disk0/.ephemeral_nfs/envs/pythonEnv-f53ae975-90ca-450a-9977-b7bcfa997cc0/python_start_notebook_scoped.sh, /databricks/spark/python/pyspark/wrapped_python.py, root, /local_disk0/.ephemeral_nfs/envs/pythonEnv-f53ae975-90ca-450a-9977-b7bcfa997cc0/bin/python, /databricks/python_shell/scripts/db_ipykernel_launcher.py, -f, /databricks/kernel-connections/aef1ecd2c1e6320143ffecc0926a07b67af0f5973eb52c62f54ef9d596339b57.json]
26/01/08 16:16:31 INFO IpykernelUtils$: Established and started ipyKernelProcess for REPL ReplId-19b9e-5c1b5-0 (5 - 4)
26/01/08 16:16:31 INFO IpykernelUtils$: Cgroup isolation disabled, not placing python process with ReplId=ReplId-19b9e-5c1b5-0 in repl cgroup
26/01/08 16:16:31 INFO IpykernelUtils$: Configured ipykernel stdout and stdin (5 - 5)
26/01/08 16:16:33 INFO IpykernelUtils$: Connection file updated for REPL ReplId-19b9e-5c1b5-0 (5 - 6)
26/01/08 16:16:33 INFO JupyterDriverLocal: iPykernel process started and configured for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true) (5 - 7)
26/01/08 16:16:33 INFO JupyterDriverLocal: Jupyter client and comm channels configured for repl ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true) (5 - 8)
26/01/08 16:16:33 INFO JupyterDriverLocal: Watchdog thread started for repl ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true) (5 - 9)
26/01/08 16:16:34 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
26/01/08 16:16:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@20f97e2e size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:16:37 ERROR DriverCorral$: Could not get repl, python repl for context Some(8925385330113782073) is not ready or in a bad state.
26/01/08 16:16:37 ERROR DriverCorral$: Invalid state, unknown type for python repl: scala.runtime.BoxedUnit
26/01/08 16:16:40 INFO JupyterDriverLocal: JupyterKernelListener instantiated and started for REPL ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true) (5 - 10)
26/01/08 16:16:40 INFO PythonDriverWrapper: Driver instantiated for ReplId-19b9e-5c1b5-0 (6)
26/01/08 16:16:40 INFO NotebookScopedPythonEnvManager: Pip metadata is empty, cleanup old pip configuration if exists
26/01/08 16:16:40 INFO PythonDriverWrapper: REPL started but is idle: ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true) (7)
26/01/08 16:16:40 INFO PythonDriverWrapper: setupRepl:ReplInfo(driverReplId=ReplId-19b9e-5c1b5-0, chauffeurReplId=ReplId-19b9e-5c1b5-0,
 executionContextId=Some(ExecutionContextIdV2(8925385330113782073)), lazyInfoInitialized=true): finished to load
26/01/08 16:16:40 INFO PythonDriverWrapper: Finished setting-up REPL ReplId-19b9e-5c1b5-0, accepting commands (8)
26/01/08 16:16:40 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:16:40 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:16:40 INFO ProgressReporter$: Added result fetcher for 1767888395088_8277233520537652579_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:41 WARN WsfsHttpClient: Host http://databricks.node.host.local does not exist: java.net.UnknownHostException: databricks.node.host.local: Name or service not known
26/01/08 16:16:41 WARN WsfsHttpClient: Host http://node.host.local does not exist: java.net.UnknownHostException: node.host.local: Name or service not known
26/01/08 16:16:41 INFO ProgressReporter$: Reporting progress for running commands: 1767888395088_8277233520537652579_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:41 WARN JupyterKernelListener: Results buffer should be empty before command run but it is:
--start--
AnsiResult(Thu Jan  8 16:16:34 2026 Connection to spark from PID  2426
Thu Jan  8 16:16:34 2026 Initialized gateway on port 44083
Thu Jan  8 16:16:34 2026 Connected to spark.
,Some(stderr),Map(),Map(),List(),List(),Map())
--end--

Disregarding messages.

26/01/08 16:16:41 INFO ProgressReporter$: Reporting partial results for running commands: 1767888395088_8277233520537652579_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:42 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8277233520537652579_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:42 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8277233520537652579_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:42 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:16:42 INFO ProgressReporter$: Added result fetcher for 1767888395088_6084743123164793356_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:43 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
26/01/08 16:16:43 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:16:43 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(686ca54d)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604/jdbc_drivers/__unitystorage/schemas/b8494da4-84bb-491e-8f20-8be304588f99/volumes/dca69f58-0089-490e-8659-549c43d0ccc6 with credential = CredentialScopeADLSTokenProvider with jvmId = 656
26/01/08 16:16:43 INFO NativeADLGen2RequestComparisonHandler: Native - hadoop request mismatch: Request type: Listing$
Extra params in native request: [st=2026-01-08t15%3a57%3a56z&sv=2020-02-10&ske=2026-01-08t17%3a57%3a56z&sig=gqm6w%2fp%2fdqylvxzfaklw3enm2staje9l2p6uxmvs90q%3d&sktid=54e85725-ed2a-49a4-a19e-11c8d29f9a0f&se=2026-01-08t17%3a16%3a43z&sdd=7&skoid=b0a1dcb0-4ca9-4b42-995f-82e66e87d647&spr=https&sks=b&skt=2026-01-08t15%3a57%3a56z&sp=rl&skv=2025-01-05&sr=d]
Extra params in java request: [st,sv,ske,sig,sktid,sdd,se,skoid,spr,sks,skt,sp,sr,skv]

26/01/08 16:16:44 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6084743123164793356_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:44 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6084743123164793356_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:44 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:16:44 INFO ProgressReporter$: Added result fetcher for 1767888395088_8843610246152641689_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:44 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 47 DFA states in the parser. Total cached DFA states: 127in the parser. Driver memory: 7888437248.
26/01/08 16:16:44 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 127in the parser. Driver memory: 7888437248.
26/01/08 16:16:44 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 127in the parser. Driver memory: 7888437248.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:44 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 20.330944 ms.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:44 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.execution.command.CreateExternalUserDefinedFunctionCommand).
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:44 INFO ClusterLoadMonitor: Added query with execution ID:25. Current active queries:1
26/01/08 16:16:44 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:44 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:44 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:44 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 127in the parser. Driver memory: 7888437248.
26/01/08 16:16:44 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 127in the parser. Driver memory: 7888437248.
26/01/08 16:16:44 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 127in the parser. Driver memory: 7888437248.
26/01/08 16:16:44 INFO ClusterLoadMonitor: Removed query with execution ID:25. Current active queries:0
26/01/08 16:16:44 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:44 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:16:44 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 54 DFA states in the parser. Total cached DFA states: 181in the parser. Driver memory: 7888437248.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:16:44 INFO QueryProfileListener: Query profile sent to logger, seq number: 25, app id: local-1767888418069
26/01/08 16:16:44 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.058096 ms.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:44 INFO ClusterLoadMonitor: Added query with execution ID:26. Current active queries:1
26/01/08 16:16:44 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 4, computed in 0 ms.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:16:44 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:16:44 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:16:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:45 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:16:45 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:16:45 INFO CodeGenerator: Code generated in 209.81675 ms
26/01/08 16:16:46 INFO SparkContext: Starting job: wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27
26/01/08 16:16:46 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
26/01/08 16:16:46 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Metadata GC Threshold
 StartTime: 611167
 Duration: 64
26/01/08 16:16:46 INFO MemoryUsageTracker: GC notification:
 Name: PS MarkSweep,
 Action: end of major GC,
 Cause: Metadata GC Threshold
 StartTime: 611231
 Duration: 397
26/01/08 16:16:46 INFO DAGScheduler: Got job 0 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) with 1 output partitions
26/01/08 16:16:46 INFO DAGScheduler: Final stage: ResultStage 0 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27)
26/01/08 16:16:46 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:16:46 INFO DAGScheduler: Missing parents: List()
26/01/08 16:16:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27), which has no missing parents
26/01/08 16:16:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) with jobGroupId 1767888395088_8843610246152641689_87cc06e830a94580b7d96eee88a5464d and executionId 26 (first 15 tasks are for partitions Vector(0))
26/01/08 16:16:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
26/01/08 16:16:47 INFO TaskSetManager: TaskSet 0.0 using PreferredLocationsV1
26/01/08 16:16:47 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767888395088, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767888395088. Created 1767888395088 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 16:16:47 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool 1767888395088
26/01/08 16:16:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:16:47 INFO MemoryStore: MemoryStore started with capacity 4.2 GiB
26/01/08 16:16:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 180.1 KiB, free 4.2 GiB)
26/01/08 16:16:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 56.8 KiB, free 4.2 GiB)
26/01/08 16:16:47 INFO SparkContext: Created broadcast 0 from broadcast at TaskSetManager.scala:848
26/01/08 16:16:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar with timestamp 1767888473619
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3a. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3n. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfss. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme gs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme r2. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme wasb. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 INFO SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme wasbs. Previous value: com.databricks.sql.io.LokiFileSystem
26/01/08 16:16:47 ERROR SparkHadoopUtil: No filesystem implementation found for sharepoint scheme.
26/01/08 16:16:47 ERROR SparkHadoopUtil: No filesystem implementation found for gdrive scheme.
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFileef91beb40c7e49e2a31efb30a7a6b0f811691922980680940284/org_reactivestreams_reactive_streams_1_0_4.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_reactivestreams_reactive_streams_1_0_4.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar with timestamp 1767888471765
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFilef274f3d3436044d0a5e3ef2f4b2f206510750762803436435180/io_netty_netty_resolver_4_1_127_Final.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_resolver_4_1_127_Final.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar with timestamp 1767888474971
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFile10e67ca59a5c4c8aaaae9b6484f022955947072855073834437/org_neo4j_caniuse_core_1_3_0.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_core_1_3_0.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar with timestamp 1767888472528
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFileff27af2d023b406e9c2b9018bd1848b18773558120779780398/io_netty_netty_codec_4_1_127_Final.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_codec_4_1_127_Final.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar with timestamp 1767888474093
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFile8aa53f98a83f47139e43d2d9d364c9348280177078608123856/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar with timestamp 1767888473459
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFileed070bbb234043eeb89489f3b1bb35e36634387443193481259/org_apiguardian_apiguardian_api_1_1_2.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_apiguardian_apiguardian_api_1_1_2.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar with timestamp 1767888471658
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFile2dd3166bc9bf4e59a03657865c97c59c16309537759821994014/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar with timestamp 1767888472360
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFile22f2fdc54ea34a4489dfc68b7c203e3c13093837975560820942/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar with timestamp 1767888473146
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFile8f562c7a5d524c38b180d727607c4f2914795527702938709369/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar with timestamp 1767888471559
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFile313916e4c616495e83214cc65d52cd9c17446655868284407340/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar with timestamp 1767888474235
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFile3969c2ad3ab947169006e2ed3b5652cf5364875458168276999/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar with timestamp 1767888473942
26/01/08 16:16:47 INFO Utils: /local_disk0/tmp/addedFile6f943ffe955145cc8b91fda8a37c201b16113626410679181726/io_projectreactor_reactor_core_3_6_11.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_projectreactor_reactor_core_3_6_11.jar
26/01/08 16:16:47 INFO Executor: Fetching file:/local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar with timestamp 1767888474563
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile3dcabc844fc0444ebac73cc0c5f9137c2136845549088839229/org_neo4j_caniuse_api_1_3_0.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_api_1_3_0.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar with timestamp 1767888472856
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFiled31d5b099d1949faaa16d9fb4bfd8b7c7738882117041388276/io_netty_netty_transport_4_1_127_Final.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_transport_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar with timestamp 1767888474705
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile4d4cf85aa4514032aa9f4fd32b44b38a17219976044335557643/io_netty_netty_tcnative_classes_2_0_73_Final.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_tcnative_classes_2_0_73_Final.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar with timestamp 1767888472210
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFilee3ed6430e0ea473191b050bc7a78e46d11738404083553575729/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar with timestamp 1767888472041
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile994327af52ec42549e5519d899edecfc60629362718241159/org_slf4j_slf4j_api_2_0_17.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_slf4j_slf4j_api_2_0_17.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar with timestamp 1767888474824
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile2ebc3b3f715c4209b3cce0942c290b0d15121170789125904351/org_jetbrains_annotations_13_0.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_jetbrains_annotations_13_0.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar with timestamp 1767888472690
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile3b0f4ff1eed242bb95accc70a3e65bae17948704254783969378/org_neo4j_caniuse_neo4j_detection_1_3_0.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_neo4j_detection_1_3_0.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar with timestamp 1767888478251
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFileeb2b0c8c0d544bdf99b7263b20b315ae2062813863589309948/neo4j_jdbc_full_bundle.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/neo4j_jdbc_full_bundle.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar with timestamp 1767888473002
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFilebe45feda2c1c4c4ba46cd9537dfede695168330132197898499/io_netty_netty_handler_4_1_127_Final.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_handler_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar with timestamp 1767888471901
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile47fa7703711b43eca557e5721532f2f74307563078911012219/io_netty_netty_common_4_1_127_Final.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_common_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar with timestamp 1767888474396
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile640f119eb3c844fda8c60d4622c2d7476030349873618481399/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar with timestamp 1767888473757
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile7bb3f6bf250a46619ddb39e61a873ad913758717224434513985/io_netty_netty_buffer_4_1_127_Final.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_buffer_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Fetching file:/local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar with timestamp 1767888473287
26/01/08 16:16:48 INFO Utils: /local_disk0/tmp/addedFile23964a206af646e5bb0282cdbcc206a66294590108352688063/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar with timestamp 1767888474155
26/01/08 16:16:48 INFO TransportClientFactory: Successfully created connection to /10.139.64.4:40407 after 40 ms (0 ms spent in bootstraps)
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp11058686689316157800.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp11058686689316157800.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_transport_native_unix_common_4_1_127_Final.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_resolver_4_1_127_Final.jar with timestamp 1767888471821
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_resolver_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp14569814001238873923.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp14569814001238873923.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_resolver_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_resolver_4_1_127_Final.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_jetbrains_annotations_13_0.jar with timestamp 1767888474885
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_jetbrains_annotations_13_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp12983523475449002340.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp12983523475449002340.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_jetbrains_annotations_13_0.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_jetbrains_annotations_13_0.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_caniuse_neo4j_detection_1_3_0.jar with timestamp 1767888472743
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_caniuse_neo4j_detection_1_3_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp9491009731428024917.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp9491009731428024917.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_neo4j_detection_1_3_0.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_neo4j_detection_1_3_0.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar with timestamp 1767888472261
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp11547714346121240794.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp11547714346121240794.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_connector_apache_spark_2_13_5_3_10_for_spark_3.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_buffer_4_1_127_Final.jar with timestamp 1767888473828
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_buffer_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp16481073623768208781.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp16481073623768208781.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_buffer_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_buffer_4_1_127_Final.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_transport_4_1_127_Final.jar with timestamp 1767888472913
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_transport_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp8969231300400582870.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp8969231300400582870.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_transport_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_transport_4_1_127_Final.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar with timestamp 1767888471686
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp1642018652633768541.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp1642018652633768541.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_jetbrains_kotlin_kotlin_stdlib_2_1_20.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar with timestamp 1767888472412
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp7483682486799886689.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp7483682486799886689.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_driver_neo4j_java_driver_slim_4_4_21.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/neo4j_jdbc_full_bundle.jar with timestamp 1767888478283
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/neo4j_jdbc_full_bundle.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp14203401589881785270.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp14203401589881785270.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/neo4j_jdbc_full_bundle.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/neo4j_jdbc_full_bundle.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_common_4_1_127_Final.jar with timestamp 1767888471969
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_common_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp13449647284520466614.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp13449647284520466614.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_common_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_common_4_1_127_Final.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_projectreactor_reactor_core_3_6_11.jar with timestamp 1767888474005
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_projectreactor_reactor_core_3_6_11.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp11707261566690395185.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp11707261566690395185.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_projectreactor_reactor_core_3_6_11.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_projectreactor_reactor_core_3_6_11.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar with timestamp 1767888474307
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp10660443091303197500.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp10660443091303197500.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_reauth_driver_1_0_0_rc2.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_handler_4_1_127_Final.jar with timestamp 1767888473031
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_handler_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp2962482974165646873.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp2962482974165646873.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_handler_4_1_127_Final.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_handler_4_1_127_Final.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_caniuse_api_1_3_0.jar with timestamp 1767888474623
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_caniuse_api_1_3_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp16717513283663785089.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp16717513283663785089.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_api_1_3_0.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_api_1_3_0.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_slf4j_slf4j_api_2_0_17.jar with timestamp 1767888472095
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_slf4j_slf4j_api_2_0_17.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp7812930029700531927.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp7812930029700531927.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_slf4j_slf4j_api_2_0_17.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_slf4j_slf4j_api_2_0_17.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar with timestamp 1767888473343
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp6234196361170118299.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp6234196361170118299.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_authn_spi_1_0_0_rc2.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_tcnative_classes_2_0_73_Final.jar with timestamp 1767888474758
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_tcnative_classes_2_0_73_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp16775230181308999032.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp16775230181308999032.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_tcnative_classes_2_0_73_Final.jar
26/01/08 16:16:48 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_tcnative_classes_2_0_73_Final.jar to class loader default
26/01/08 16:16:48 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_caniuse_core_1_3_0.jar with timestamp 1767888475029
26/01/08 16:16:48 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_caniuse_core_1_3_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp5390025580022626146.tmp
26/01/08 16:16:48 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp5390025580022626146.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_core_1_3_0.jar
26/01/08 16:16:49 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_caniuse_core_1_3_0.jar to class loader default
26/01/08 16:16:49 INFO Executor: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_codec_4_1_127_Final.jar with timestamp 1767888472581
26/01/08 16:16:49 INFO Utils: Fetching spark://10.139.64.4:40407/jars/io_netty_netty_codec_4_1_127_Final.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp8372953305192485258.tmp
26/01/08 16:16:49 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp8372953305192485258.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_codec_4_1_127_Final.jar
26/01/08 16:16:49 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/io_netty_netty_codec_4_1_127_Final.jar to class loader default
26/01/08 16:16:49 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar with timestamp 1767888474441
26/01/08 16:16:49 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp18444456109357813032.tmp
26/01/08 16:16:49 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp18444456109357813032.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar
26/01/08 16:16:49 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_connectors_commons_authn_provided_1_0_0_rc2.jar to class loader default
26/01/08 16:16:49 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_apiguardian_apiguardian_api_1_1_2.jar with timestamp 1767888473518
26/01/08 16:16:49 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_apiguardian_apiguardian_api_1_1_2.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp13019931520098282739.tmp
26/01/08 16:16:49 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp13019931520098282739.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_apiguardian_apiguardian_api_1_1_2.jar
26/01/08 16:16:49 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_apiguardian_apiguardian_api_1_1_2.jar to class loader default
26/01/08 16:16:49 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_reactivestreams_reactive_streams_1_0_4.jar with timestamp 1767888473682
26/01/08 16:16:49 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_reactivestreams_reactive_streams_1_0_4.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp14889499220028367098.tmp
26/01/08 16:16:49 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp14889499220028367098.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_reactivestreams_reactive_streams_1_0_4.jar
26/01/08 16:16:49 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_reactivestreams_reactive_streams_1_0_4.jar to class loader default
26/01/08 16:16:49 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar with timestamp 1767888473189
26/01/08 16:16:49 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp1236044594605269689.tmp
26/01/08 16:16:49 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp1236044594605269689.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar
26/01/08 16:16:49 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
26/01/08 16:16:49 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_connector_apache_spark_2_13_common_5_3_10_for_spark_3.jar to class loader default
26/01/08 16:16:49 INFO Executor: Fetching spark://10.139.64.4:40407/jars/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar with timestamp 1767888471591
26/01/08 16:16:49 INFO Utils: Fetching spark://10.139.64.4:40407/jars/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp5377944439510448360.tmp
26/01/08 16:16:49 INFO Utils: /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/fetchFileTemp5377944439510448360.tmp has been previously copied to /local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar
26/01/08 16:16:49 INFO Executor: Adding file:/local_disk0/spark-942ad966-ee98-413a-9809-84cfed03beae/userFiles-32e23d48-8b33-46e5-96e3-5cf3cbb47041/org_neo4j_neo4j_cypher_dsl_2022_11_0.jar to class loader default
26/01/08 16:16:49 INFO CodeGenerator: Code generated in 36.394099 ms
26/01/08 16:16:49 INFO CodeGenerator: Code generated in 18.445239 ms
26/01/08 16:16:49 INFO BaseAllocator: Debug mode disabled. Enable with the VM option -Darrow.memory.debug.allocator=true.
26/01/08 16:16:49 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
26/01/08 16:16:49 INFO CheckAllocator: Using DefaultAllocationManager at memory-netty--org.apache.arrow__arrow-memory-netty__18.2.0.jar!/org/apache/arrow/memory/netty/DefaultAllocationManagerFactory.class
26/01/08 16:16:49 INFO CodeGenerator: Code generated in 14.391421 ms
26/01/08 16:16:49 INFO EvalExternalUDFExec: Times: init = 459 ms
26/01/08 16:16:49 INFO CodeGenerator: Code generated in 46.38487 ms
26/01/08 16:16:50 INFO ExternalUDFRunner: Getting mounts: None None Some(/local_disk0/.ephemeral_nfs/cluster_libraries/python) Some(/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-19b9e-5c1b5-0) None
26/01/08 16:16:50 INFO ExternalUDFRunner: Creating payload with mountConfig: List()
26/01/08 16:16:50 INFO SandboxApiClient: Configuring safespark client to use keepalive interval Some(300 seconds), timeout Some(20000 milliseconds), and performSafeSparkDnsLookup Some(false). 
26/01/08 16:16:50 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xe8036363]'
26/01/08 16:16:50 WARN SandboxApiClient: Channel is not ready state: IDLE
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:16:50 INFO DispatcherImpl: Session requested for no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36: Map(executionId -> 26, stageId -> 0, taskAttemptId -> 0, DB_SESSION_UUID -> f53ae975-90ca-450a-9977-b7bcfa997cc0) - using apiType = ApiAdapter and executionType = SCALAR.
26/01/08 16:16:50 INFO DispatcherImpl: Session requested: no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:16:50 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:<unset-channel-id> UsageRefCount increased to 1.
26/01/08 16:16:50 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:<unset-channel-id> TaskContextRefCount increased to 1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:16:50 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:<unset-channel-id> transition from EMPTY to INITIALIZING finished
26/01/08 16:16:50 INFO SandboxMemoryHolder: Acquired 314572800 bytes for no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36, currently used: 314815371
26/01/08 16:16:50 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:<unset-channel-id> got memoryConsumer no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36
26/01/08 16:16:50 INFO SandboxWarmpool: Checking availability of mounts before replenishing.
26/01/08 16:16:50 INFO DatabricksUtils: Environment directory found at /local_disk0/.ephemeral_nfs/envs after 1 attempts
26/01/08 16:16:50 INFO DatabricksUtils: Environment directory found at /local_disk0/.ephemeral_nfs/repl_tmp_data after 1 attempts
26/01/08 16:16:50 INFO DatabricksUtils: Environment directory found at /local_disk0/.ephemeral_nfs/udf_envs after 1 attempts
26/01/08 16:16:50 INFO DatabricksUtils: Environment directory found at /local_disk0/.ephemeral_nfs/cluster_libraries after 1 attempts
26/01/08 16:16:50 INFO DatabricksUtils: Environment directory found at /local_disk0/.ephemeral_nfs/artifacts after 1 attempts
26/01/08 16:16:50 INFO SandboxWarmpool: Mounts are available!
26/01/08 16:16:50 INFO SandboxMemoryHolder: Acquired 377487360 bytes for WARMPOOL, currently used: 692302731
26/01/08 16:16:50 WARN WarmpoolSandboxChannelFactory: Acquiring from warmpool failed, fallback to direct creation.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.ucTimeoutSeconds from default: 60.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:16:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:16:51 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xc51e819e]'
26/01/08 16:16:51 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:16:51 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:16:51 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:16:52 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
26/01/08 16:16:52 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:16:52 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:16:52 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 354 milliseconds)
26/01/08 16:16:52 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:16:52 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:16:52 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:16:52 INFO ThreadDumpManager: ThreadDumpManager: thread dump requested due to Reason: {DAG_SCHEDULER_NO_ACTIVE_JOB.toString}, Count: 1
26/01/08 16:16:52 INFO ThreadDumpManager: ThreadDumpManager: thread dump took 24 ms.
26/01/08 16:16:52 INFO ThreadDumpManager: ThreadDumpManager: updated thread dump for 348 threads, removed 0 inactive threads.
26/01/08 16:16:52 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:16:52 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:16:52 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 303 milliseconds)
26/01/08 16:16:53 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x2c26d7b3]'
26/01/08 16:16:55 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
26/01/08 16:16:55 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x7f75e8bc]'
26/01/08 16:16:56 INFO PlainSandboxChannelFactory: Got sandbox: 2030223589249704145
26/01/08 16:16:56 INFO PlainSandboxChannelFactory: Got sandbox: 7690579267257963872
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:16:56 INFO PlainSandboxChannelFactory: Got sandbox: 4019183287962612715
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:16:56 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:4019183287962612715 got sandboxChannel 4019183287962612715
26/01/08 16:16:56 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:4019183287962612715 transition from INITIALIZING to INITIALIZED finished
26/01/08 16:16:56 INFO DispatcherImpl: Mapping sandbox 4019183287962612715 to session no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36.
26/01/08 16:16:56 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:16:56 INFO UDFSessionFactory: UDFSession is created.
26/01/08 16:16:57 INFO UDFSession: Session (4019183287962612715): setting payload.
26/01/08 16:16:57 INFO UDFSession: Session (4019183287962612715): setup done.
26/01/08 16:16:58 INFO Dispatcher: Session None completion listener called
26/01/08 16:16:58 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
26/01/08 16:16:58 INFO DispatcherImpl: Session: no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36, sandbox: UDFChannel(state='INITIALIZED', id=4019183287962612715, usageRefCount=1, taskContextRefCount=1) reclaiming.
26/01/08 16:16:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:16:58 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:4019183287962612715 UsageRefCount decreased to 0.
26/01/08 16:16:58 INFO DispatcherImpl: disposeSandboxLocked is called for sandbox UDFChannel(state='INITIALIZED', id=4019183287962612715, usageRefCount=0, taskContextRefCount=1) session no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36 with throwable: None
26/01/08 16:16:58 INFO DispatcherImpl: Unmapped sandbox 4019183287962612715 from session no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36
26/01/08 16:16:58 INFO UDFChannel: Disposing sandbox (isError=false): UDFChannel(state='INITIALIZED', id=4019183287962612715, usageRefCount=0, taskContextRefCount=1)
26/01/08 16:16:58 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:4019183287962612715 transition from INITIALIZED to DISPOSING finished
26/01/08 16:16:58 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:4019183287962612715 transition from DISPOSING to DISPOSED finished
26/01/08 16:16:58 INFO DispatcherImpl: disposeSandboxLocked is called for sandbox UDFChannel(state='DISPOSED', id=4019183287962612715, usageRefCount=0, taskContextRefCount=1) session no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36 with throwable: Some(java.lang.RuntimeException: ManagedChannel failed, is in state SHUTDOWN)
26/01/08 16:16:58 INFO DispatcherImpl: Could not unmap sandbox 4019183287962612715 from session no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36
26/01/08 16:16:58 INFO UDFChannel: key:no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36-channelId:4019183287962612715 TaskContextRefCount decreased to 0.
26/01/08 16:16:58 INFO EdgeTaskResultLoggingUtils: Serializing task result with SQL metrics serialization improvement enabled.
26/01/08 16:16:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4487 bytes result sent to driver
26/01/08 16:16:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11183 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:16:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:16:58 INFO DAGScheduler: ResultStage 0 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) finished in 11659 ms
26/01/08 16:16:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:16:58 INFO TaskSchedulerImpl: Canceling stage 0
26/01/08 16:16:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Result stage finished
26/01/08 16:16:58 INFO DAGScheduler: Job 0 finished: wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27, took 11800.107538 ms
26/01/08 16:16:58 WARN AppStatusListener: AppStatusListener.getTaskMetricsSubset() took 17 ms
26/01/08 16:16:58 INFO ClusterLoadMonitor: Removed query with execution ID:26. Current active queries:0
26/01/08 16:16:58 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:16:58 INFO WSFSDriverManager$: Received response with error tip message from WSFS: {"errorTipMessage":"[Trace ID: 00-c0dfc55bab8fc77d321dc761e7230b24-2267ed0a6be4d209-00]"}
26/01/08 16:16:58 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8843610246152641689_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:58 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8843610246152641689_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:58 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:16:58 INFO ProgressReporter$: Added result fetcher for 1767888395088_7587634746911292420_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:59 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7587634746911292420_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:59 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7587634746911292420_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:59 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:16:59 INFO ProgressReporter$: Added result fetcher for 1767888395088_4683183763801793435_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:16:59 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x04fe6bf6]'
26/01/08 16:16:59 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:00 INFO QueryProfileListener: Query profile sent to logger, seq number: 26, app id: local-1767888418069
26/01/08 16:17:00 INFO AsyncEventQueue: Process of event SparkListenerQueryProfileParamsReady(executionId=26, ...) bylistener org.apache.spark.sql.QueryProfileListener took 1866.185138ms.
26/01/08 16:17:01 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
26/01/08 16:17:01 INFO DriverFactory: Routing driver instance 385665805 created for server address 3f1f827a.databases.neo4j.io:7687
26/01/08 16:17:01 INFO SandboxMemoryHolderImpl: MemoryConsumer released 314572800 bytes for no-reuse-c3575b1b-00aa-44d6-897c-982b089b6a36, currently used: 377729931
26/01/08 16:17:01 INFO InternalDriver: Closing driver instance 385665805
26/01/08 16:17:01 INFO QueryPlanningTracker: Query phase analysis took 2s before execution.
26/01/08 16:17:01 INFO ClusterLoadMonitor: Added query with execution ID:27. Current active queries:1
26/01/08 16:17:01 INFO ClusterLoadMonitor: Removed query with execution ID:27. Current active queries:0
26/01/08 16:17:01 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:01 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.net.UnknownHostException: 3f1f827a.databases.neo4j.io: Name or service not known
	at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)
	at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:89)
	at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
	at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
	at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
	at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
	at scala.Option.flatMap(Option.scala:283)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause
		at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
		at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:934)
		at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)
		at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)
		at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
		at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
		at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
		at org.neo4j.driver.internal.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:35)
		at org.neo4j.driver.internal.cluster.RediscoveryImpl.resolveAllByDomainName(RediscoveryImpl.java:380)
		at org.neo4j.driver.internal.cluster.RediscoveryImpl.resolve(RediscoveryImpl.java:337)
		at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.supportsMultiDb(LoadBalancer.java:176)
		at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.verifyConnectivity(LoadBalancer.java:150)
		at org.neo4j.driver.internal.SessionFactoryImpl.verifyConnectivity(SessionFactoryImpl.java:74)
		at org.neo4j.driver.internal.InternalDriver.verifyConnectivityAsync(InternalDriver.java:129)
		at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
		at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
		at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
		at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
		at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
		at scala.Option.flatMap(Option.scala:283)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)
		at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:89)
		at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
		at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
		at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
		at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
		at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
		at scala.Option.flatMap(Option.scala:283)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:17:02 INFO ProgressReporter$: Removed result fetcher for 1767888395088_4683183763801793435_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:02 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_4683183763801793435_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:02 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:02 INFO ProgressReporter$: Added result fetcher for 1767888395088_5967167206725723043_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:02 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088065/48186c3d-0253-4e38-92b7-413a333f9b5a] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:17:02 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088065/48186c3d-0253-4e38-92b7-413a333f9b5a] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a27103d method=PUT
26/01/08 16:17:02 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088065/48186c3d-0253-4e38-92b7-413a333f9b5a] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:17:02 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:02 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:04 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
26/01/08 16:17:05 INFO PlainSandboxChannelFactory: Got sandbox: 3413338802587618480
26/01/08 16:17:05 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x204196d1]'
26/01/08 16:17:05 WARN MemoryTracker: Allocated 142117416 bytes for org.apache.spark.sql.catalyst.analysis.ResolveDataSource
26/01/08 16:17:05 INFO QueryPlanningTracker: Query phase analysis took 2s before execution.
26/01/08 16:17:05 INFO ClusterLoadMonitor: Added query with execution ID:28. Current active queries:1
26/01/08 16:17:05 INFO ClusterLoadMonitor: Removed query with execution ID:28. Current active queries:0
26/01/08 16:17:05 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:05 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more

26/01/08 16:17:05 INFO PlainSandboxChannelFactory: Got sandbox: 2083538306371450563
26/01/08 16:17:06 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5967167206725723043_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:06 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5967167206725723043_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:06 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:06 INFO ProgressReporter$: Added result fetcher for 1767888395088_8671193214201869806_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6acb4e0f size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:17:06 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:06 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:06 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:06 INFO ClusterLoadMonitor: Added query with execution ID:29. Current active queries:1
26/01/08 16:17:06 INFO ClusterLoadMonitor: Removed query with execution ID:29. Current active queries:0
26/01/08 16:17:06 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more

26/01/08 16:17:06 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:06 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088067/43ab0841-87d3-442d-b3b9-91e5d4bffde8] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:17:06 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088067/43ab0841-87d3-442d-b3b9-91e5d4bffde8] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a27106a method=PUT
26/01/08 16:17:06 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088067/43ab0841-87d3-442d-b3b9-91e5d4bffde8] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:17:07 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8671193214201869806_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:07 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8671193214201869806_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:07 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:07 INFO ProgressReporter$: Added result fetcher for 1767888395088_9220270715685241601_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:07 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:07 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:07 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:07 INFO ClusterLoadMonitor: Added query with execution ID:30. Current active queries:1
26/01/08 16:17:07 INFO ClusterLoadMonitor: Removed query with execution ID:30. Current active queries:0
26/01/08 16:17:07 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:07 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more

26/01/08 16:17:07 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088068/533d7a0a-f331-47f2-8b76-76d9afe72638] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:17:07 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088068/533d7a0a-f331-47f2-8b76-76d9afe72638] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a27107a method=PUT
26/01/08 16:17:07 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088068/533d7a0a-f331-47f2-8b76-76d9afe72638] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:17:08 INFO ProgressReporter$: Removed result fetcher for 1767888395088_9220270715685241601_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:08 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_9220270715685241601_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:08 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:08 INFO ProgressReporter$: Added result fetcher for 1767888395088_7389221885633213963_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:08 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088069/441979ae-7ce9-45e7-9c7f-24b38156f0fe] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:17:08 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088069/441979ae-7ce9-45e7-9c7f-24b38156f0fe] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a27108c method=PUT
26/01/08 16:17:08 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088069/441979ae-7ce9-45e7-9c7f-24b38156f0fe] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:17:08 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:08 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:08 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:08 INFO ClusterLoadMonitor: Added query with execution ID:31. Current active queries:1
26/01/08 16:17:08 INFO ClusterLoadMonitor: Removed query with execution ID:31. Current active queries:0
26/01/08 16:17:08 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:08 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more

26/01/08 16:17:09 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7389221885633213963_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:09 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7389221885633213963_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:09 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:09 INFO ProgressReporter$: Added result fetcher for 1767888395088_6342991846743387586_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:09 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 9 DFA states in the parser. Total cached DFA states: 190in the parser. Driver memory: 7888437248.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:17:09 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 28.98691 ms.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:09 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.DropConnectionCommand).
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:09 INFO ClusterLoadMonitor: Added query with execution ID:32. Current active queries:1
26/01/08 16:17:09 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:09 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:17:09 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:09 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088070/266e2f22-1410-4434-8043-64b047e52838] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:17:09 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088070/266e2f22-1410-4434-8043-64b047e52838] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a2710a4 method=PUT
26/01/08 16:17:09 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088070/266e2f22-1410-4434-8043-64b047e52838] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:17:09 INFO ClusterLoadMonitor: Removed query with execution ID:32. Current active queries:0
26/01/08 16:17:09 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:09 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:17:09 INFO QueryProfileListener: Query profile sent to logger, seq number: 27, app id: local-1767888418069
26/01/08 16:17:09 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 27 DFA states in the parser. Total cached DFA states: 217in the parser. Driver memory: 7888437248.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:17:09 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.036619 ms.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:09 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.CreateConnectionCommand).
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:09 INFO ClusterLoadMonitor: Added query with execution ID:33. Current active queries:1
26/01/08 16:17:09 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:09 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:17:09 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:09 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:10 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
26/01/08 16:17:10 INFO ClusterLoadMonitor: Removed query with execution ID:33. Current active queries:0
26/01/08 16:17:10 INFO QueryProfileListener: Query profile sent to logger, seq number: 28, app id: local-1767888418069
26/01/08 16:17:10 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:10 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:17:10 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6342991846743387586_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:10 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6342991846743387586_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:10 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:10 INFO ProgressReporter$: Added result fetcher for 1767888395088_5600478543661960285_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:10 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 9 DFA states in the parser. Total cached DFA states: 226in the parser. Driver memory: 7888437248.
26/01/08 16:17:10 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:17:10 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.023382 ms.
26/01/08 16:17:10 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:10 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.DescribeConnectionCommand).
26/01/08 16:17:10 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:10 INFO ClusterLoadMonitor: Added query with execution ID:34. Current active queries:1
26/01/08 16:17:10 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 2, computed in 0 ms.
26/01/08 16:17:10 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:10 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:17:10 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:17:10 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:17:10 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:10 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:10 INFO ClusterLoadMonitor: Removed query with execution ID:34. Current active queries:0
26/01/08 16:17:10 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:10 INFO QueryProfileListener: Query profile sent to logger, seq number: 29, app id: local-1767888418069
26/01/08 16:17:10 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:17:10 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.025297 ms.
26/01/08 16:17:10 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:11 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.043375 ms.
26/01/08 16:17:11 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:11 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.050434 ms.
26/01/08 16:17:11 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:11 INFO ClusterLoadMonitor: Added query with execution ID:35. Current active queries:1
26/01/08 16:17:11 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 10, computed in 0 ms.
26/01/08 16:17:11 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:11 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:17:11 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:17:11 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:17:11 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:11 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:11 INFO CodeGenerator: Code generated in 8.228049 ms
26/01/08 16:17:11 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.054534 ms.
26/01/08 16:17:11 INFO CodeGenerator: Code generated in 18.500972 ms
26/01/08 16:17:11 INFO ClusterLoadMonitor: Removed query with execution ID:35. Current active queries:0
26/01/08 16:17:11 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:11 INFO QueryProfileListener: Query profile sent to logger, seq number: 30, app id: local-1767888418069
26/01/08 16:17:11 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5600478543661960285_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:11 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5600478543661960285_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:11 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:11 INFO ProgressReporter$: Added result fetcher for 1767888395088_5814254520772742264_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:11 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:11 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:11 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:17:11 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:17:11 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:11 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:11 INFO Utils: resolved command to be run: List(id, -nu, 1000)
26/01/08 16:17:11 INFO UDFScalaEnvManager: Changing ownership of directory /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487 to session user ubuntu
26/01/08 16:17:12 INFO Utils: resolved command to be run: List(kill, -USR1, 2813)
26/01/08 16:17:13 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
26/01/08 16:17:13 INFO EnvironmentFuseUtils$: Successfully executed copy udf jars
26/01/08 16:17:13 ERROR LocalFuseProcess: Failed to get pid namespace id for 2813 with error java.nio.file.NoSuchFileException: /proc/2813/ns/pid
26/01/08 16:17:13 ERROR LocalFuseProcess: Failed to get pid namespace id for 2813 with error java.nio.file.NoSuchFileException: /proc/2813/ns/pid
26/01/08 16:17:13 INFO UDFScalaEnvManager: Make virtualenv directory accessible to root and nobody: /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487
26/01/08 16:17:14 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 2157 ms
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:17:14 INFO DispatcherImpl: Grpc session requested for grpc-session-789ee5a2-aac9-45e4-808f-28f5cbf074ad: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:17:14 INFO DispatcherImpl: Sandbox grpc-session-789ee5a2-aac9-45e4-808f-28f5cbf074ad - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:17:14 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.ucTimeoutSeconds from default: 60.
26/01/08 16:17:15 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x090b87d1]'
26/01/08 16:17:16 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
26/01/08 16:17:17 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:17 WARN ProductionClientProvider: Failed to connect to a remote jdbc service
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:17 INFO DispatcherImpl: Grpc session: grpc-session-789ee5a2-aac9-45e4-808f-28f5cbf074ad closing exc is None
26/01/08 16:17:18 INFO QueryPlanningTracker: Query phase analysis took 6s before execution.
26/01/08 16:17:18 INFO ClusterLoadMonitor: Added query with execution ID:36. Current active queries:1
26/01/08 16:17:18 INFO ClusterLoadMonitor: Removed query with execution ID:36. Current active queries:0
26/01/08 16:17:18 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:18 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:17:18 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5814254520772742264_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:18 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5814254520772742264_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:18 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:18 INFO ProgressReporter$: Added result fetcher for 1767888395088_5763620890601121105_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:18 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:18 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:18 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:17:18 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:17:18 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:18 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:18 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 3 ms
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:17:18 INFO DispatcherImpl: Grpc session requested for grpc-session-2fea6e60-67cf-4fa7-9600-546669f2110c: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:17:18 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:17:18 INFO DispatcherImpl: Sandbox grpc-session-2fea6e60-67cf-4fa7-9600-546669f2110c - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:17:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:17:18 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:17:18 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 16:17:18 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 656
26/01/08 16:17:19 INFO DriverCorral: DBFS health check ok
26/01/08 16:17:19 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:17:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:17:19 INFO DriverCorral: Metastore health check ok
26/01/08 16:17:21 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:21 WARN ProductionClientProvider: Failed to connect to a remote jdbc service
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:21 INFO DispatcherImpl: Grpc session: grpc-session-2fea6e60-67cf-4fa7-9600-546669f2110c closing exc is None
26/01/08 16:17:22 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:17:22 INFO ClusterLoadMonitor: Added query with execution ID:37. Current active queries:1
26/01/08 16:17:22 INFO ClusterLoadMonitor: Removed query with execution ID:37. Current active queries:0
26/01/08 16:17:22 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:22 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:17:22 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5763620890601121105_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:22 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5763620890601121105_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:22 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:22 INFO ProgressReporter$: Added result fetcher for 1767888395088_5593628611068742512_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:22 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 46 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:17:22 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:17:23 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:17:23 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:17:23 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:23 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:23 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:17:23 INFO DispatcherImpl: Grpc session requested for grpc-session-2f9623f0-ad48-45c9-8e43-f4b5bdc6adcc: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:17:23 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:17:23 INFO DispatcherImpl: Sandbox grpc-session-2f9623f0-ad48-45c9-8e43-f4b5bdc6adcc - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:17:26 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:26 WARN ProductionClientProvider: Failed to connect to a remote jdbc service
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:26 INFO DispatcherImpl: Grpc session: grpc-session-2f9623f0-ad48-45c9-8e43-f4b5bdc6adcc closing exc is None
26/01/08 16:17:26 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:17:26 INFO ClusterLoadMonitor: Added query with execution ID:38. Current active queries:1
26/01/08 16:17:26 INFO ClusterLoadMonitor: Removed query with execution ID:38. Current active queries:0
26/01/08 16:17:26 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:26 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:17:27 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5593628611068742512_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:27 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5593628611068742512_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:27 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:27 INFO ProgressReporter$: Added result fetcher for 1767888395088_8676275025256535941_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:27 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:27 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:27 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:17:27 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:17:27 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:27 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:27 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:17:27 INFO DispatcherImpl: Grpc session requested for grpc-session-849c38af-bca7-49d3-92fd-c3c245084955: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:17:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:17:27 INFO DispatcherImpl: Sandbox grpc-session-849c38af-bca7-49d3-92fd-c3c245084955 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:17:30 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:30 WARN ProductionClientProvider: Failed to connect to a remote jdbc service
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:30 INFO DispatcherImpl: Grpc session: grpc-session-849c38af-bca7-49d3-92fd-c3c245084955 closing exc is None
26/01/08 16:17:31 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:17:31 INFO ClusterLoadMonitor: Added query with execution ID:39. Current active queries:1
26/01/08 16:17:31 INFO ClusterLoadMonitor: Removed query with execution ID:39. Current active queries:0
26/01/08 16:17:31 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:31 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:17:31 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8676275025256535941_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:31 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8676275025256535941_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:31 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:31 INFO ProgressReporter$: Added result fetcher for 1767888395088_6630682142647540446_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:31 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:31 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:31 INFO ClusterLoadMonitor: Added query with execution ID:40. Current active queries:1
26/01/08 16:17:31 INFO ClusterLoadMonitor: Removed query with execution ID:40. Current active queries:0
26/01/08 16:17:31 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:31 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
		at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.databaseUnavailableError(ChannelConnectedListener.java:153)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.ChannelConnectedListener.operationComplete(ChannelConnectedListener.java:40)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:124)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:89)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:228)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:639)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:119)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: 3f1f827a.databases.neo4j.io
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at org.neo4j.jdbc.internal.shaded.bolt.DefaultDomainNameResolver.resolve(DefaultDomainNameResolver.java:33)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.NettyDomainNameResolver.doResolve(NettyDomainNameResolver.java:39)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at org.neo4j.jdbc.internal.shaded.io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at org.neo4j.jdbc.internal.shaded.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:220)
	... 22 more

26/01/08 16:17:32 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:17:32 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6630682142647540446_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:32 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6630682142647540446_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:32 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:32 INFO ProgressReporter$: Added result fetcher for 1767888395088_6435038424927667970_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:32 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6435038424927667970_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:32 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6435038424927667970_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:32 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:32 INFO ProgressReporter$: Added result fetcher for 1767888395088_7025770146408674393_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:32 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088080/7a13b6e7-b24e-4c5e-9b06-d8379824b61b] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:17:33 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088080/7a13b6e7-b24e-4c5e-9b06-d8379824b61b] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a2711b8 method=PUT
26/01/08 16:17:33 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088080/7a13b6e7-b24e-4c5e-9b06-d8379824b61b] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:17:33 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7025770146408674393_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:33 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7025770146408674393_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:33 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:33 INFO ProgressReporter$: Added result fetcher for 1767888395088_8965791901734182716_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:33 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:33 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:33 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:17:33 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:17:33 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:33 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:33 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 2 ms
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:17:33 INFO DispatcherImpl: Grpc session requested for grpc-session-ec2ac0dc-ee6c-4731-bf9e-dd6c96dbdde3: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:17:33 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:17:33 INFO DispatcherImpl: Sandbox grpc-session-ec2ac0dc-ee6c-4731-bf9e-dd6c96dbdde3 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:17:34 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x9175c374]'
26/01/08 16:17:36 INFO ApiClient: Connection with key None has been finalized and removed.
26/01/08 16:17:36 INFO ApiClient: Connection with key None has been finalized and removed.
26/01/08 16:17:36 INFO ApiClient: Connection with key None has been finalized and removed.
26/01/08 16:17:36 INFO ApiClient: Connection with key None has been finalized and removed.
26/01/08 16:17:36 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:36 WARN ProductionClientProvider: Failed to connect to a remote jdbc service
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:36 INFO DispatcherImpl: Grpc session: grpc-session-ec2ac0dc-ee6c-4731-bf9e-dd6c96dbdde3 closing exc is None
26/01/08 16:17:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6c6a9cb9 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:17:37 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:17:37 INFO ClusterLoadMonitor: Added query with execution ID:41. Current active queries:1
26/01/08 16:17:37 INFO ClusterLoadMonitor: Removed query with execution ID:41. Current active queries:0
26/01/08 16:17:37 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:37 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:17:37 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8965791901734182716_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:37 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8965791901734182716_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:37 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:37 INFO ProgressReporter$: Added result fetcher for 1767888395088_7079692226138115888_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:37 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:37 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:37 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:17:37 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:17:37 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:37 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:37 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 0 ms
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:17:37 INFO DispatcherImpl: Grpc session requested for grpc-session-73e9bad0-cb85-4fa3-9277-f3819ae13b79: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:17:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:17:37 INFO DispatcherImpl: Sandbox grpc-session-73e9bad0-cb85-4fa3-9277-f3819ae13b79 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:17:40 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:40 WARN ProductionClientProvider: Failed to connect to a remote jdbc service
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:40 INFO DispatcherImpl: Grpc session: grpc-session-73e9bad0-cb85-4fa3-9277-f3819ae13b79 closing exc is None
26/01/08 16:17:41 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:17:41 INFO ClusterLoadMonitor: Added query with execution ID:42. Current active queries:1
26/01/08 16:17:41 INFO ClusterLoadMonitor: Removed query with execution ID:42. Current active queries:0
26/01/08 16:17:41 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:41 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:17:41 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7079692226138115888_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:41 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7079692226138115888_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:41 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:41 INFO ProgressReporter$: Added result fetcher for 1767888395088_8240317877781837751_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:41 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:41 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:17:41 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:17:41 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:17:41 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:41 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:17:41 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:17:41 INFO DispatcherImpl: Grpc session requested for grpc-session-2f923b98-1bf9-4457-a06f-a016d445ea59: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:17:41 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:17:41 INFO DispatcherImpl: Sandbox grpc-session-2f923b98-1bf9-4457-a06f-a016d445ea59 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:17:43 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:17:44 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:44 WARN ProductionClientProvider: Failed to connect to a remote jdbc service
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:17:44 INFO DispatcherImpl: Grpc session: grpc-session-2f923b98-1bf9-4457-a06f-a016d445ea59 closing exc is None
26/01/08 16:17:45 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:17:45 INFO ClusterLoadMonitor: Added query with execution ID:43. Current active queries:1
26/01/08 16:17:45 INFO ClusterLoadMonitor: Removed query with execution ID:43. Current active queries:0
26/01/08 16:17:45 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.sql.SQLException: INTERNAL: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Unable to connect to 3f1f827a.databases.neo4j.io:7687, ensure the database is running and that there is a working network connection to it.
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
		at scala.Option.getOrElse(Option.scala:201)
		at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
		at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
		at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
		at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
		at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
		at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:17:45 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:45 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8240317877781837751_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:45 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8240317877781837751_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:45 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:45 INFO ProgressReporter$: Added result fetcher for 1767888395088_6628889829169334199_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:45 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/e88d4e1f-3da5-4617-afb1-1e4d9d398bba] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:17:45 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/e88d4e1f-3da5-4617-afb1-1e4d9d398bba] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a271250 method=PUT
26/01/08 16:17:45 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/e88d4e1f-3da5-4617-afb1-1e4d9d398bba] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:17:45 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.02071 ms.
26/01/08 16:17:45 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:45 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.062502 ms.
26/01/08 16:17:45 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:45 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.075256 ms.
26/01/08 16:17:45 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:45 INFO ClusterLoadMonitor: Added query with execution ID:44. Current active queries:1
26/01/08 16:17:45 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 22, computed in 0 ms.
26/01/08 16:17:45 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:17:45 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:17:45 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:17:45 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:17:45 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:45 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:17:45 INFO CodeGenerator: Code generated in 27.485729 ms
26/01/08 16:17:45 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:17:45 INFO DAGScheduler: Got job 1 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 16:17:45 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:17:45 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:17:45 INFO DAGScheduler: Missing parents: List()
26/01/08 16:17:45 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:17:45 INFO DAGScheduler: submitMissingTasks(ResultStage 1): 1 / 4 partitions missing, starting partial re-computation
26/01/08 16:17:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_6628889829169334199_87cc06e830a94580b7d96eee88a5464d and executionId 44 (first 15 tasks are for partitions Vector(0))
26/01/08 16:17:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
26/01/08 16:17:45 INFO TaskSetManager: TaskSet 1.0 using PreferredLocationsV1
26/01/08 16:17:45 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767888395088, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767888395088. Created 1767888395088 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 16:17:45 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool 1767888395088
26/01/08 16:17:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:17:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 166.4 KiB, free 3.9 GiB)
26/01/08 16:17:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 51.1 KiB, free 3.9 GiB)
26/01/08 16:17:45 INFO SparkContext: Created broadcast 1 from broadcast at TaskSetManager.scala:848
26/01/08 16:17:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:17:45 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.unityCatalog.enforce.permissions from SQLConf: false.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableOnDedicatedCluster from default: true.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.requireUnityCatalog from default: true.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.unityCatalog.enabled from SQLConf: true.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.maxRetries from default: 10.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.retryInitialBackoffMs from default: 1000.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.retryMaxBackoffMs from default: 30000.
26/01/08 16:17:45 INFO SafeSparkConf$SafeSpark: Retrieved conf value for sspark.databricks.safespark.service.credential.server.timeoutMs from default: 60000.
26/01/08 16:17:45 INFO SafeSparkCredentialServerManager: Creating credential server for socket /databricks/sparkconnect/creds/shared-credential-server138f22cc-24e5-4a68-a138-d8554f0db4c9.sock
26/01/08 16:17:45 INFO SafeSparkCredentialServerManager: Creating credential server (attempt 1) for socket /databricks/sparkconnect/creds/shared-credential-server138f22cc-24e5-4a68-a138-d8554f0db4c9.sock
26/01/08 16:17:46 INFO SafeSparkCredentialRetrievalServer: Adjusted permissions on socket file at: /databricks/sparkconnect/creds/shared-credential-server138f22cc-24e5-4a68-a138-d8554f0db4c9.sock
26/01/08 16:17:46 INFO SandboxCredentialInfoRegistry: Registered sandbox info for UID 0
26/01/08 16:17:46 INFO SafeSparkCredentialRetrievalServer: Registered sandbox for UID 0
26/01/08 16:17:46 INFO PythonWorkerFactory: Waiting for python envs to be ready: List(None, Some(/local_disk0/.ephemeral_nfs/cluster_libraries/python))
26/01/08 16:17:46 INFO DatabricksUtils: Environment directory found at /local_disk0/.ephemeral_nfs/cluster_libraries/python after 1 attempts
26/01/08 16:17:46 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
26/01/08 16:17:46 WARN WsfsHttpClient: Host http://databricks.node.host.local does not exist: java.net.UnknownHostException: databricks.node.host.local: Name or service not known
26/01/08 16:17:46 WARN WsfsHttpClient: Host http://node.host.local does not exist: java.net.UnknownHostException: node.host.local: Name or service not known
26/01/08 16:17:46 INFO CodeGenerator: Code generated in 38.076255 ms
26/01/08 16:17:46 INFO PythonRunner: Times: total = 695, boot = 496, init = 199, finish = 0
26/01/08 16:17:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2579 bytes result sent to driver
26/01/08 16:17:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 778 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:17:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:17:46 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 49771
26/01/08 16:17:46 INFO DAGScheduler: ResultStage 1 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 797 ms
26/01/08 16:17:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:17:46 INFO TaskSchedulerImpl: Canceling stage 1
26/01/08 16:17:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Result stage finished
26/01/08 16:17:46 INFO DAGScheduler: Job 1 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 812.71388 ms
26/01/08 16:17:46 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:17:46 INFO DAGScheduler: Got job 2 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 3 output partitions
26/01/08 16:17:46 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:17:46 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:17:46 INFO DAGScheduler: Missing parents: List()
26/01/08 16:17:46 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:17:46 INFO DAGScheduler: submitMissingTasks(ResultStage 2): 3 / 4 partitions missing, starting partial re-computation
26/01/08 16:17:46 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_6628889829169334199_87cc06e830a94580b7d96eee88a5464d and executionId 44 (first 15 tasks are for partitions Vector(1, 2, 3))
26/01/08 16:17:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks resource profile 0
26/01/08 16:17:46 INFO TaskSetManager: TaskSet 2.0 using PreferredLocationsV1
26/01/08 16:17:46 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool 1767888395088
26/01/08 16:17:46 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.139.64.4,executor driver, partition 1, PROCESS_LOCAL, 
26/01/08 16:17:46 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (10.139.64.4,executor driver, partition 2, PROCESS_LOCAL, 
26/01/08 16:17:46 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (10.139.64.4,executor driver, partition 3, PROCESS_LOCAL, 
26/01/08 16:17:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 166.4 KiB, free 3.9 GiB)
26/01/08 16:17:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 51.1 KiB, free 3.9 GiB)
26/01/08 16:17:46 INFO SparkContext: Created broadcast 2 from broadcast at TaskSetManager.scala:848
26/01/08 16:17:46 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
26/01/08 16:17:46 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
26/01/08 16:17:46 INFO Executor: Running task 2.0 in stage 2.0 (TID 4)
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:17:46 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:17:46 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:17:46 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:17:46 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:17:46 INFO PythonRunner: Times: total = 11, boot = -69, init = 80, finish = 0
26/01/08 16:17:46 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2700 bytes result sent to driver
26/01/08 16:17:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 97 ms on 10.139.64.4 (executor driver) (1/3)
26/01/08 16:17:46 INFO PythonRunner: Times: total = 63, boot = 10, init = 53, finish = 0
26/01/08 16:17:46 INFO Executor: Finished task 2.0 in stage 2.0 (TID 4). 2705 bytes result sent to driver
26/01/08 16:17:46 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 109 ms on 10.139.64.4 (executor driver) (2/3)
26/01/08 16:17:46 INFO PythonRunner: Times: total = 160, boot = 48, init = 112, finish = 0
26/01/08 16:17:46 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 2700 bytes result sent to driver
26/01/08 16:17:46 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 203 ms on 10.139.64.4 (executor driver) (3/3)
26/01/08 16:17:46 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:17:46 INFO DAGScheduler: ResultStage 2 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 219 ms
26/01/08 16:17:46 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:17:46 INFO TaskSchedulerImpl: Canceling stage 2
26/01/08 16:17:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Result stage finished
26/01/08 16:17:46 INFO DAGScheduler: Job 2 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 227.081161 ms
26/01/08 16:17:46 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.041675 ms.
26/01/08 16:17:46 INFO CodeGenerator: Code generated in 7.175783 ms
26/01/08 16:17:46 INFO ClusterLoadMonitor: Removed query with execution ID:44. Current active queries:0
26/01/08 16:17:46 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:17:46 INFO QueryProfileListener: Query profile sent to logger, seq number: 31, app id: local-1767888418069
26/01/08 16:17:47 INFO WSFSDriverManager$: Received response with error tip message from WSFS: {"errorTipMessage":"[Trace ID: 00-e5d85f4e8d764068b26bd2195b242546-af028d9d46cd1ce9-00]"}
26/01/08 16:17:47 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6628889829169334199_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:47 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6628889829169334199_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:47 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:47 INFO ProgressReporter$: Added result fetcher for 1767888395088_7337274969245370578_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:47 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7337274969245370578_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:47 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7337274969245370578_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:47 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:17:47 INFO ProgressReporter$: Added result fetcher for 1767888395088_4964816375824711245_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:47 INFO ProgressReporter$: Removed result fetcher for 1767888395088_4964816375824711245_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:47 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_4964816375824711245_87cc06e830a94580b7d96eee88a5464d
26/01/08 16:17:49 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
26/01/08 16:17:50 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:17:52 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
26/01/08 16:17:52 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:17:53 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:17:56 INFO ApiClient: Connection with key None has been finalized and removed.
26/01/08 16:17:56 INFO ApiClient: Connection with key None has been finalized and removed.
26/01/08 16:17:56 INFO ApiClient: Connection with key None has been finalized and removed.
26/01/08 16:18:00 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xb2d1eb73]'
26/01/08 16:18:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@1d72c753 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:18:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:18:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7691c2dd size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:18:39 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x9f389206]'
26/01/08 16:19:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@461a5136 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:19:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:19:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7e2d9218 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:19:43 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xf7d9ab56]'
26/01/08 16:20:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@75ee8ad1 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:20:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:20:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@d11399b size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:21:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@19965aee size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:21:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:21:29 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:21:33 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:33 INFO ProgressReporter$: Added result fetcher for 1767888395088_8559176380413591154_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:33 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8559176380413591154_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:33 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8559176380413591154_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:33 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:33 INFO ProgressReporter$: Added result fetcher for 1767888395088_6505535411564626573_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:34 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
26/01/08 16:21:34 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:21:34 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(686ca54d)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604/jdbc_drivers/__unitystorage/schemas/b8494da4-84bb-491e-8f20-8be304588f99/volumes/dca69f58-0089-490e-8659-549c43d0ccc6 with credential = CredentialScopeADLSTokenProvider with jvmId = 656
26/01/08 16:21:34 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6505535411564626573_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:34 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6505535411564626573_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:34 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:34 INFO ProgressReporter$: Added result fetcher for 1767888395088_5392217182629833529_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:34 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xde934633]'
26/01/08 16:21:34 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:21:34 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:21:34 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:21:34 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.023605 ms.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:34 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.execution.command.CreateExternalUserDefinedFunctionCommand).
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:34 INFO ClusterLoadMonitor: Added query with execution ID:45. Current active queries:1
26/01/08 16:21:34 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:34 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:21:34 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:21:34 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:21:34 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:21:34 INFO DBCEventLoggingListener: Rolling event log; numTimesRolledOver = 2
26/01/08 16:21:34 INFO DBCEventLoggingListener: Rolled active log file /databricks/driver/eventlogs/5894523486654786537/eventlog to /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-20, size = 1845185
26/01/08 16:21:34 INFO DBCEventLoggingListener: Logging events to eventlogs/5894523486654786537/eventlog
26/01/08 16:21:34 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:21:34 WARN SimpleFunctionRegistry: The function connectiontest replaced a previously registered function.
26/01/08 16:21:34 INFO ClusterLoadMonitor: Removed query with execution ID:45. Current active queries:0
26/01/08 16:21:34 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:21:34 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:21:34 INFO QueryProfileListener: Query profile sent to logger, seq number: 32, app id: local-1767888418069
26/01/08 16:21:34 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:21:34 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.048988 ms.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:34 INFO DBCEventLoggingListener: Compressed rolled file /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-20 to /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-20.gz in 45ms, size = 217099
26/01/08 16:21:34 INFO DBCEventLoggingListener: Deleted rolled file eventlogs/5894523486654786537/eventlog-2026-01-08--16-20
26/01/08 16:21:34 INFO ClusterLoadMonitor: Added query with execution ID:46. Current active queries:1
26/01/08 16:21:34 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 4, computed in 0 ms.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:34 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:21:34 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:21:34 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:21:34 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:21:34 INFO CodeGenerator: Code generated in 18.661208 ms
26/01/08 16:21:34 INFO SparkContext: Starting job: wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27
26/01/08 16:21:34 INFO DAGScheduler: Got job 3 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) with 1 output partitions
26/01/08 16:21:34 INFO DAGScheduler: Final stage: ResultStage 3 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27)
26/01/08 16:21:34 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:21:34 INFO DAGScheduler: Missing parents: List()
26/01/08 16:21:34 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27), which has no missing parents
26/01/08 16:21:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) with jobGroupId 1767888395088_5392217182629833529_79d4619fa3904f3683a8b1f3c79aef1d and executionId 46 (first 15 tasks are for partitions Vector(0))
26/01/08 16:21:34 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
26/01/08 16:21:34 INFO TaskSetManager: TaskSet 3.0 using PreferredLocationsV1
26/01/08 16:21:34 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767888395088, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767888395088. Created 1767888395088 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 16:21:34 INFO FairSchedulableBuilder: Added task set TaskSet_3.0 tasks to pool 1767888395088
26/01/08 16:21:34 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:21:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 180.1 KiB, free 3.9 GiB)
26/01/08 16:21:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 56.8 KiB, free 3.9 GiB)
26/01/08 16:21:34 INFO SparkContext: Created broadcast 3 from broadcast at TaskSetManager.scala:848
26/01/08 16:21:34 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)
26/01/08 16:21:34 INFO EvalExternalUDFExec: Times: init = 2 ms
26/01/08 16:21:34 INFO ExternalUDFRunner: Getting mounts: None None Some(/local_disk0/.ephemeral_nfs/cluster_libraries/python) Some(/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-19b9e-5c1b5-0) None
26/01/08 16:21:34 INFO ExternalUDFRunner: Creating payload with mountConfig: List()
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:21:34 INFO DispatcherImpl: Session requested for no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d: Map(executionId -> 46, stageId -> 3, taskAttemptId -> 5, DB_SESSION_UUID -> f53ae975-90ca-450a-9977-b7bcfa997cc0) - using apiType = ApiAdapter and executionType = SCALAR.
26/01/08 16:21:34 INFO DispatcherImpl: Session requested: no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:21:34 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:<unset-channel-id> UsageRefCount increased to 1.
26/01/08 16:21:34 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:<unset-channel-id> TaskContextRefCount increased to 1.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:21:34 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:<unset-channel-id> transition from EMPTY to INITIALIZING finished
26/01/08 16:21:34 INFO SandboxMemoryHolder: Acquired 314572800 bytes for no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d, currently used: 692990659
26/01/08 16:21:34 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:<unset-channel-id> got memoryConsumer no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:21:34 INFO WarmpoolSandboxChannelFactory: Got sandbox from warmpool: Some(7690579267257963872)
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:21:34 INFO WarmpoolSandboxChannelFactory: Dropping privilege!
26/01/08 16:21:34 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:7690579267257963872 got sandboxChannel 7690579267257963872
26/01/08 16:21:34 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:7690579267257963872 transition from INITIALIZING to INITIALIZED finished
26/01/08 16:21:34 INFO DispatcherImpl: Mapping sandbox 7690579267257963872 to session no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d.
26/01/08 16:21:34 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:21:34 INFO UDFSessionFactory: UDFSession is created.
26/01/08 16:21:34 INFO UDFSession: Session (7690579267257963872): setting payload.
26/01/08 16:21:34 INFO UDFSession: Session (7690579267257963872): setup done.
26/01/08 16:21:35 INFO Dispatcher: Session None completion listener called
26/01/08 16:21:35 INFO DispatcherImpl: Session: no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d, sandbox: UDFChannel(state='INITIALIZED', id=7690579267257963872, usageRefCount=1, taskContextRefCount=1) reclaiming.
26/01/08 16:21:35 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:21:35 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:7690579267257963872 UsageRefCount decreased to 0.
26/01/08 16:21:35 INFO DispatcherImpl: disposeSandboxLocked is called for sandbox UDFChannel(state='INITIALIZED', id=7690579267257963872, usageRefCount=0, taskContextRefCount=1) session no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d with throwable: None
26/01/08 16:21:35 INFO DispatcherImpl: Unmapped sandbox 7690579267257963872 from session no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d
26/01/08 16:21:35 INFO UDFChannel: Disposing sandbox (isError=false): UDFChannel(state='INITIALIZED', id=7690579267257963872, usageRefCount=0, taskContextRefCount=1)
26/01/08 16:21:35 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:7690579267257963872 transition from INITIALIZED to DISPOSING finished
26/01/08 16:21:35 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:7690579267257963872 transition from DISPOSING to DISPOSED finished
26/01/08 16:21:35 INFO UDFChannel: key:no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d-channelId:7690579267257963872 TaskContextRefCount decreased to 0.
26/01/08 16:21:35 INFO DispatcherImpl: disposeSandboxLocked is called for sandbox UDFChannel(state='DISPOSED', id=7690579267257963872, usageRefCount=0, taskContextRefCount=0) session no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d with throwable: Some(java.lang.RuntimeException: ManagedChannel failed, is in state SHUTDOWN)
26/01/08 16:21:35 INFO DispatcherImpl: Could not unmap sandbox 7690579267257963872 from session no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d
26/01/08 16:21:35 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 4495 bytes result sent to driver
26/01/08 16:21:35 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 395 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:21:35 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:21:35 INFO DAGScheduler: ResultStage 3 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) finished in 410 ms
26/01/08 16:21:35 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:21:35 INFO TaskSchedulerImpl: Canceling stage 3
26/01/08 16:21:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Result stage finished
26/01/08 16:21:35 INFO DAGScheduler: Job 3 finished: wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27, took 413.985207 ms
26/01/08 16:21:35 INFO ClusterLoadMonitor: Removed query with execution ID:46. Current active queries:0
26/01/08 16:21:35 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:21:35 INFO QueryProfileListener: Query profile sent to logger, seq number: 33, app id: local-1767888418069
26/01/08 16:21:35 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5392217182629833529_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:35 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5392217182629833529_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:35 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:35 INFO ProgressReporter$: Added result fetcher for 1767888395088_6648591842195388165_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:35 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6648591842195388165_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:35 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6648591842195388165_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:35 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:35 INFO ProgressReporter$: Added result fetcher for 1767888395088_8258427759913972415_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:35 INFO SandboxMemoryHolderImpl: MemoryConsumer released 314572800 bytes for no-reuse-46c2eac4-de2c-4e6b-8738-04ba71d2971d, currently used: 378417859
26/01/08 16:21:35 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:35 INFO DriverFactory: Routing driver instance 129311298 created for server address 3f1f827a.databases.neo4j.io:7687
26/01/08 16:21:36 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 901268
 Duration: 37
26/01/08 16:21:36 INFO InternalDriver: Closing driver instance 129311298
26/01/08 16:21:36 INFO ConnectionPoolImpl: Closing connection pool towards 3f1f827a.databases.neo4j.io(20.124.3.249):7687
26/01/08 16:21:36 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:36 INFO ClusterLoadMonitor: Added query with execution ID:47. Current active queries:1
26/01/08 16:21:36 INFO ClusterLoadMonitor: Removed query with execution ID:47. Current active queries:0
26/01/08 16:21:36 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:21:36 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
org.neo4j.driver.exceptions.ServiceUnavailableException: Unable to connect to database management service, ensure the database is running and that there is a working network connection to it.
	at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)
	at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:89)
	at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
	at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
	at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
	at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
	at scala.Option.flatMap(Option.scala:283)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.neo4j.driver.internal.util.ErrorUtil$InternalExceptionCause
		at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.lambda$verifyConnectivity$3(LoadBalancer.java:156)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.driver.internal.util.Futures.lambda$asCompletionStage$0(Futures.java:75)
		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
		at io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:35)
		at io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:503)
		at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:111)
		at org.neo4j.driver.internal.util.Futures.blockingGet(Futures.java:89)
		at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
		at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
		at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
		at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
		at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
		at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
		at scala.Option.flatMap(Option.scala:283)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.driver.exceptions.ServiceUnavailableException: Failed to perform multi-databases feature detection with the following servers: [3f1f827a.databases.neo4j.io(20.124.3.249):7687]
	at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.supportsMultiDb(LoadBalancer.java:181)
	at org.neo4j.driver.internal.cluster.loadbalancing.LoadBalancer.verifyConnectivity(LoadBalancer.java:150)
	at org.neo4j.driver.internal.SessionFactoryImpl.verifyConnectivity(SessionFactoryImpl.java:74)
	at org.neo4j.driver.internal.InternalDriver.verifyConnectivityAsync(InternalDriver.java:129)
	at org.neo4j.driver.internal.InternalDriver.verifyConnectivity(InternalDriver.java:144)
	at org.neo4j.spark.util.ValidateConnection.validate(Validations.scala:226)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:34)
	at org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:34)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:177)
	at org.neo4j.spark.util.Validations$.validate(Validations.scala:34)
	at org.neo4j.spark.DataSource.inferSchema(DataSource.scala:57)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:105)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:287)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$4(ResolveDataSource.scala:114)
	at scala.Option.flatMap(Option.scala:283)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:112)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.neo4j.driver.exceptions.ServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
		at org.neo4j.driver.internal.util.ErrorUtil.newConnectionTerminatedError(ErrorUtil.java:48)
		at org.neo4j.driver.internal.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:76)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:412)
		at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:377)
		at io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1192)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
		at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
		at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
		at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		... 1 more

26/01/08 16:21:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@b39a696 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:21:37 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8258427759913972415_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:37 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8258427759913972415_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:37 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:37 INFO ProgressReporter$: Added result fetcher for 1767888395088_8291739721364719145_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:37 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088065/524b5e07-29ca-4b0b-9316-220ec02d7dcc] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:21:37 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:37 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:37 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088065/524b5e07-29ca-4b0b-9316-220ec02d7dcc] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a2718d0 method=PUT
26/01/08 16:21:37 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088065/524b5e07-29ca-4b0b-9316-220ec02d7dcc] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:21:37 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:37 INFO ClusterLoadMonitor: Added query with execution ID:48. Current active queries:1
26/01/08 16:21:37 INFO ClusterLoadMonitor: Removed query with execution ID:48. Current active queries:0
26/01/08 16:21:37 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:21:37 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.newConnectionTerminatedError(HandshakeHandler.java:126)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:120)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:21:38 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8291739721364719145_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:38 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_8291739721364719145_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:38 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:38 INFO ProgressReporter$: Added result fetcher for 1767888395088_6453551405746206287_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:38 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:38 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:38 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:38 INFO ClusterLoadMonitor: Added query with execution ID:49. Current active queries:1
26/01/08 16:21:38 INFO ClusterLoadMonitor: Removed query with execution ID:49. Current active queries:0
26/01/08 16:21:38 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:21:38 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.newConnectionTerminatedError(HandshakeHandler.java:126)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:120)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:21:38 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088067/ccf771c6-7d18-46a0-bbee-a75e6fd683cf] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:21:38 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088067/ccf771c6-7d18-46a0-bbee-a75e6fd683cf] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a2718e3 method=PUT
26/01/08 16:21:38 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088067/ccf771c6-7d18-46a0-bbee-a75e6fd683cf] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:21:39 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6453551405746206287_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:39 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6453551405746206287_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:39 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:39 INFO ProgressReporter$: Added result fetcher for 1767888395088_4953715675517208370_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:39 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088068/ec0e0d65-bc14-4bc3-9374-cb5216e2a65f] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:21:39 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088068/ec0e0d65-bc14-4bc3-9374-cb5216e2a65f] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a2718f9 method=PUT
26/01/08 16:21:39 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088068/ec0e0d65-bc14-4bc3-9374-cb5216e2a65f] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:21:39 INFO PlainSandboxChannelFactory: Got sandbox: 5806230651922721073
26/01/08 16:21:39 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:39 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:39 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:21:39 INFO ClusterLoadMonitor: Added query with execution ID:50. Current active queries:1
26/01/08 16:21:39 INFO ClusterLoadMonitor: Removed query with execution ID:50. Current active queries:0
26/01/08 16:21:39 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:21:39 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.util.concurrent.CompletionException: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.NettyBoltConnectionProvider.lambda$connect$2(NettyBoltConnectionProvider.java:155)
		at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:934)
		at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:911)
		at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
		at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.lambda$fail$3(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:199)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.fail(HandshakeHandler.java:212)
		at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:121)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
		at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
		at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection to the database terminated. Please ensure that your database is listening on the correct host and port and that you have compatible encryption settings both on Neo4j server and driver. Note that the default encryption setting has changed in Neo4j 4.0.
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.newConnectionTerminatedError(HandshakeHandler.java:126)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.async.connection.HandshakeHandler.channelInactive(HandshakeHandler.java:120)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:427)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:392)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1191)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
	at org.neo4j.jdbc.internal.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:21:40 INFO ProgressReporter$: Removed result fetcher for 1767888395088_4953715675517208370_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:40 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_4953715675517208370_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:40 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:21:40 INFO ProgressReporter$: Added result fetcher for 1767888395088_7989320765335622597_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:40 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:40 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:21:40 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088069/80b53d23-07bd-42ad-a8e8-e2dee00055f4] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:21:40 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088069/80b53d23-07bd-42ad-a8e8-e2dee00055f4] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a27190c method=PUT
26/01/08 16:21:40 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088069/80b53d23-07bd-42ad-a8e8-e2dee00055f4] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:21:40 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT COUNT(*) AS cnt
                           FROM Flight f
                           NATURAL JOIN DEPARTS_FROM r
                           NATURAL JOIN Airport a) SPARK_GEN_SUBQ_52 WHERE 1=0
26/01/08 16:21:42 INFO ProgressReporter$: [117 occurrences] Reporting partial results for running commands: 1767888395088_7989320765335622597_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:21:52 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:21:52 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:21:52 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:21:52 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:21:52 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:21:52 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 214 milliseconds)
26/01/08 16:21:52 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:21:52 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:21:52 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:21:52 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:21:52 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:21:52 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 191 milliseconds)
26/01/08 16:22:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@a000d22 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:22:08 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:22:09 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:22:11 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:22:17 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:22:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:22:18 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:22:18 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 16:22:18 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 656
26/01/08 16:22:19 INFO DriverCorral: DBFS health check ok
26/01/08 16:22:19 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:22:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:22:19 INFO DriverCorral: Metastore health check ok
26/01/08 16:22:24 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:22:28 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:22:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@ed93018 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:22:42 INFO QueryPlanningTracker: Query phase analysis took 62s before execution.
26/01/08 16:22:42 INFO ClusterLoadMonitor: Added query with execution ID:51. Current active queries:1
26/01/08 16:22:42 INFO ClusterLoadMonitor: Removed query with execution ID:51. Current active queries:0
26/01/08 16:22:42 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:22:42 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.lang.RuntimeException: org.neo4j.jdbc.Neo4jException: general processing exception - An error occurred while handling request
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.nodeOrPattern(SqlToCypher.java:2081)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2048)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2038)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveJoin(SqlToCypher.java:2128)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2034)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.lambda$createOngoingReadingFromSources$0(SqlToCypher.java:529)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1003)
	at java.base/java.util.Collections$UnmodifiableCollection$1.forEachRemaining(Collections.java:1062)
	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:575)
	at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:616)
	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:622)
	at java.base/java.util.stream.ReferencePipeline.toList(ReferencePipeline.java:627)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.createOngoingReadingFromSources(SqlToCypher.java:529)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.statement(SqlToCypher.java:473)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.build(SqlToCypher.java:349)
	at org.neo4j.jdbc.translator.impl.SqlToCypher.translate0(SqlToCypher.java:255)
	at org.neo4j.jdbc.translator.impl.SqlToCypher.translate(SqlToCypher.java:250)
	at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1114)
	at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1085)
	at org.neo4j.jdbc.StatementImpl.processSQL(StatementImpl.java:706)
	at org.neo4j.jdbc.StatementImpl.lambda$executeQuery0$0(StatementImpl.java:156)
	at org.neo4j.jdbc.StatementImpl.recordEvent(StatementImpl.java:425)
	at org.neo4j.jdbc.StatementImpl.executeQuery0(StatementImpl.java:153)
	at org.neo4j.jdbc.PreparedStatementImpl.executeQuery(PreparedStatementImpl.java:168)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.nodeOrPattern(SqlToCypher.java:2081)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2048)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2038)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveJoin(SqlToCypher.java:2128)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2034)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.lambda$createOngoingReadingFromSources$0(SqlToCypher.java:529)
		at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
		at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1003)
		at java.base/java.util.Collections$UnmodifiableCollection$1.forEachRemaining(Collections.java:1062)
		at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845)
		at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
		at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
		at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:575)
		at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
		at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:616)
		at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:622)
		at java.base/java.util.stream.ReferencePipeline.toList(ReferencePipeline.java:627)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.createOngoingReadingFromSources(SqlToCypher.java:529)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.statement(SqlToCypher.java:473)
		at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.build(SqlToCypher.java:349)
		at org.neo4j.jdbc.translator.impl.SqlToCypher.translate0(SqlToCypher.java:255)
		at org.neo4j.jdbc.translator.impl.SqlToCypher.translate(SqlToCypher.java:250)
		at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1114)
		at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1085)
		at org.neo4j.jdbc.StatementImpl.processSQL(StatementImpl.java:706)
		at org.neo4j.jdbc.StatementImpl.lambda$executeQuery0$0(StatementImpl.java:156)
		at org.neo4j.jdbc.StatementImpl.recordEvent(StatementImpl.java:425)
		at org.neo4j.jdbc.StatementImpl.executeQuery0(StatementImpl.java:153)
		at org.neo4j.jdbc.PreparedStatementImpl.executeQuery(PreparedStatementImpl.java:168)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.Neo4jException: general processing exception - An error occurred while handling request
	at org.neo4j.jdbc.DefaultTransactionImpl.execute(DefaultTransactionImpl.java:272)
	at org.neo4j.jdbc.DefaultTransactionImpl.runAndPull(DefaultTransactionImpl.java:130)
	at org.neo4j.jdbc.DatabaseMetadataImpl.doQuery(DatabaseMetadataImpl.java:1977)
	at org.neo4j.jdbc.DatabaseMetadataImpl.doQueryForResultSet(DatabaseMetadataImpl.java:1970)
	at org.neo4j.jdbc.DatabaseMetadataImpl.getTables0(DatabaseMetadataImpl.java:1038)
	at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1708)
	at org.neo4j.jdbc.DatabaseMetadataImpl.getTables(DatabaseMetadataImpl.java:1018)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.nodeOrPattern(SqlToCypher.java:2063)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2048)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2038)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveJoin(SqlToCypher.java:2128)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.resolveTableOrJoin(SqlToCypher.java:2034)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.lambda$createOngoingReadingFromSources$0(SqlToCypher.java:529)
	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273)
	at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1003)
	at java.base/java.util.Collections$UnmodifiableCollection$1.forEachRemaining(Collections.java:1062)
	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:575)
	at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:616)
	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:622)
	at java.base/java.util.stream.ReferencePipeline.toList(ReferencePipeline.java:627)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.createOngoingReadingFromSources(SqlToCypher.java:529)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.statement(SqlToCypher.java:473)
	at org.neo4j.jdbc.translator.impl.SqlToCypher$ContextAwareStatementBuilder.build(SqlToCypher.java:349)
	at org.neo4j.jdbc.translator.impl.SqlToCypher.translate0(SqlToCypher.java:255)
	at org.neo4j.jdbc.translator.impl.SqlToCypher.translate(SqlToCypher.java:250)
	at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1114)
	at org.neo4j.jdbc.ConnectionImpl$TranslatorChain.apply(ConnectionImpl.java:1085)
	at org.neo4j.jdbc.StatementImpl.processSQL(StatementImpl.java:706)
	at org.neo4j.jdbc.StatementImpl.lambda$executeQuery0$0(StatementImpl.java:156)
	at org.neo4j.jdbc.StatementImpl.recordEvent(StatementImpl.java:425)
	at org.neo4j.jdbc.StatementImpl.executeQuery0(StatementImpl.java:153)
	at org.neo4j.jdbc.PreparedStatementImpl.executeQuery(PreparedStatementImpl.java:168)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.neo4j.jdbc.internal.shaded.bolt.exception.BoltServiceUnavailableException: Connection is closed
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.flush(BoltConnectionImpl.java:205)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.lambda$writeAndFlush$2(BoltConnectionImpl.java:144)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.lambda$executeInEventLoop$15(BoltConnectionImpl.java:604)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.lambda$executeInEventLoop$16(BoltConnectionImpl.java:613)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.executeInEventLoop(BoltConnectionImpl.java:619)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.executeInEventLoop(BoltConnectionImpl.java:603)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.writeAndFlush(BoltConnectionImpl.java:142)
	at org.neo4j.jdbc.DefaultTransactionImpl.lambda$runAndPull$0(DefaultTransactionImpl.java:125)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1150)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)
	at org.neo4j.jdbc.internal.shaded.bolt.netty.impl.BoltConnectionImpl.lambda$executeInEventLoop$16(BoltConnectionImpl.java:613)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.neo4j.jdbc.internal.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.neo4j.jdbc.internal.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more

26/01/08 16:22:43 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7989320765335622597_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:43 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7989320765335622597_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:43 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:22:43 INFO ProgressReporter$: Added result fetcher for 1767888395088_4825116692886780118_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:43 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088070/9b3793f7-700f-4120-bb34-d23e31a60ac1] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:22:43 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:22:43 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.024217 ms.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:43 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.DropConnectionCommand).
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:43 INFO ClusterLoadMonitor: Added query with execution ID:52. Current active queries:1
26/01/08 16:22:43 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:43 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:22:43 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:22:43 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088070/9b3793f7-700f-4120-bb34-d23e31a60ac1] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a271b44 method=PUT
26/01/08 16:22:43 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088070/9b3793f7-700f-4120-bb34-d23e31a60ac1] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:22:43 INFO ClusterLoadMonitor: Removed query with execution ID:52. Current active queries:0
26/01/08 16:22:43 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:22:43 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:22:43 INFO QueryProfileListener: Query profile sent to logger, seq number: 34, app id: local-1767888418069
26/01/08 16:22:43 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:22:43 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.021673 ms.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:43 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.CreateConnectionCommand).
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:43 INFO ClusterLoadMonitor: Added query with execution ID:53. Current active queries:1
26/01/08 16:22:43 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:43 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:22:43 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:22:43 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:22:43 INFO ProgressReporter$: [37 occurrences] Reporting progress for running commands: 1767888395088_4825116692886780118_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:43 INFO ClusterLoadMonitor: Removed query with execution ID:53. Current active queries:0
26/01/08 16:22:43 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:22:43 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:22:43 INFO QueryProfileListener: Query profile sent to logger, seq number: 35, app id: local-1767888418069
26/01/08 16:22:43 INFO ProgressReporter$: Removed result fetcher for 1767888395088_4825116692886780118_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:44 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_4825116692886780118_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:44 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:22:44 INFO ProgressReporter$: Added result fetcher for 1767888395088_6959168449106411660_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:44 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:22:44 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.019805 ms.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:44 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.DescribeConnectionCommand).
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:44 INFO ClusterLoadMonitor: Added query with execution ID:54. Current active queries:1
26/01/08 16:22:44 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 2, computed in 0 ms.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:44 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:22:44 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:22:44 INFO ClusterLoadMonitor: Removed query with execution ID:54. Current active queries:0
26/01/08 16:22:44 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:22:44 INFO CurrentQueryContext: Thread Thread[Thread-109,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:22:44 INFO QueryProfileListener: Query profile sent to logger, seq number: 36, app id: local-1767888418069
26/01/08 16:22:44 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.018469 ms.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:44 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.045791 ms.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:44 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.071627 ms.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:44 INFO ClusterLoadMonitor: Added query with execution ID:55. Current active queries:1
26/01/08 16:22:44 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 10, computed in 0 ms.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:22:44 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:22:44 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:22:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:22:44 INFO CodeGenerator: Code generated in 8.093191 ms
26/01/08 16:22:44 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.059434 ms.
26/01/08 16:22:44 INFO CodeGenerator: Code generated in 7.97278 ms
26/01/08 16:22:44 INFO ClusterLoadMonitor: Removed query with execution ID:55. Current active queries:0
26/01/08 16:22:44 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:22:44 INFO QueryProfileListener: Query profile sent to logger, seq number: 37, app id: local-1767888418069
26/01/08 16:22:44 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6959168449106411660_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:44 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6959168449106411660_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:44 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:22:44 INFO ProgressReporter$: Added result fetcher for 1767888395088_5449203666122584325_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:44 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:22:44 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:22:44 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:22:44 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:22:44 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:22:44 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:22:44 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:22:44 INFO DispatcherImpl: Grpc session requested for grpc-session-c60c6b54-c11f-40b7-9275-b6c75ef17825: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:22:44 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:22:44 INFO DispatcherImpl: Sandbox grpc-session-c60c6b54-c11f-40b7-9275-b6c75ef17825 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:22:48 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1 AS test) SPARK_GEN_SUBQ_60 WHERE 1=0
26/01/08 16:22:56 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: UNKNOWN: Application error processing RPC
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:22:56 INFO DispatcherImpl: Grpc session: grpc-session-c60c6b54-c11f-40b7-9275-b6c75ef17825 closing exc is None
26/01/08 16:22:56 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:22:56 INFO QueryPlanningTracker: Query phase analysis took 11s before execution.
26/01/08 16:22:56 INFO ClusterLoadMonitor: Added query with execution ID:56. Current active queries:1
26/01/08 16:22:56 INFO ClusterLoadMonitor: Removed query with execution ID:56. Current active queries:0
26/01/08 16:22:56 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:22:56 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.lang.RuntimeException: Connection was closed before the operation completed.
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
	at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
		at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:22:56 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5449203666122584325_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:56 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5449203666122584325_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:56 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:22:56 INFO ProgressReporter$: Added result fetcher for 1767888395088_5596054112887670495_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:22:56 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/17f8aa61-c835-4a3a-aafc-3dafe4f89327] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:22:56 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/17f8aa61-c835-4a3a-aafc-3dafe4f89327] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a271bdc method=PUT
26/01/08 16:22:56 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/17f8aa61-c835-4a3a-aafc-3dafe4f89327] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:22:57 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:22:57 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:22:57 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:22:57 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:22:57 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:22:57 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:22:57 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:22:57 INFO DispatcherImpl: Grpc session requested for grpc-session-9c931045-b222-4344-ae5d-0deb1d20c550: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:22:57 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:22:57 INFO DispatcherImpl: Sandbox grpc-session-9c931045-b222-4344-ae5d-0deb1d20c550 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:23:00 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test) SPARK_GEN_SUBQ_61 WHERE 1=0
26/01/08 16:23:00 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:23:00 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 16:23:00 INFO DispatcherImpl: Grpc session: grpc-session-9c931045-b222-4344-ae5d-0deb1d20c550 closing exc is None
26/01/08 16:23:00 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:23:00 INFO ClusterLoadMonitor: Added query with execution ID:57. Current active queries:1
26/01/08 16:23:00 INFO ClusterLoadMonitor: Removed query with execution ID:57. Current active queries:0
26/01/08 16:23:00 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:23:00 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
org.apache.spark.SparkException: [JDBC_EXTERNAL_ENGINE_SYNTAX_ERROR.DURING_OUTPUT_SCHEMA_RESOLUTION] JDBC external engine syntax error. The error was caused by the query SELECT * FROM (/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test) SPARK_GEN_SUBQ_61 WHERE 1=0. error: syntax error or access rule violation - invalid syntax. The error occurred during output schema resolution. SQLSTATE: 42000
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.sql.SQLException: error: syntax error or access rule violation - invalid syntax
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:62)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:59)
	at scala.collection.IterableOnceOps.collectFirst(IterableOnce.scala:1256)
	at scala.collection.IterableOnceOps.collectFirst$(IterableOnce.scala:1248)
	at scala.collection.AbstractIterable.collectFirst(Iterable.scala:935)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$1(GrpcExceptionWrapper.scala:59)
	at scala.Option.flatMap(Option.scala:283)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:58)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcPrepareStatementClient$$anon$1.onError(JdbcPrepareStatementClient.scala:33)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:23:01 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5596054112887670495_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:23:01 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5596054112887670495_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:23:01 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:23:01 INFO ProgressReporter$: Added result fetcher for 1767888395088_5578434263993437816_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:23:01 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:23:01 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:23:01 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:23:01 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/cc6c7f16-c185-4904-aeb4-639b74f041b0] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:23:01 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:23:01 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:23:01 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:23:01 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:23:01 INFO DispatcherImpl: Grpc session requested for grpc-session-bdccf3e7-3b9e-4b8e-aa58-34fe6ab07cb8: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:23:01 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:23:01 INFO DispatcherImpl: Sandbox grpc-session-bdccf3e7-3b9e-4b8e-aa58-34fe6ab07cb8 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:23:01 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/cc6c7f16-c185-4904-aeb4-639b74f041b0] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a271c1a method=PUT
26/01/08 16:23:01 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/cc6c7f16-c185-4904-aeb4-639b74f041b0] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:23:04 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1 AS test) SPARK_GEN_SUBQ_62 WHERE 1=0
26/01/08 16:23:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@25b35a07 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:23:16 INFO ApiClient: Connection with key Some(5207805391053782721) has been finalized and removed.
26/01/08 16:23:16 INFO ApiClient: Connection with key Some(-5092163293908269461) has been finalized and removed.
26/01/08 16:23:16 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: UNKNOWN: Application error processing RPC
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:23:16 INFO DispatcherImpl: Grpc session: grpc-session-bdccf3e7-3b9e-4b8e-aa58-34fe6ab07cb8 closing exc is None
26/01/08 16:23:16 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:23:16 INFO QueryPlanningTracker: Query phase analysis took 14s before execution.
26/01/08 16:23:16 INFO ClusterLoadMonitor: Added query with execution ID:58. Current active queries:1
26/01/08 16:23:16 INFO ClusterLoadMonitor: Removed query with execution ID:58. Current active queries:0
26/01/08 16:23:16 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:23:16 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.lang.RuntimeException: Connection was closed before the operation completed.
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
	at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
	at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
		at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
		at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
		at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:796)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:762)
		at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:803)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:23:16 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5578434263993437816_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:23:16 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5578434263993437816_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:23:16 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:23:16 INFO ProgressReporter$: Added result fetcher for 1767888395088_5663509088015945062_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:23:16 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/f64a332c-20c3-45e0-99cb-2fa197295787] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:23:16 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:23:16 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/f64a332c-20c3-45e0-99cb-2fa197295787] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a271caa method=PUT
26/01/08 16:23:16 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/f64a332c-20c3-45e0-99cb-2fa197295787] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:23:17 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:23:17 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:23:17 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:23:17 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:23:17 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:23:17 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:23:17 INFO DispatcherImpl: Grpc session requested for grpc-session-ff2a19e1-d2d6-40ef-878b-4bec78b474f8: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:23:17 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:23:17 INFO DispatcherImpl: Sandbox grpc-session-ff2a19e1-d2d6-40ef-878b-4bec78b474f8 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:23:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:23:20 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT COUNT(*) AS flight_count FROM Flight) SPARK_GEN_SUBQ_63 WHERE 1=0
26/01/08 16:23:26 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x583dfac3]'
26/01/08 16:23:36 INFO ApiClient: Connection with key Some(-982416782572804788) has been finalized and removed.
26/01/08 16:23:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@63f84531 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:24:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@699b3d71 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:24:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:24:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2e0599aa size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:25:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@11d98d0f size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:25:09 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xe4a2bd95]'
26/01/08 16:25:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:25:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@30a993c8 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:26:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@694019 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:26:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:26:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2234177 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:26:42 INFO ProgressReporter$: [593 occurrences] Reporting partial results for running commands: 1767888395088_5663509088015945062_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:26:50 INFO SandboxWarmpool: Shutting down sandbox pool due to inactivity for more than 5 minutes.
26/01/08 16:26:50 INFO SandboxMemoryHolderImpl: MemoryConsumer released 377487360 bytes for WARMPOOL, currently used: 0
26/01/08 16:26:52 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:26:52 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:26:52 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:26:53 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:26:53 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:26:53 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 226 milliseconds)
26/01/08 16:26:53 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:26:53 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:26:53 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:26:53 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:26:53 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:26:53 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 213 milliseconds)
26/01/08 16:26:56 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x98026e93]'
26/01/08 16:27:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@5f4f15c9 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:27:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:27:18 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:27:18 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 16:27:18 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 656
26/01/08 16:27:19 INFO DriverCorral: DBFS health check ok
26/01/08 16:27:19 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:27:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:27:19 INFO DriverCorral: Metastore health check ok
26/01/08 16:27:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2384fa3b size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:27:50 INFO ArmeriaCommChannelWebSocketHandler: [session: 618750256] onWebSocketClose with statusCode:1001 Endpoint unavailable, reason: java.util.concurrent.TimeoutException: Idle timeout expired: 300000/300000 ms
26/01/08 16:27:50 INFO ArmeriaCommChannelWebSocketHandler: [session: 618750256] onWebSocketComplete
26/01/08 16:27:50 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 618750256] Stop ArmeriaMessageSendTask
26/01/08 16:27:50 INFO ArmeriaCommChannelWebSocketHandler: [session: 621452219] onWebSocketSubscribe with headers: Map(db-outgoing-buffer-throttler-burst -> List(60000000), db-outgoing-buffer-throttler-steady-rate -> List(6000000), db-outgoing-buffer-throttler-warning-interval-sec -> List(60))
26/01/08 16:27:50 INFO OutgoingDirectNotebookBufferRateLimiter: OutgoingDirectNotebookBufferRateLimiter initialized with clusterBurst=60000000, clusterSteadyRate=6000000 
26/01/08 16:27:50 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 621452219] Start ArmeriaMessageSendTask
26/01/08 16:28:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@69a70fda size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:28:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:28:33 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xd664ac9c]'
26/01/08 16:28:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7117c360 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:29:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@56d171d6 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:29:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:29:20 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:29:25 INFO DAGScheduler: Asked to cancel job group 1767888395088_5663509088015945062_79d4619fa3904f3683a8b1f3c79aef1d with cancelFutureJobs=false
26/01/08 16:29:25 WARN DAGScheduler: Failed to cancel job group 1767888395088_5663509088015945062_79d4619fa3904f3683a8b1f3c79aef1d. Cannot find active jobs for it.
26/01/08 16:29:25 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:29:26 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5663509088015945062_79d4619fa3904f3683a8b1f3c79aef1d
26/01/08 16:29:26 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767888395088,None,Some(79d4619fa3904f3683a8b1f3c79aef1d)).
26/01/08 16:29:30 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:30 INFO ProgressReporter$: Added result fetcher for 1767888395088_9160384734423504227_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:30 INFO ProgressReporter$: [6 occurrences] Reporting progress for running commands: 1767888395088_9160384734423504227_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:30 INFO ProgressReporter$: Removed result fetcher for 1767888395088_9160384734423504227_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:30 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_9160384734423504227_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:30 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:30 INFO ProgressReporter$: Added result fetcher for 1767888395088_7947890815341028006_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:30 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
26/01/08 16:29:31 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:29:31 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(686ca54d)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604/jdbc_drivers/__unitystorage/schemas/b8494da4-84bb-491e-8f20-8be304588f99/volumes/dca69f58-0089-490e-8659-549c43d0ccc6 with credential = CredentialScopeADLSTokenProvider with jvmId = 656
26/01/08 16:29:31 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7947890815341028006_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:31 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7947890815341028006_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:31 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:31 INFO ProgressReporter$: Added result fetcher for 1767888395088_4930806377537111180_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:31 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:29:31 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:29:31 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:29:31 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.030401 ms.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:31 INFO CurrentQueryContext: Thread Thread[Thread-184,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.execution.command.CreateExternalUserDefinedFunctionCommand).
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:31 INFO ClusterLoadMonitor: Added query with execution ID:59. Current active queries:1
26/01/08 16:29:31 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:31 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:31 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:31 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:29:31 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:29:31 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:29:31 WARN SimpleFunctionRegistry: The function connectiontest replaced a previously registered function.
26/01/08 16:29:31 INFO ClusterLoadMonitor: Removed query with execution ID:59. Current active queries:0
26/01/08 16:29:31 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:31 INFO CurrentQueryContext: Thread Thread[Thread-184,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:29:31 INFO QueryProfileListener: Query profile sent to logger, seq number: 38, app id: local-1767888418069
26/01/08 16:29:31 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 272in the parser. Driver memory: 7888437248.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:29:31 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.048165 ms.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:31 INFO ClusterLoadMonitor: Added query with execution ID:60. Current active queries:1
26/01/08 16:29:31 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 4, computed in 0 ms.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:31 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:31 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:31 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:31 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:29:31 INFO CodeGenerator: Code generated in 24.310644 ms
26/01/08 16:29:31 INFO SparkContext: Starting job: wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27
26/01/08 16:29:31 INFO DAGScheduler: Got job 4 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) with 1 output partitions
26/01/08 16:29:31 INFO DAGScheduler: Final stage: ResultStage 4 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27)
26/01/08 16:29:31 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:29:31 INFO DAGScheduler: Missing parents: List()
26/01/08 16:29:31 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27), which has no missing parents
26/01/08 16:29:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) with jobGroupId 1767888395088_4930806377537111180_a7e03e1a29a1442c8abafce5434ba77b and executionId 60 (first 15 tasks are for partitions Vector(0))
26/01/08 16:29:31 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
26/01/08 16:29:31 INFO TaskSetManager: TaskSet 4.0 using PreferredLocationsV1
26/01/08 16:29:31 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767888395088, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767888395088. Created 1767888395088 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 16:29:31 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool 1767888395088
26/01/08 16:29:31 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:29:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 180.1 KiB, free 4.2 GiB)
26/01/08 16:29:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 56.8 KiB, free 4.2 GiB)
26/01/08 16:29:31 INFO SparkContext: Created broadcast 4 from broadcast at TaskSetManager.scala:848
26/01/08 16:29:31 INFO Executor: Running task 0.0 in stage 4.0 (TID 6)
26/01/08 16:29:31 INFO TransportClientFactory: Found inactive connection to /10.139.64.4:40407, creating a new one.
26/01/08 16:29:31 INFO TransportClientFactory: Successfully created connection to /10.139.64.4:40407 after 1 ms (0 ms spent in bootstraps)
26/01/08 16:29:31 INFO CodeGenerator: Code generated in 17.247877 ms
26/01/08 16:29:31 INFO CodeGenerator: Code generated in 19.178967 ms
26/01/08 16:29:31 INFO CodeGenerator: Code generated in 20.29172 ms
26/01/08 16:29:31 INFO EvalExternalUDFExec: Times: init = 55 ms
26/01/08 16:29:31 INFO CodeGenerator: Code generated in 29.20992 ms
26/01/08 16:29:31 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 1376832
 Duration: 35
26/01/08 16:29:31 INFO ExternalUDFRunner: Getting mounts: None None Some(/local_disk0/.ephemeral_nfs/cluster_libraries/python) Some(/local_disk0/.ephemeral_nfs/repl_tmp_data/ReplId-19b9e-5c1b5-0) None
26/01/08 16:29:31 INFO ExternalUDFRunner: Creating payload with mountConfig: List()
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:29:31 INFO DispatcherImpl: Session requested for no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57: Map(executionId -> 60, stageId -> 4, taskAttemptId -> 6, DB_SESSION_UUID -> f53ae975-90ca-450a-9977-b7bcfa997cc0) - using apiType = ApiAdapter and executionType = SCALAR.
26/01/08 16:29:31 INFO DispatcherImpl: Session requested: no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:29:31 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:<unset-channel-id> UsageRefCount increased to 1.
26/01/08 16:29:31 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:<unset-channel-id> TaskContextRefCount increased to 1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:29:31 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:<unset-channel-id> transition from EMPTY to INITIALIZING finished
26/01/08 16:29:31 INFO SandboxMemoryHolder: Acquired 314572800 bytes for no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57, currently used: 314815422
26/01/08 16:29:31 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:<unset-channel-id> got memoryConsumer no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:29:31 WARN WarmpoolSandboxChannelFactory: Acquiring from warmpool failed, fallback to direct creation.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:29:31 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:29:34 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
26/01/08 16:29:36 INFO PlainSandboxChannelFactory: Got sandbox: 1753826202148715167
26/01/08 16:29:36 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:1753826202148715167 got sandboxChannel 1753826202148715167
26/01/08 16:29:36 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:1753826202148715167 transition from INITIALIZING to INITIALIZED finished
26/01/08 16:29:36 INFO DispatcherImpl: Mapping sandbox 1753826202148715167 to session no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:29:36 INFO UDFSessionFactory: UDFSession is created.
26/01/08 16:29:36 INFO UDFSession: Session (1753826202148715167): setting payload.
26/01/08 16:29:36 INFO UDFSession: Session (1753826202148715167): setup done.
26/01/08 16:29:36 INFO Dispatcher: Session None completion listener called
26/01/08 16:29:36 INFO DispatcherImpl: Session: no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57, sandbox: UDFChannel(state='INITIALIZED', id=1753826202148715167, usageRefCount=1, taskContextRefCount=1) reclaiming.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:29:36 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:1753826202148715167 UsageRefCount decreased to 0.
26/01/08 16:29:36 INFO DispatcherImpl: disposeSandboxLocked is called for sandbox UDFChannel(state='INITIALIZED', id=1753826202148715167, usageRefCount=0, taskContextRefCount=1) session no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57 with throwable: None
26/01/08 16:29:36 INFO DispatcherImpl: Unmapped sandbox 1753826202148715167 from session no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57
26/01/08 16:29:36 INFO UDFChannel: Disposing sandbox (isError=false): UDFChannel(state='INITIALIZED', id=1753826202148715167, usageRefCount=0, taskContextRefCount=1)
26/01/08 16:29:36 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:1753826202148715167 transition from INITIALIZED to DISPOSING finished
26/01/08 16:29:36 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:1753826202148715167 transition from DISPOSING to DISPOSED finished
26/01/08 16:29:36 INFO DispatcherImpl: disposeSandboxLocked is called for sandbox UDFChannel(state='DISPOSED', id=1753826202148715167, usageRefCount=0, taskContextRefCount=1) session no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57 with throwable: Some(java.lang.RuntimeException: ManagedChannel failed, is in state SHUTDOWN)
26/01/08 16:29:36 INFO DispatcherImpl: Could not unmap sandbox 1753826202148715167 from session no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57
26/01/08 16:29:36 INFO UDFChannel: key:no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57-channelId:1753826202148715167 TaskContextRefCount decreased to 0.
26/01/08 16:29:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 6). 4537 bytes result sent to driver
26/01/08 16:29:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 4940 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:29:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:29:36 INFO DAGScheduler: ResultStage 4 (wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27) finished in 4951 ms
26/01/08 16:29:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:29:36 INFO TaskSchedulerImpl: Canceling stage 4
26/01/08 16:29:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Result stage finished
26/01/08 16:29:36 INFO DAGScheduler: Job 4 finished: wrapper at /root/.ipykernel/2426/command-7730399508088061-2893810390:27, took 4956.261866 ms
26/01/08 16:29:36 INFO ClusterLoadMonitor: Removed query with execution ID:60. Current active queries:0
26/01/08 16:29:36 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:36 INFO QueryProfileListener: Query profile sent to logger, seq number: 39, app id: local-1767888418069
26/01/08 16:29:36 INFO ProgressReporter$: Removed result fetcher for 1767888395088_4930806377537111180_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:36 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_4930806377537111180_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:36 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:36 INFO ProgressReporter$: Added result fetcher for 1767888395088_7827452891028591627_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@52000c15 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:29:36 INFO PlainSandboxChannelFactory: Got sandbox: 342229341528214757
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:29:36 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:29:37 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
26/01/08 16:29:37 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7827452891028591627_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:37 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7827452891028591627_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:37 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:37 INFO ProgressReporter$: Added result fetcher for 1767888395088_6700824468971415723_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:37 INFO PlainSandboxChannelFactory: Got sandbox: 7760370372094427099
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:29:37 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.externalUdf.scala.clientImageOnClassicFix.enabled from default: true.
26/01/08 16:29:37 INFO SandboxMemoryHolderImpl: MemoryConsumer released 314572800 bytes for no-reuse-3cde4277-f95f-446d-9b9c-00aaefe99d57, currently used: 242622
26/01/08 16:29:37 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:37 INFO DriverFactory: Routing driver instance 623508037 created for server address 3f1f827a.databases.neo4j.io:7687
26/01/08 16:29:38 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 19 DFA states in the parser. Total cached DFA states: 291in the parser. Driver memory: 7888437248.
26/01/08 16:29:38 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 47.768793 ms.
26/01/08 16:29:38 INFO QueryPlanningTracker: Query phase analysis took 1s before execution.
26/01/08 16:29:38 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 1.766521 ms.
26/01/08 16:29:38 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:38 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 1.182546 ms.
26/01/08 16:29:38 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:38 INFO ClusterLoadMonitor: Added query with execution ID:61. Current active queries:1
26/01/08 16:29:38 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 10, computed in 0 ms.
26/01/08 16:29:38 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:38 INFO V2ScanRelationPushDown: 
Output: message#55, value#56L
           
26/01/08 16:29:38 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:38 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:38 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:38 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:38 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:38 INFO CodeGenerator: Code generated in 30.211728 ms
26/01/08 16:29:38 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:29:38 INFO DAGScheduler: Got job 5 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 16:29:38 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:29:38 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:29:38 INFO DAGScheduler: Missing parents: List()
26/01/08 16:29:38 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[26] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:29:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[26] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_6700824468971415723_a7e03e1a29a1442c8abafce5434ba77b and executionId 61 (first 15 tasks are for partitions Vector(0))
26/01/08 16:29:38 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
26/01/08 16:29:38 INFO TaskSetManager: TaskSet 5.0 using PreferredLocationsV1
26/01/08 16:29:38 INFO FairSchedulableBuilder: Added task set TaskSet_5.0 tasks to pool 1767888395088
26/01/08 16:29:38 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:29:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.0 KiB, free 4.2 GiB)
26/01/08 16:29:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 4.2 GiB)
26/01/08 16:29:38 INFO SparkContext: Created broadcast 5 from broadcast at TaskSetManager.scala:848
26/01/08 16:29:38 INFO Executor: Running task 0.0 in stage 5.0 (TID 7)
26/01/08 16:29:38 INFO CodeGenerator: Code generated in 50.325191 ms
26/01/08 16:29:39 INFO Neo4jPartitionReader: Running the following query on Neo4j: WITH $scriptResult AS scriptResult RETURN 'Spark Connector Works!' AS message, 1 AS value SKIP 0 LIMIT 21
26/01/08 16:29:39 INFO Executor: Finished task 0.0 in stage 5.0 (TID 7). 1463 bytes result sent to driver
26/01/08 16:29:39 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 245 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:29:39 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:29:39 INFO DAGScheduler: ResultStage 5 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 295 ms
26/01/08 16:29:39 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:29:39 INFO TaskSchedulerImpl: Canceling stage 5
26/01/08 16:29:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Result stage finished
26/01/08 16:29:39 INFO DAGScheduler: Job 5 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 316.687907 ms
26/01/08 16:29:39 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.050404 ms.
26/01/08 16:29:39 INFO CodeGenerator: Code generated in 8.50496 ms
26/01/08 16:29:39 INFO ClusterLoadMonitor: Removed query with execution ID:61. Current active queries:0
26/01/08 16:29:39 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:39 INFO QueryProfileListener: Query profile sent to logger, seq number: 40, app id: local-1767888418069
26/01/08 16:29:39 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6700824468971415723_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:39 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6700824468971415723_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:39 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:39 INFO ProgressReporter$: Added result fetcher for 1767888395088_7948242864248964211_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:39 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:39 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:39 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM Aircraft WHERE 1=0
26/01/08 16:29:40 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
26/01/08 16:29:42 INFO PlainSandboxChannelFactory: Got sandbox: 2261979409111889769
26/01/08 16:29:42 INFO PlainSandboxChannelFactory: Got sandbox: 5940387832007603297
26/01/08 16:29:43 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 4 DFA states in the parser. Total cached DFA states: 295in the parser. Driver memory: 7888437248.
26/01/08 16:29:43 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.017794 ms.
26/01/08 16:29:43 INFO QueryPlanningTracker: Query phase analysis took 4s before execution.
26/01/08 16:29:43 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.060874 ms.
26/01/08 16:29:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:43 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.070742 ms.
26/01/08 16:29:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:43 INFO ClusterLoadMonitor: Added query with execution ID:62. Current active queries:1
26/01/08 16:29:43 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 30, computed in 0 ms.
26/01/08 16:29:43 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:43 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:43 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:43 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:44 INFO JDBCDatabaseMetadata: closed connection during metadata fetch
26/01/08 16:29:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:44 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:44 INFO CodeGenerator: Code generated in 19.743407 ms
26/01/08 16:29:44 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:29:44 INFO DAGScheduler: Got job 6 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 16:29:44 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:29:44 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:29:44 INFO DAGScheduler: Missing parents: List()
26/01/08 16:29:44 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[28] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:29:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[28] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_7948242864248964211_a7e03e1a29a1442c8abafce5434ba77b and executionId 62 (first 15 tasks are for partitions Vector(0))
26/01/08 16:29:44 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/01/08 16:29:44 INFO TaskSetManager: TaskSet 6.0 using PreferredLocationsV1
26/01/08 16:29:44 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767888395088, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767888395088. Created 1767888395088 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 16:29:44 INFO FairSchedulableBuilder: Added task set TaskSet_6.0 tasks to pool 1767888395088
26/01/08 16:29:44 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:29:44 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 166.1 KiB, free 4.2 GiB)
26/01/08 16:29:44 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 50.7 KiB, free 4.2 GiB)
26/01/08 16:29:44 INFO SparkContext: Created broadcast 6 from broadcast at TaskSetManager.scala:848
26/01/08 16:29:44 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
26/01/08 16:29:44 INFO JDBCRDD: Generated JDBC query to fetch data: SELECT "v$id","aircraft_id","tail_number","icao24","model","operator","manufacturer" FROM Aircraft     LIMIT 6 
26/01/08 16:29:44 INFO CodeGenerator: Code generated in 24.236302 ms
26/01/08 16:29:44 INFO JDBCRDD: closed connection
26/01/08 16:29:44 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1842 bytes result sent to driver
26/01/08 16:29:44 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 598 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:29:44 INFO TaskSchedulerImpl: Removed TaskSet 6.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:29:44 INFO DAGScheduler: ResultStage 6 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 628 ms
26/01/08 16:29:44 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:29:44 INFO TaskSchedulerImpl: Canceling stage 6
26/01/08 16:29:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Result stage finished
26/01/08 16:29:44 INFO DAGScheduler: Job 6 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 633.810801 ms
26/01/08 16:29:44 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.062438 ms.
26/01/08 16:29:44 INFO CodeGenerator: Code generated in 8.252811 ms
26/01/08 16:29:44 INFO ClusterLoadMonitor: Removed query with execution ID:62. Current active queries:0
26/01/08 16:29:44 INFO QueryProfileListener: Query profile sent to logger, seq number: 41, app id: local-1767888418069
26/01/08 16:29:44 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:45 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7948242864248964211_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:45 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7948242864248964211_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:45 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:45 INFO ProgressReporter$: Added result fetcher for 1767888395088_7092456058671069012_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:45 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:45 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:45 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1 AS value) SPARK_GEN_SUBQ_64 WHERE 1=0
26/01/08 16:29:45 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 5 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 16:29:45 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.016901 ms.
26/01/08 16:29:45 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:45 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.032062 ms.
26/01/08 16:29:45 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:45 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.037103 ms.
26/01/08 16:29:45 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:45 INFO ClusterLoadMonitor: Added query with execution ID:63. Current active queries:1
26/01/08 16:29:45 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 6, computed in 0 ms.
26/01/08 16:29:45 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:45 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:45 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:45 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:45 INFO JDBCDatabaseMetadata: closed connection during metadata fetch
26/01/08 16:29:45 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:45 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:45 INFO CodeGenerator: Code generated in 19.507163 ms
26/01/08 16:29:45 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:29:45 INFO DAGScheduler: Got job 7 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 16:29:45 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:29:45 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:29:45 INFO DAGScheduler: Missing parents: List()
26/01/08 16:29:45 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:29:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_7092456058671069012_a7e03e1a29a1442c8abafce5434ba77b and executionId 63 (first 15 tasks are for partitions Vector(0))
26/01/08 16:29:45 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/01/08 16:29:45 INFO TaskSetManager: TaskSet 7.0 using PreferredLocationsV1
26/01/08 16:29:45 INFO FairSchedulableBuilder: Added task set TaskSet_7.0 tasks to pool 1767888395088
26/01/08 16:29:45 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:29:45 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 163.0 KiB, free 4.2 GiB)
26/01/08 16:29:45 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 50.0 KiB, free 4.2 GiB)
26/01/08 16:29:45 INFO SparkContext: Created broadcast 7 from broadcast at TaskSetManager.scala:848
26/01/08 16:29:45 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
26/01/08 16:29:45 INFO JDBCRDD: Generated JDBC query to fetch data: SELECT "value" FROM (SELECT 1 AS value) SPARK_GEN_SUBQ_64     LIMIT 21 
26/01/08 16:29:45 INFO CodeGenerator: Code generated in 29.971647 ms
26/01/08 16:29:45 INFO JDBCRDD: closed connection
26/01/08 16:29:45 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1463 bytes result sent to driver
26/01/08 16:29:45 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 249 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:29:45 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:29:45 INFO DAGScheduler: ResultStage 7 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 259 ms
26/01/08 16:29:45 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:29:45 INFO TaskSchedulerImpl: Canceling stage 7
26/01/08 16:29:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Result stage finished
26/01/08 16:29:45 INFO DAGScheduler: Job 7 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 263.008805 ms
26/01/08 16:29:45 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.053206 ms.
26/01/08 16:29:45 INFO CodeGenerator: Code generated in 5.270158 ms
26/01/08 16:29:45 INFO ClusterLoadMonitor: Removed query with execution ID:63. Current active queries:0
26/01/08 16:29:45 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:45 INFO QueryProfileListener: Query profile sent to logger, seq number: 42, app id: local-1767888418069
26/01/08 16:29:46 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7092456058671069012_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:46 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7092456058671069012_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:46 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:46 INFO ProgressReporter$: Added result fetcher for 1767888395088_5434432230194452868_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:46 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:46 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:46 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT COUNT(*) AS flight_count FROM Flight) SPARK_GEN_SUBQ_86 WHERE 1=0
26/01/08 16:29:46 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 16:29:46 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.013614 ms.
26/01/08 16:29:46 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:46 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.043058 ms.
26/01/08 16:29:46 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:46 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.036972 ms.
26/01/08 16:29:46 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:46 INFO ClusterLoadMonitor: Added query with execution ID:64. Current active queries:1
26/01/08 16:29:46 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 6, computed in 0 ms.
26/01/08 16:29:46 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:46 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:46 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:46 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:46 INFO JDBCDatabaseMetadata: closed connection during metadata fetch
26/01/08 16:29:46 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:46 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:46 INFO CodeGenerator: Code generated in 19.927557 ms
26/01/08 16:29:46 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:29:46 INFO DAGScheduler: Got job 8 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 16:29:46 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:29:46 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:29:46 INFO DAGScheduler: Missing parents: List()
26/01/08 16:29:46 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[32] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:29:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[32] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_5434432230194452868_a7e03e1a29a1442c8abafce5434ba77b and executionId 64 (first 15 tasks are for partitions Vector(0))
26/01/08 16:29:46 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/01/08 16:29:46 INFO TaskSetManager: TaskSet 8.0 using PreferredLocationsV1
26/01/08 16:29:46 INFO FairSchedulableBuilder: Added task set TaskSet_8.0 tasks to pool 1767888395088
26/01/08 16:29:46 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:29:46 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 163.1 KiB, free 4.2 GiB)
26/01/08 16:29:46 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 50.0 KiB, free 4.2 GiB)
26/01/08 16:29:46 INFO SparkContext: Created broadcast 8 from broadcast at TaskSetManager.scala:848
26/01/08 16:29:46 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
26/01/08 16:29:46 INFO JDBCRDD: Generated JDBC query to fetch data: SELECT "flight_count" FROM (SELECT COUNT(*) AS flight_count FROM Flight) SPARK_GEN_SUBQ_86     LIMIT 21 
26/01/08 16:29:47 INFO CodeGenerator: Code generated in 31.68875 ms
26/01/08 16:29:47 INFO JDBCRDD: closed connection
26/01/08 16:29:47 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 1463 bytes result sent to driver
26/01/08 16:29:47 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 349 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:29:47 INFO TaskSchedulerImpl: Removed TaskSet 8.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:29:47 INFO DAGScheduler: ResultStage 8 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 358 ms
26/01/08 16:29:47 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:29:47 INFO TaskSchedulerImpl: Canceling stage 8
26/01/08 16:29:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Result stage finished
26/01/08 16:29:47 INFO DAGScheduler: Job 8 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 362.549924 ms
26/01/08 16:29:47 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.03826 ms.
26/01/08 16:29:47 INFO ClusterLoadMonitor: Removed query with execution ID:64. Current active queries:0
26/01/08 16:29:47 INFO QueryProfileListener: Query profile sent to logger, seq number: 43, app id: local-1767888418069
26/01/08 16:29:47 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:47 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5434432230194452868_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:47 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5434432230194452868_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:47 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:47 INFO ProgressReporter$: Added result fetcher for 1767888395088_6958481556386282925_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:47 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:47 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:47 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT COUNT(*) AS cnt
                           FROM Flight f
                           NATURAL JOIN DEPARTS_FROM r
                           NATURAL JOIN Airport a) SPARK_GEN_SUBQ_108 WHERE 1=0
26/01/08 16:29:48 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 16:29:48 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.01378 ms.
26/01/08 16:29:48 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:48 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.045141 ms.
26/01/08 16:29:48 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:48 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.249918 ms.
26/01/08 16:29:48 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:48 INFO ClusterLoadMonitor: Added query with execution ID:65. Current active queries:1
26/01/08 16:29:48 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 6, computed in 0 ms.
26/01/08 16:29:48 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:48 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:48 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:48 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:48 INFO JDBCDatabaseMetadata: closed connection during metadata fetch
26/01/08 16:29:48 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:48 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:48 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:29:48 INFO DAGScheduler: Got job 9 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 16:29:48 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:29:48 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:29:48 INFO DAGScheduler: Missing parents: List()
26/01/08 16:29:48 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[34] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:29:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[34] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_6958481556386282925_a7e03e1a29a1442c8abafce5434ba77b and executionId 65 (first 15 tasks are for partitions Vector(0))
26/01/08 16:29:48 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
26/01/08 16:29:48 INFO TaskSetManager: TaskSet 9.0 using PreferredLocationsV1
26/01/08 16:29:48 INFO FairSchedulableBuilder: Added task set TaskSet_9.0 tasks to pool 1767888395088
26/01/08 16:29:48 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:29:48 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 163.3 KiB, free 4.2 GiB)
26/01/08 16:29:48 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 50.1 KiB, free 4.2 GiB)
26/01/08 16:29:48 INFO SparkContext: Created broadcast 9 from broadcast at TaskSetManager.scala:848
26/01/08 16:29:48 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)
26/01/08 16:29:48 INFO JDBCRDD: Generated JDBC query to fetch data: SELECT "cnt" FROM (SELECT COUNT(*) AS cnt
                           FROM Flight f
                           NATURAL JOIN DEPARTS_FROM r
                           NATURAL JOIN Airport a) SPARK_GEN_SUBQ_108     LIMIT 21 
26/01/08 16:29:49 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
26/01/08 16:29:49 INFO JDBCRDD: closed connection
26/01/08 16:29:49 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 1463 bytes result sent to driver
26/01/08 16:29:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 721 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:29:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:29:49 INFO DAGScheduler: ResultStage 9 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 727 ms
26/01/08 16:29:49 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:29:49 INFO TaskSchedulerImpl: Canceling stage 9
26/01/08 16:29:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Result stage finished
26/01/08 16:29:49 INFO DAGScheduler: Job 9 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 732.901002 ms
26/01/08 16:29:49 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.050884 ms.
26/01/08 16:29:49 INFO ClusterLoadMonitor: Removed query with execution ID:65. Current active queries:0
26/01/08 16:29:49 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:49 INFO QueryProfileListener: Query profile sent to logger, seq number: 44, app id: local-1767888418069
26/01/08 16:29:49 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6958481556386282925_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:49 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6958481556386282925_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:49 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:49 INFO ProgressReporter$: Added result fetcher for 1767888395088_5039283254701636909_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:49 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:29:49 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.035954 ms.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:49 INFO CurrentQueryContext: Thread Thread[Thread-184,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.DropConnectionCommand).
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:49 INFO ClusterLoadMonitor: Added query with execution ID:66. Current active queries:1
26/01/08 16:29:49 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:49 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:49 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:49 INFO ClusterLoadMonitor: Removed query with execution ID:66. Current active queries:0
26/01/08 16:29:49 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:49 INFO CurrentQueryContext: Thread Thread[Thread-184,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:29:49 INFO QueryProfileListener: Query profile sent to logger, seq number: 45, app id: local-1767888418069
26/01/08 16:29:49 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:29:49 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.032431 ms.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:49 INFO CurrentQueryContext: Thread Thread[Thread-184,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.CreateConnectionCommand).
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:49 INFO ClusterLoadMonitor: Added query with execution ID:67. Current active queries:1
26/01/08 16:29:49 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:49 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:49 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:49 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:49 INFO ClusterLoadMonitor: Removed query with execution ID:67. Current active queries:0
26/01/08 16:29:49 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:49 INFO CurrentQueryContext: Thread Thread[Thread-184,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:29:49 INFO QueryProfileListener: Query profile sent to logger, seq number: 46, app id: local-1767888418069
26/01/08 16:29:50 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5039283254701636909_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:50 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5039283254701636909_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:50 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:50 INFO ProgressReporter$: Added result fetcher for 1767888395088_5816522549557629623_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:50 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:29:50 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.036037 ms.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:50 INFO CurrentQueryContext: Thread Thread[Thread-184,5,main]: Setting current query category as an executable command (Command is a class com.databricks.sql.managedcatalog.command.DescribeConnectionCommand).
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:50 INFO ClusterLoadMonitor: Added query with execution ID:68. Current active queries:1
26/01/08 16:29:50 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 2, computed in 0 ms.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:50 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:50 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:50 INFO ClusterLoadMonitor: Removed query with execution ID:68. Current active queries:0
26/01/08 16:29:50 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:50 INFO CurrentQueryContext: Thread Thread[Thread-184,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:29:50 INFO QueryProfileListener: Query profile sent to logger, seq number: 47, app id: local-1767888418069
26/01/08 16:29:50 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.013759 ms.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:50 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.039928 ms.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:50 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.040141 ms.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:50 INFO ClusterLoadMonitor: Added query with execution ID:69. Current active queries:1
26/01/08 16:29:50 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 10, computed in 0 ms.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:29:50 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:29:50 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:50 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:29:50 INFO CodeGenerator: Code generated in 5.522756 ms
26/01/08 16:29:50 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.051775 ms.
26/01/08 16:29:50 INFO ClusterLoadMonitor: Removed query with execution ID:69. Current active queries:0
26/01/08 16:29:50 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:29:50 INFO QueryProfileListener: Query profile sent to logger, seq number: 48, app id: local-1767888418069
26/01/08 16:29:50 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5816522549557629623_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:50 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_5816522549557629623_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:50 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:29:50 INFO ProgressReporter$: Added result fetcher for 1767888395088_5441592106705553246_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:29:50 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:50 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:29:50 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:29:50 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:29:50 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:29:50 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:29:50 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:29:50 INFO DispatcherImpl: Grpc session requested for grpc-session-9856ea69-8122-42e4-956e-1e39890410e7: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:29:50 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:29:50 INFO DispatcherImpl: Sandbox grpc-session-9856ea69-8122-42e4-956e-1e39890410e7 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:29:52 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
26/01/08 16:29:54 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1 AS test) SPARK_GEN_SUBQ_130 WHERE 1=0
26/01/08 16:29:55 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
26/01/08 16:29:55 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:29:55 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 16:29:55 INFO DispatcherImpl: Grpc session: grpc-session-ff2a19e1-d2d6-40ef-878b-4bec78b474f8 closing exc is None
26/01/08 16:29:55 INFO QueryPlanningTracker: Query phase analysis took 398s before execution.
26/01/08 16:29:56 INFO ApiClient: Connection with key Some(-955883605184183080) has been finalized and removed.
26/01/08 16:30:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3081050 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:30:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:30:24 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x5f3982c9]'
26/01/08 16:30:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4ccbdf92 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:31:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@1149581c size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:31:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:31:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@43fb9e84 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:31:42 INFO ProgressReporter$: [577 occurrences] Reporting partial results for running commands: 1767888395088_5441592106705553246_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:31:53 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:31:53 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:31:53 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:31:53 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:31:53 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:31:53 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 214 milliseconds)
26/01/08 16:31:53 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:31:53 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:31:53 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:31:53 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:31:53 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:31:53 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 198 milliseconds)
26/01/08 16:32:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@329c3b size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:32:11 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xf25ba7a1]'
26/01/08 16:32:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:32:18 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:32:18 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 16:32:18 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 656
26/01/08 16:32:19 INFO DriverCorral: DBFS health check ok
26/01/08 16:32:19 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:32:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:32:19 INFO DriverCorral: Metastore health check ok
26/01/08 16:32:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/8e8d0713-c87d-4f92-832a-7a8ff88bcee7] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:32:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/8e8d0713-c87d-4f92-832a-7a8ff88bcee7] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a27305f method=PUT
26/01/08 16:32:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/8e8d0713-c87d-4f92-832a-7a8ff88bcee7] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:32:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@1297f4e0 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:33:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4945c716 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:33:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:33:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@56557f04 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:34:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4292e2b1 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:34:10 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x055b323b]'
26/01/08 16:34:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:34:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/4ac441dd-99b8-4f56-a884-9ddd860b4d53] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:34:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/4ac441dd-99b8-4f56-a884-9ddd860b4d53] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a27348d method=PUT
26/01/08 16:34:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/4ac441dd-99b8-4f56-a884-9ddd860b4d53] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:34:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6f0f2ef7 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:34:50 INFO SandboxWarmpool: Shutting down sandbox pool due to inactivity for more than 5 minutes.
26/01/08 16:35:06 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@8153977 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:35:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:35:36 INFO DAGScheduler: Asked to cancel job group 1767888395088_5441592106705553246_a7e03e1a29a1442c8abafce5434ba77b with cancelFutureJobs=false
26/01/08 16:35:36 WARN DAGScheduler: Failed to cancel job group 1767888395088_5441592106705553246_a7e03e1a29a1442c8abafce5434ba77b. Cannot find active jobs for it.
26/01/08 16:35:36 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:35:36 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5441592106705553246_a7e03e1a29a1442c8abafce5434ba77b
26/01/08 16:35:36 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767888395088,None,Some(a7e03e1a29a1442c8abafce5434ba77b)).
26/01/08 16:35:36 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7545da2e size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:35:42 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:35:42 INFO ProgressReporter$: Added result fetcher for 1767888395088_4812196440855657557_0c9cc576fc0744bdbc06848703d76231
26/01/08 16:35:42 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:35:42 INFO ProgressReporter$: [19 occurrences] Reporting progress for running commands: 1767888395088_4812196440855657557_0c9cc576fc0744bdbc06848703d76231
26/01/08 16:35:42 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:35:42 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:35:42 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:35:42 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:35:42 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:35:42 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:35:42 INFO DispatcherImpl: Grpc session requested for grpc-session-61a2a191-73e6-4252-bc3c-5e70b0e9ed5f: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:35:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:35:42 INFO DispatcherImpl: Sandbox grpc-session-61a2a191-73e6-4252-bc3c-5e70b0e9ed5f - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:35:45 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test) SPARK_GEN_SUBQ_131 WHERE 1=0
26/01/08 16:35:46 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:35:46 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 16:35:46 INFO DispatcherImpl: Grpc session: grpc-session-61a2a191-73e6-4252-bc3c-5e70b0e9ed5f closing exc is None
26/01/08 16:35:46 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 16:35:46 INFO ClusterLoadMonitor: Added query with execution ID:70. Current active queries:1
26/01/08 16:35:46 INFO ClusterLoadMonitor: Removed query with execution ID:70. Current active queries:0
26/01/08 16:35:46 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:35:46 INFO DBCEventLoggingListener: Rolling event log; numTimesRolledOver = 3
26/01/08 16:35:46 INFO DBCEventLoggingListener: Rolled active log file /databricks/driver/eventlogs/5894523486654786537/eventlog to /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-30, size = 1872067
26/01/08 16:35:46 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
org.apache.spark.SparkException: [JDBC_EXTERNAL_ENGINE_SYNTAX_ERROR.DURING_OUTPUT_SCHEMA_RESOLUTION] JDBC external engine syntax error. The error was caused by the query SELECT * FROM (/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test) SPARK_GEN_SUBQ_131 WHERE 1=0. error: syntax error or access rule violation - invalid syntax. The error occurred during output schema resolution. SQLSTATE: 42000
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.sql.SQLException: error: syntax error or access rule violation - invalid syntax
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:62)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:59)
	at scala.collection.IterableOnceOps.collectFirst(IterableOnce.scala:1256)
	at scala.collection.IterableOnceOps.collectFirst$(IterableOnce.scala:1248)
	at scala.collection.AbstractIterable.collectFirst(Iterable.scala:935)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$1(GrpcExceptionWrapper.scala:59)
	at scala.Option.flatMap(Option.scala:283)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:58)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcPrepareStatementClient$$anon$1.onError(JdbcPrepareStatementClient.scala:33)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 16:35:46 INFO DBCEventLoggingListener: Logging events to eventlogs/5894523486654786537/eventlog
26/01/08 16:35:46 INFO DBCEventLoggingListener: Compressed rolled file /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-30 to /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-30.gz in 58ms, size = 343380
26/01/08 16:35:46 INFO DBCEventLoggingListener: Deleted rolled file eventlogs/5894523486654786537/eventlog-2026-01-08--16-30
26/01/08 16:35:46 INFO ProgressReporter$: Removed result fetcher for 1767888395088_4812196440855657557_0c9cc576fc0744bdbc06848703d76231
26/01/08 16:35:46 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_4812196440855657557_0c9cc576fc0744bdbc06848703d76231
26/01/08 16:35:46 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/0a36a796-cff7-4456-99ad-0fbcb3a41816] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:35:46 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/0a36a796-cff7-4456-99ad-0fbcb3a41816] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a273715 method=PUT
26/01/08 16:35:46 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/0a36a796-cff7-4456-99ad-0fbcb3a41816] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:35:56 INFO ApiClient: Connection with key Some(-3209571783288514422) has been finalized and removed.
26/01/08 16:35:58 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:35:58 INFO ProgressReporter$: Added result fetcher for 1767888395088_5627295391570957750_43fd801969ba4b3496debd8e08bbb888
26/01/08 16:35:58 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 16:35:58 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:35:58 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:35:58 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:35:58 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:35:58 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:35:58 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:35:58 INFO DispatcherImpl: Grpc session requested for grpc-session-32a8a3df-0b70-4966-8ae1-ba287fbaa443: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:35:58 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:35:58 INFO DispatcherImpl: Sandbox grpc-session-32a8a3df-0b70-4966-8ae1-ba287fbaa443 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:36:01 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1 AS test) SPARK_GEN_SUBQ_132 WHERE 1=0
26/01/08 16:36:06 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:36:06 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 16:36:06 INFO DispatcherImpl: Grpc session: grpc-session-9856ea69-8122-42e4-956e-1e39890410e7 closing exc is None
26/01/08 16:36:06 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x53f53965]'
26/01/08 16:36:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2017395d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:36:07 INFO QueryPlanningTracker: Query phase analysis took 376s before execution.
26/01/08 16:36:16 INFO ApiClient: Connection with key Some(-4061924199299661378) has been finalized and removed.
26/01/08 16:36:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:36:35 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/92264009-d0f8-4fe6-ab2e-276c47a992ef] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:36:35 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/92264009-d0f8-4fe6-ab2e-276c47a992ef] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a2738b7 method=PUT
26/01/08 16:36:35 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/92264009-d0f8-4fe6-ab2e-276c47a992ef] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:36:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@36f9c33e size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:36:42 INFO ProgressReporter$: [562 occurrences] Reporting partial results for running commands: 1767888395088_5627295391570957750_43fd801969ba4b3496debd8e08bbb888
26/01/08 16:36:52 INFO HmrNonQueryEventLogger: NQL logging rate limit was initialized to : 1 log lines per call site per second, bypass allow list: AppStatusListener.scala
26/01/08 16:36:52 WARN DeadlockDetector: Deadlock detection requested for the following reasons: Reason: {DAG_SCHEDULER_NO_ACTIVE_JOB.toString}, Count: 1
Deadlock detector did not find any deadlocks, but it is likely that a component is hanging.
26/01/08 16:36:53 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:36:53 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:36:53 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:36:53 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:36:53 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:36:53 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 239 milliseconds)
26/01/08 16:36:53 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:36:53 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:36:53 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:36:54 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:36:54 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:36:54 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 210 milliseconds)
26/01/08 16:36:59 INFO ContextCleaner: Skipping full GC. Observed GC latency: 299 s,# skipped GCs: 0
26/01/08 16:37:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@17069a17 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:37:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:37:18 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:37:18 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 1844062
 Duration: 23
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:37:19 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 16:37:19 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 656
26/01/08 16:37:19 INFO DriverCorral: DBFS health check ok
26/01/08 16:37:19 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:37:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:37:19 INFO DriverCorral: Metastore health check ok
26/01/08 16:37:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4cc9a673 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:38:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@550da366 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:38:14 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x8610b22f]'
26/01/08 16:38:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:38:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/75a31c07-e9fd-429c-ae57-0eb05f88d38e] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:38:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/75a31c07-e9fd-429c-ae57-0eb05f88d38e] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a273ce6 method=PUT
26/01/08 16:38:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/75a31c07-e9fd-429c-ae57-0eb05f88d38e] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:38:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2d157aca size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:38:42 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:39:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@ee09b5d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:39:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:39:21 INFO DAGScheduler: Asked to cancel job group 1767888395088_5627295391570957750_43fd801969ba4b3496debd8e08bbb888 with cancelFutureJobs=false
26/01/08 16:39:21 WARN DAGScheduler: Failed to cancel job group 1767888395088_5627295391570957750_43fd801969ba4b3496debd8e08bbb888. Cannot find active jobs for it.
26/01/08 16:39:21 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:39:21 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5627295391570957750_43fd801969ba4b3496debd8e08bbb888
26/01/08 16:39:21 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767888395088,None,Some(43fd801969ba4b3496debd8e08bbb888)).
26/01/08 16:39:26 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:39:26 INFO ProgressReporter$: Added result fetcher for 1767888395088_5486131923250247745_75ff6ad407ae463e969d3224e3f13ad2
26/01/08 16:39:26 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:39:27 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:39:27 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:39:27 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:39:27 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:39:27 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:39:27 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 0 ms
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:39:27 INFO DispatcherImpl: Grpc session requested for grpc-session-33693146-5bf2-4c07-b79f-2931eaefd0b1: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:39:27 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:39:27 INFO DispatcherImpl: Sandbox grpc-session-33693146-5bf2-4c07-b79f-2931eaefd0b1 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:39:30 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT COUNT(*) AS flight_count FROM Flight) SPARK_GEN_SUBQ_133 WHERE 1=0
26/01/08 16:39:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@f1b4ecc size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:39:51 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
	at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:796)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:762)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:803)
	at jdk.internal.reflect.GeneratedMethodAccessor349.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:39:51 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 16:39:51 INFO DispatcherImpl: Grpc session: grpc-session-32a8a3df-0b70-4966-8ae1-ba287fbaa443 closing exc is None
26/01/08 16:39:51 INFO QueryPlanningTracker: Query phase analysis took 233s before execution.
26/01/08 16:39:56 INFO ApiClient: Connection with key Some(6333931384704461177) has been finalized and removed.
26/01/08 16:40:06 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x6917db9d]'
26/01/08 16:40:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3735dada size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:40:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:40:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088078/c2a5d203-0d9a-4b87-a802-92804b16cc4d] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:40:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088078/c2a5d203-0d9a-4b87-a802-92804b16cc4d] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a274117 method=PUT
26/01/08 16:40:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088078/c2a5d203-0d9a-4b87-a802-92804b16cc4d] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:40:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@31f025dc size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:41:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3b407acc size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:41:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:41:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6e783e46 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:41:41 INFO ProgressReporter$: [586 occurrences] Reporting partial results for running commands: 1767888395088_5486131923250247745_75ff6ad407ae463e969d3224e3f13ad2
26/01/08 16:41:54 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:41:54 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:41:54 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:41:54 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:41:54 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:41:54 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 254 milliseconds)
26/01/08 16:41:54 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:41:54 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:41:54 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:41:54 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:41:54 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:41:54 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 221 milliseconds)
26/01/08 16:41:58 WARN SparkContext: Requesting executors is not supported by current scheduler.
26/01/08 16:41:58 INFO DeadlockDetector: Requested deadlock detection caused by: DAG_SCHEDULER_NO_ACTIVE_JOB
26/01/08 16:41:58 INFO HangingThreadDetector: Requested hanging thread detection caused by: DAG_SCHEDULER_NO_ACTIVE_JOB
26/01/08 16:41:59 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xa7753783]'
26/01/08 16:42:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7782da59 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:42:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:42:18 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:42:18 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 16:42:18 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 656
26/01/08 16:42:19 INFO DriverCorral: DBFS health check ok
26/01/08 16:42:19 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:42:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:42:19 INFO DriverCorral: Metastore health check ok
26/01/08 16:42:30 INFO DAGScheduler: Asked to cancel job group 1767888395088_5486131923250247745_75ff6ad407ae463e969d3224e3f13ad2 with cancelFutureJobs=false
26/01/08 16:42:30 WARN DAGScheduler: Failed to cancel job group 1767888395088_5486131923250247745_75ff6ad407ae463e969d3224e3f13ad2. Cannot find active jobs for it.
26/01/08 16:42:30 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5486131923250247745_75ff6ad407ae463e969d3224e3f13ad2
26/01/08 16:42:30 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767888395088,None,Some(75ff6ad407ae463e969d3224e3f13ad2)).
26/01/08 16:42:34 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:42:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@476ccfb0 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:42:38 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:42:38 INFO ProgressReporter$: Added result fetcher for 1767888395088_7748756662946361605_9e875253ec604b9d9a81fc5da46178b9
26/01/08 16:42:38 WARN JupyterKernelListener: Results buffer should be empty before command run but it is:
--start--
AnsiResult(DEBUG:ThreadMonitor:Logging python thread stack frames for MainThread and py4j threads:
DEBUG:ThreadMonitor:Logging Thread-21 (run) stack frames:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py", line 766, in run_closure
    _threading_Thread_run(self)
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 521, in run
    self.wait_for_commands()
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 593, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)

DEBUG:ThreadMonitor:Logging Thread-20 (run) stack frames:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py", line 766, in run_closure
    _threading_Thread_run(self)
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 521, in run
    self.wait_for_commands()
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 593, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)

DEBUG:ThreadMonitor:Logging Thread-19 (run) stack frames:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py", line 766, in run_closure
    _threading_Thread_run(self)
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 521, in run
    self.wait_for_commands()
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 593, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)

DEBUG:ThreadMonitor:Logging Thread-18 (run) stack frames:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py", line 766, in run_closure
    _threading_Thread_run(self)
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 521, in run
    self.wait_for_commands()
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 593, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)

DEBUG:ThreadMonitor:Logging Thread-17 (run) stack frames:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py", line 766, in run_closure
    _threading_Thread_run(self)
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 521, in run
    self.wait_for_commands()
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 593, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)

DEBUG:ThreadMonitor:Logging Thread-13 (run) stack frames:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py", line 766, in run_closure
    _threading_Thread_run(self)
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 521, in run
    self.wait_for_commands()
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 593, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)

DEBUG:ThreadMonitor:Logging Thread-12 (run) stack frames:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py", line 766, in run_closure
    _threading_Thread_run(self)
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 521, in run
    self.wait_for_commands()
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py", line 593, in wait_for_commands
    command = smart_decode(self.stream.readline())[:-1]
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)

DEBUG:ThreadMonitor:Logging Thread-4 (run) stack frames:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 2323, in run
    readable, writable, errored = select.select(

DEBUG:ThreadMonitor:Logging MainThread stack frames:
  File "/databricks/python_shell/scripts/db_ipykernel_launcher.py", line 52, in <module>
    main()
  File "/databricks/python_shell/scripts/db_ipykernel_launcher.py", line 48, in main
    DatabricksKernelApp.launch_instance(config=databricks_kernel_config())
  File "/databricks/python/lib/python3.12/site-packages/traitlets/config/application.py", line 1075, in launch_instance
    app.start()
  File "/databricks/python/lib/python3.12/site-packages/ipykernel/kernelapp.py", line 739, in start
    self.io_loop.start()
  File "/databricks/python/lib/python3.12/site-packages/tornado/platform/asyncio.py", line 205, in start
    self.asyncio_loop.run_forever()
  File "/usr/lib/python3.12/asyncio/base_events.py", line 641, in run_forever
    self._run_once()
  File "/usr/lib/python3.12/asyncio/base_events.py", line 1949, in _run_once
    event_list = self._selector.select(timeout)
  File "/usr/lib/python3.12/selectors.py", line 468, in select
    fd_event_list = self._selector.poll(timeout, max_ev)

,Some(stderr),Map(),Map(),List(),List(),Map())
--end--

Disregarding messages.

26/01/08 16:42:38 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:42:38 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:42:38 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1) SPARK_GEN_SUBQ_134 WHERE 1=0
26/01/08 16:42:38 INFO ProgressReporter$: [3 occurrences] Reporting progress for running commands: 1767888395088_7748756662946361605_9e875253ec604b9d9a81fc5da46178b9
26/01/08 16:42:38 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 16:42:38 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.014531 ms.
26/01/08 16:42:38 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:42:38 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.356593 ms.
26/01/08 16:42:38 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:42:38 INFO ClusterLoadMonitor: Added query with execution ID:71. Current active queries:1
26/01/08 16:42:38 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 3, computed in 0 ms.
26/01/08 16:42:38 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:42:38 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:42:38 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:42:38 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:42:39 INFO JDBCDatabaseMetadata: closed connection during metadata fetch
26/01/08 16:42:39 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:42:39 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:42:39 INFO DBCEventLoggingListener: Rolling event log; numTimesRolledOver = 4
26/01/08 16:42:39 INFO DBCEventLoggingListener: Rolled active log file /databricks/driver/eventlogs/5894523486654786537/eventlog to /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-40, size = 27800
26/01/08 16:42:39 INFO DBCEventLoggingListener: Logging events to eventlogs/5894523486654786537/eventlog
26/01/08 16:42:39 INFO DBCEventLoggingListener: Compressed rolled file /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-40 to /databricks/driver/eventlogs/5894523486654786537/eventlog-2026-01-08--16-40.gz in 1ms, size = 7429
26/01/08 16:42:39 INFO DBCEventLoggingListener: Deleted rolled file eventlogs/5894523486654786537/eventlog-2026-01-08--16-40
26/01/08 16:42:39 INFO CodeGenerator: Code generated in 15.085265 ms
26/01/08 16:42:39 INFO SparkContext: Starting job: wrapper at /root/.ipykernel/2426/command-7730399508088080-1602692301:112
26/01/08 16:42:39 INFO DAGScheduler: Got job 10 (wrapper at /root/.ipykernel/2426/command-7730399508088080-1602692301:112) with 1 output partitions
26/01/08 16:42:39 INFO DAGScheduler: Final stage: ResultStage 10 (wrapper at /root/.ipykernel/2426/command-7730399508088080-1602692301:112)
26/01/08 16:42:39 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:42:39 INFO DAGScheduler: Missing parents: List()
26/01/08 16:42:39 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[36] at wrapper at /root/.ipykernel/2426/command-7730399508088080-1602692301:112), which has no missing parents
26/01/08 16:42:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[36] at wrapper at /root/.ipykernel/2426/command-7730399508088080-1602692301:112) with jobGroupId 1767888395088_7748756662946361605_9e875253ec604b9d9a81fc5da46178b9 and executionId 71 (first 15 tasks are for partitions Vector(0))
26/01/08 16:42:39 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
26/01/08 16:42:39 INFO TaskSetManager: TaskSet 10.0 using PreferredLocationsV1
26/01/08 16:42:39 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767888395088, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767888395088. Created 1767888395088 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 16:42:39 INFO FairSchedulableBuilder: Added task set TaskSet_10.0 tasks to pool 1767888395088
26/01/08 16:42:39 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:42:39 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 162.8 KiB, free 4.2 GiB)
26/01/08 16:42:39 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 50.0 KiB, free 4.2 GiB)
26/01/08 16:42:39 INFO SparkContext: Created broadcast 10 from broadcast at TaskSetManager.scala:848
26/01/08 16:42:39 INFO Executor: Running task 0.0 in stage 10.0 (TID 12)
26/01/08 16:42:39 INFO JDBCRDD: Generated JDBC query to fetch data: SELECT "1" FROM (SELECT 1) SPARK_GEN_SUBQ_134     LIMIT 1 
26/01/08 16:42:39 INFO TransportClientFactory: Found inactive connection to /10.139.64.4:40407, creating a new one.
26/01/08 16:42:39 INFO TransportClientFactory: Successfully created connection to /10.139.64.4:40407 after 0 ms (0 ms spent in bootstraps)
26/01/08 16:42:39 INFO CodeGenerator: Code generated in 21.220511 ms
26/01/08 16:42:39 INFO JDBCRDD: closed connection
26/01/08 16:42:39 INFO Executor: Finished task 0.0 in stage 10.0 (TID 12). 1438 bytes result sent to driver
26/01/08 16:42:39 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 246 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:42:39 INFO TaskSchedulerImpl: Removed TaskSet 10.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:42:39 INFO DAGScheduler: ResultStage 10 (wrapper at /root/.ipykernel/2426/command-7730399508088080-1602692301:112) finished in 251 ms
26/01/08 16:42:39 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:42:39 INFO TaskSchedulerImpl: Canceling stage 10
26/01/08 16:42:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Result stage finished
26/01/08 16:42:39 INFO DAGScheduler: Job 10 finished: wrapper at /root/.ipykernel/2426/command-7730399508088080-1602692301:112, took 255.18177 ms
26/01/08 16:42:39 INFO ClusterLoadMonitor: Removed query with execution ID:71. Current active queries:0
26/01/08 16:42:39 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:42:39 INFO QueryProfileListener: Query profile sent to logger, seq number: 49, app id: local-1767888418069
26/01/08 16:42:43 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:42:58 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7748756662946361605_9e875253ec604b9d9a81fc5da46178b9
26/01/08 16:42:58 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7748756662946361605_9e875253ec604b9d9a81fc5da46178b9
26/01/08 16:43:02 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:43:02 INFO ProgressReporter$: Added result fetcher for 1767888395088_7029410049421338354_96a74354594d48a5a111f78fcf563d1d
26/01/08 16:43:02 INFO ProgressReporter$: Removed result fetcher for 1767888395088_7029410049421338354_96a74354594d48a5a111f78fcf563d1d
26/01/08 16:43:02 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_7029410049421338354_96a74354594d48a5a111f78fcf563d1d
26/01/08 16:43:05 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:43:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7fc24b8b size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:43:07 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:43:07 INFO ProgressReporter$: Added result fetcher for 1767888395088_6632885108707747079_fe04ee7e58884bd0b1902874e1f7d8c4
26/01/08 16:43:08 INFO ProgressReporter$: Removed result fetcher for 1767888395088_6632885108707747079_fe04ee7e58884bd0b1902874e1f7d8c4
26/01/08 16:43:08 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767888395088_6632885108707747079_fe04ee7e58884bd0b1902874e1f7d8c4
26/01/08 16:43:10 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:43:13 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:43:13 INFO ProgressReporter$: Added result fetcher for 1767888395088_5663541259152720060_be227b289c764bb2ad35c768fea29b37
26/01/08 16:43:13 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:43:13 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:43:13 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:43:13 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:43:13 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:43:13 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:43:13 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:43:13 INFO DispatcherImpl: Grpc session requested for grpc-session-4283fc4a-2d2a-4277-ade7-d63c6efdf0fb: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:43:13 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:43:13 INFO DispatcherImpl: Sandbox grpc-session-4283fc4a-2d2a-4277-ade7-d63c6efdf0fb - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:43:16 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:43:16 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM MaintenanceEvent WHERE 1=0
26/01/08 16:43:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:43:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@50e7c03b size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:43:42 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x0e52b30e]'
26/01/08 16:44:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@31d6986c size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:44:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:44:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088083/17325641-9b0a-40bd-b6ae-8b2b405b57e9] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:44:35 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088083/17325641-9b0a-40bd-b6ae-8b2b405b57e9] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a274960 method=PUT
26/01/08 16:44:35 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088083/17325641-9b0a-40bd-b6ae-8b2b405b57e9] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:44:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@10e6d0a9 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:45:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7d154f2d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:45:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:45:30 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xf4b85e75]'
26/01/08 16:45:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@131daeac size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:46:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@160d415b size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:46:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:46:35 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088083/db3231ba-c9c8-47b8-8947-a2738a0891c0] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:46:35 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088083/db3231ba-c9c8-47b8-8947-a2738a0891c0] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a274d8e method=PUT
26/01/08 16:46:35 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088083/db3231ba-c9c8-47b8-8947-a2738a0891c0] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:46:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@30c8b8ab size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:46:41 INFO ProgressReporter$: [551 occurrences] Reporting partial results for running commands: 1767888395088_5663541259152720060_be227b289c764bb2ad35c768fea29b37
26/01/08 16:46:43 INFO DAGScheduler: Asked to cancel job group 1767888395088_5663541259152720060_be227b289c764bb2ad35c768fea29b37 with cancelFutureJobs=false
26/01/08 16:46:43 WARN DAGScheduler: Failed to cancel job group 1767888395088_5663541259152720060_be227b289c764bb2ad35c768fea29b37. Cannot find active jobs for it.
26/01/08 16:46:43 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:46:43 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:46:43 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767888395088,None,Some(be227b289c764bb2ad35c768fea29b37)).
26/01/08 16:46:43 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5663541259152720060_be227b289c764bb2ad35c768fea29b37
26/01/08 16:46:48 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:46:48 INFO ProgressReporter$: Added result fetcher for 1767888395088_5032290156466522117_ad3159b256494bf09cd8854512ad3b8d
26/01/08 16:46:48 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:46:48 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:46:48 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:46:48 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:46:48 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:46:48 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:46:48 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 0 ms
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:46:48 INFO DispatcherImpl: Grpc session requested for grpc-session-1730a83c-4880-498c-b139-4d29d3221d77: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:46:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:46:48 INFO DispatcherImpl: Sandbox grpc-session-1730a83c-4880-498c-b139-4d29d3221d77 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:46:51 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM Flight WHERE 1=0
26/01/08 16:46:52 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:46:52 INFO ThreadDumpManager: ThreadDumpManager: thread dump requested due to Reason: {DAG_SCHEDULER_NO_ACTIVE_JOB.toString}, Count: 1
26/01/08 16:46:52 INFO ThreadDumpManager: ThreadDumpManager: thread dump took 12 ms.
26/01/08 16:46:52 INFO ThreadDumpManager: ThreadDumpManager: updated thread dump for 392 threads, removed 22 inactive threads.
26/01/08 16:46:52 WARN HangingThreadDetector: 
Hung Thread Detected:
Thread ID: 139
Time Spent Hanging: 1799993 ms
Stuck at Frame: java.base@17.0.16/java.lang.Thread.sleep(Native Method)
Latest Stack Trace:
java.base@17.0.16/java.lang.Thread.sleep(Native Method)
	at app//com.databricks.sql.history.prediction.SoftstoreSyncer$$anon$1.$anonfun$run$1(SoftstoreSyncer.scala:156)
	at app//com.databricks.sql.history.prediction.SoftstoreSyncer$$anon$1.$anonfun$run$1$adapted(SoftstoreSyncer.scala:150)
	at app//com.databricks.sql.history.prediction.SoftstoreSyncer$$anon$1$$Lambda$5486/0x00007f5d1d8d17f8.apply(Unknown Source)
	at app//com.databricks.logging.activity.ActivityContextFactory$.$anonfun$backgroundActivityInternal$3(ActivityContextFactory.scala:897)
	at app//com.databricks.logging.activity.ActivityContextFactory$$$Lambda$1295/0x00007f5d1c7a3020.apply(Unknown Source)
	at app//com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1096)
	at app//com.databricks.logging.activity.ActivityContextFactory$$$Lambda$1334/0x00007f5d1c7b3d48.apply(Unknown Source)
	at app//com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at app//com.databricks.logging.AttributionContextTracing$$Lambda$660/0x00007f5d1c4dd2b8.apply(Unknown Source)
	at app//com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at app//com.databricks.logging.AttributionContext$$$Lambda$661/0x00007f5d1c4dd580.apply(Unknown Source)
	at app//scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at app//com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at app//com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at app//com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at app//com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:63)
	at app//com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1096)
	at app//com.databricks.logging.activity.ActivityContextFactory$$$Lambda$1313/0x00007f5d1c7abc00.apply(Unknown Source)
	at app//com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at app//com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1058)
	at app//com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1039)
	at app//com.databricks.logging.activity.ActivityContextFactory$.$anonfun$backgroundActivityInternal$1(ActivityContextFactory.scala:895)
	at app//com.databricks.logging.activity.ActivityContextFactory$$$Lambda$1293/0x00007f5d1c7a2a98.apply(Unknown Source)
	at app//com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at app//com.databricks.logging.AttributionContextTracing$$Lambda$660/0x00007f5d1c4dd2b8.apply(Unknown Source)
	at app//com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at app//com.databricks.logging.AttributionContext$$$Lambda$661/0x00007f5d1c4dd580.apply(Unknown Source)
	at app//scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at app//com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at app//com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at app//com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at app//com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:63)
	at app//com.databricks.logging.activity.ActivityContextFactory$.backgroundActivityInternal(ActivityContextFactory.scala:895)
	at app//com.databricks.logging.activity.ActivityContextFactory$.withBackgroundActivity(ActivityContextFactory.scala:973)
	at app//com.databricks.sql.history.prediction.SoftstoreSyncer$$anon$1.run(SoftstoreSyncer.scala:150)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable$$Lambda$2960/0x00007f5d1ce6f718.apply$mcV$sp(Unknown Source)
	at app//scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at app//com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$$Lambda$2966/0x00007f5d1ce70c88.apply(Unknown Source)
	at app//com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$$Lambda$2965/0x00007f5d1ce709c0.apply(Unknown Source)
	at app//com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingHelper$$Lambda$2963/0x00007f5d1ce70000.apply(Unknown Source)
	at app//com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at app//org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at java.base@17.0.16/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base@17.0.16/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base@17.0.16/java.lang.Thread.run(Thread.java:840)
                
26/01/08 16:46:52 WARN HangingThreadDetector: 
Hung Thread Detected:
Thread ID: 183
Time Spent Hanging: 1799993 ms
Stuck at Frame: java.base@17.0.16/jdk.internal.misc.Unsafe.park(Native Method)
Latest Stack Trace:
java.base@17.0.16/jdk.internal.misc.Unsafe.park(Native Method)
	at java.base@17.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:341)
	at java.base@17.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:506)
	at java.base@17.0.16/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3476)
	at java.base@17.0.16/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3447)
	at java.base@17.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1630)
	at java.base@17.0.16/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
	at app//com.databricks.backend.daemon.driver.repl.ReplManagerImpl$PurgatoryThread.runInner(ReplManager.scala:119)
	at app//com.databricks.backend.daemon.driver.repl.ReplManagerImpl$PurgatoryThread.run(ReplManager.scala:110)
                
26/01/08 16:46:52 WARN HangingThreadDetector: 
Hung Thread Detected:
Thread ID: 196
Time Spent Hanging: 1799993 ms
Stuck at Frame: java.base@17.0.16/java.lang.Thread.sleep(Native Method)
Latest Stack Trace:
java.base@17.0.16/java.lang.Thread.sleep(Native Method)
	at app//com.databricks.rpc.armeria.server.internal.BaseHiccupCoordinator.doRun(BaseHiccupCoordinator.scala:137)
	at app//com.databricks.threading.NamedThread.$anonfun$run$2(NamedThread.scala:69)
	at app//com.databricks.threading.NamedThread$$Lambda$7878/0x00007f5d1e3d5300.apply$mcV$sp(Unknown Source)
	at app//scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at app//com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at app//com.databricks.logging.AttributionContextTracing$$Lambda$660/0x00007f5d1c4dd2b8.apply(Unknown Source)
	at app//com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at app//com.databricks.logging.AttributionContext$$$Lambda$661/0x00007f5d1c4dd580.apply(Unknown Source)
	at app//scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at app//com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at app//com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at app//com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at app//com.databricks.threading.NamedThread.withAttributionContext(NamedThread.scala:25)
	at app//com.databricks.threading.NamedThread.$anonfun$run$1(NamedThread.scala:68)
	at app//com.databricks.threading.NamedThread$$Lambda$7877/0x00007f5d1e3d5018.apply$mcV$sp(Unknown Source)
	at app//scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at app//com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at app//com.databricks.threading.NamedThread.run(NamedThread.scala:67)
                
26/01/08 16:46:52 WARN HangingThreadDetector: 
Hung Thread Detected:
Thread ID: 597
Time Spent Hanging: 1799993 ms
Stuck at Frame: java.base@17.0.16/jdk.internal.misc.Unsafe.park(Native Method)
Latest Stack Trace:
java.base@17.0.16/jdk.internal.misc.Unsafe.park(Native Method)
	at java.base@17.0.16/java.util.concurrent.locks.LockSupport.park(LockSupport.java:341)
	at java.base@17.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:506)
	at java.base@17.0.16/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3476)
	at java.base@17.0.16/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3447)
	at java.base@17.0.16/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1630)
	at java.base@17.0.16/java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:485)
	at java.base@17.0.16/java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:673)
	at app//com.databricks.backend.daemon.driver.ArmeriaOutgoingDirectNotebookMessageBuffer$ArmeriaMessageSendTask.run(ArmeriaOutgoingDirectNotebookMessageBuffer.scala:175)
	at app//com.databricks.backend.daemon.driver.ArmeriaOutgoingDirectNotebookMessageBuffer$ArmeriaMessageSendTask.$anonfun$start$1(ArmeriaOutgoingDirectNotebookMessageBuffer.scala:221)
	at app//com.databricks.backend.daemon.driver.ArmeriaOutgoingDirectNotebookMessageBuffer$ArmeriaMessageSendTask$$Lambda$10354/0x00007f5d1eba77c8.apply$mcV$sp(Unknown Source)
	at app//scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at app//scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at app//scala.concurrent.Future$$$Lambda$1736/0x00007f5d1c956078.apply(Unknown Source)
	at app//scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at java.base@17.0.16/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base@17.0.16/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base@17.0.16/java.lang.Thread.run(Thread.java:840)
                
26/01/08 16:46:54 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:46:54 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:46:54 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:46:54 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:46:54 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:46:54 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 256 milliseconds)
26/01/08 16:46:54 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:46:54 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:46:54 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:46:55 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:46:55 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:46:55 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 211 milliseconds)
26/01/08 16:47:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@67f3e5b8 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:47:13 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:47:13 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 16:47:13 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:47:13 INFO DispatcherImpl: Grpc session: grpc-session-33693146-5bf2-4c07-b79f-2931eaefd0b1 closing exc is None
26/01/08 16:47:13 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 16:47:13 INFO DispatcherImpl: Grpc session: grpc-session-4283fc4a-2d2a-4277-ade7-d63c6efdf0fb closing exc is None
26/01/08 16:47:13 INFO QueryPlanningTracker: Query phase analysis took 467s before execution.
26/01/08 16:47:14 INFO QueryPlanningTracker: Query phase analysis took 240s before execution.
26/01/08 16:47:16 INFO ApiClient: Connection with key Some(-98512064188715112) has been finalized and removed.
26/01/08 16:47:16 INFO ApiClient: Connection with key Some(-8374947915955379954) has been finalized and removed.
26/01/08 16:47:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:47:18 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 16:47:18 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 16:47:18 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 656
26/01/08 16:47:19 INFO DriverCorral: DBFS health check ok
26/01/08 16:47:19 INFO HiveMetaStore: 0: get_database: default
26/01/08 16:47:19 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 16:47:19 INFO DriverCorral: Metastore health check ok
26/01/08 16:47:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@19655f24 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:48:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@651ec2ca size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:48:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:48:20 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 2505516
 Duration: 19
26/01/08 16:48:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/eac87cf2-d44b-4134-831a-a8708dfad421] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:48:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/eac87cf2-d44b-4134-831a-a8708dfad421] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a2751c0 method=PUT
26/01/08 16:48:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/eac87cf2-d44b-4134-831a-a8708dfad421] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:48:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@10e62322 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:48:59 INFO DAGScheduler: Asked to cancel job group 1767888395088_5032290156466522117_ad3159b256494bf09cd8854512ad3b8d with cancelFutureJobs=false
26/01/08 16:48:59 WARN DAGScheduler: Failed to cancel job group 1767888395088_5032290156466522117_ad3159b256494bf09cd8854512ad3b8d. Cannot find active jobs for it.
26/01/08 16:48:59 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:49:00 INFO ProgressReporter$: Removed result fetcher for 1767888395088_5032290156466522117_ad3159b256494bf09cd8854512ad3b8d
26/01/08 16:49:00 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767888395088,None,Some(ad3159b256494bf09cd8854512ad3b8d)).
26/01/08 16:49:03 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 16:49:03 INFO ProgressReporter$: Added result fetcher for 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c
26/01/08 16:49:03 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 16:49:03 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.019277 ms.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:03 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 1 DFA states in the parser. Total cached DFA states: 301in the parser. Driver memory: 7888437248.
26/01/08 16:49:03 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.772991 ms.
26/01/08 16:49:03 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.003273 ms.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:03 INFO CurrentQueryContext: Thread Thread[Thread-210,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.CreateViewCommand).
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:03 INFO ClusterLoadMonitor: Added query with execution ID:72. Current active queries:1
26/01/08 16:49:03 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:03 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:49:03 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:49:03 INFO ClusterLoadMonitor: Removed query with execution ID:72. Current active queries:0
26/01/08 16:49:03 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:49:03 INFO CurrentQueryContext: Thread Thread[Thread-210,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 16:49:03 INFO QueryProfileListener: Query profile sent to logger, seq number: 50, app id: local-1767888418069
26/01/08 16:49:03 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 54 DFA states in the parser. Total cached DFA states: 355in the parser. Driver memory: 7888437248.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:49:03 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 18.105251 ms.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:03 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.130038 ms.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:03 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.113778 ms.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:03 INFO ClusterLoadMonitor: Added query with execution ID:73. Current active queries:1
26/01/08 16:49:03 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 31, computed in 0 ms.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:03 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:49:03 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:49:03 INFO ProgressReporter$: [6 occurrences] Reporting progress for running commands: 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c
26/01/08 16:49:03 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:49:03 INFO CodeGenerator: Code generated in 17.762467 ms
26/01/08 16:49:03 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:49:03 INFO DAGScheduler: Got job 11 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 16:49:03 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:49:03 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:49:03 INFO DAGScheduler: Missing parents: List()
26/01/08 16:49:03 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[42] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:49:03 INFO DAGScheduler: submitMissingTasks(ResultStage 11): 1 / 4 partitions missing, starting partial re-computation
26/01/08 16:49:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[42] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c and executionId 73 (first 15 tasks are for partitions Vector(0))
26/01/08 16:49:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
26/01/08 16:49:03 INFO TaskSetManager: TaskSet 11.0 using PreferredLocationsV1
26/01/08 16:49:03 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767888395088, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767888395088. Created 1767888395088 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 16:49:03 INFO FairSchedulableBuilder: Added task set TaskSet_11.0 tasks to pool 1767888395088
26/01/08 16:49:03 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 13) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:49:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 166.5 KiB, free 4.2 GiB)
26/01/08 16:49:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 51.3 KiB, free 4.2 GiB)
26/01/08 16:49:03 INFO SparkContext: Created broadcast 11 from broadcast at TaskSetManager.scala:848
26/01/08 16:49:03 INFO Executor: Running task 0.0 in stage 11.0 (TID 13)
26/01/08 16:49:03 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:49:03 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:49:03 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:49:03 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:49:03 INFO TransportClientFactory: Found inactive connection to /10.139.64.4:40407, creating a new one.
26/01/08 16:49:03 INFO TransportClientFactory: Successfully created connection to /10.139.64.4:40407 after 0 ms (0 ms spent in bootstraps)
26/01/08 16:49:03 INFO CodeGenerator: Code generated in 32.153339 ms
26/01/08 16:49:03 INFO Executor: Finished task 0.0 in stage 11.0 (TID 13). 2877 bytes result sent to driver
26/01/08 16:49:03 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 13) in 443 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:49:03 INFO TaskSchedulerImpl: Removed TaskSet 11.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:49:03 INFO DAGScheduler: ResultStage 11 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 448 ms
26/01/08 16:49:03 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:49:03 INFO TaskSchedulerImpl: Canceling stage 11
26/01/08 16:49:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Result stage finished
26/01/08 16:49:03 INFO DAGScheduler: Job 11 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 458.740391 ms
26/01/08 16:49:03 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.05371 ms.
26/01/08 16:49:03 INFO CodeGenerator: Code generated in 8.441998 ms
26/01/08 16:49:04 INFO ClusterLoadMonitor: Removed query with execution ID:73. Current active queries:0
26/01/08 16:49:04 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:49:04 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 6 DFA states in the parser. Total cached DFA states: 361in the parser. Driver memory: 7888437248.
26/01/08 16:49:04 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 16:49:04 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.304796 ms.
26/01/08 16:49:04 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:04 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.116542 ms.
26/01/08 16:49:04 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:04 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.124011 ms.
26/01/08 16:49:04 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:04 INFO ClusterLoadMonitor: Added query with execution ID:74. Current active queries:1
26/01/08 16:49:04 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 24, computed in 0 ms.
26/01/08 16:49:04 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 16:49:04 INFO QueryProfileListener: Query profile sent to logger, seq number: 51, app id: local-1767888418069
26/01/08 16:49:04 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 16:49:04 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 16:49:04 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 16:49:04 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:49:04 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 16:49:04 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
26/01/08 16:49:04 INFO CodeGenerator: Code generated in 21.287903 ms
26/01/08 16:49:04 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:49:04 INFO DAGScheduler: Got job 12 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 16:49:04 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:49:04 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:49:04 INFO DAGScheduler: Missing parents: List()
26/01/08 16:49:04 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[43] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:49:04 INFO DAGScheduler: submitMissingTasks(ResultStage 12): 1 / 4 partitions missing, starting partial re-computation
26/01/08 16:49:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[43] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c and executionId 74 (first 15 tasks are for partitions Vector(0))
26/01/08 16:49:04 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
26/01/08 16:49:04 INFO TaskSetManager: TaskSet 12.0 using PreferredLocationsV1
26/01/08 16:49:04 INFO FairSchedulableBuilder: Added task set TaskSet_12.0 tasks to pool 1767888395088
26/01/08 16:49:04 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 14) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 16:49:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 165.9 KiB, free 4.2 GiB)
26/01/08 16:49:04 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 4.2 GiB)
26/01/08 16:49:04 INFO SparkContext: Created broadcast 12 from broadcast at TaskSetManager.scala:848
26/01/08 16:49:04 INFO Executor: Running task 0.0 in stage 12.0 (TID 14)
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:49:04 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:49:04 INFO CodeGenerator: Code generated in 26.705146 ms
26/01/08 16:49:04 INFO PythonRunner: Times: total = 163, boot = 45, init = 118, finish = 0
26/01/08 16:49:04 INFO Executor: Finished task 0.0 in stage 12.0 (TID 14). 2598 bytes result sent to driver
26/01/08 16:49:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 14) in 202 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 16:49:04 INFO TaskSchedulerImpl: Removed TaskSet 12.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:49:04 INFO DAGScheduler: ResultStage 12 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 206 ms
26/01/08 16:49:04 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:49:04 INFO TaskSchedulerImpl: Canceling stage 12
26/01/08 16:49:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Result stage finished
26/01/08 16:49:04 INFO DAGScheduler: Job 12 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 209.919634 ms
26/01/08 16:49:04 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 16:49:04 INFO DAGScheduler: Got job 13 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 3 output partitions
26/01/08 16:49:04 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 16:49:04 INFO DAGScheduler: Parents of final stage: List()
26/01/08 16:49:04 INFO DAGScheduler: Missing parents: List()
26/01/08 16:49:04 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[43] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 16:49:04 INFO DAGScheduler: submitMissingTasks(ResultStage 13): 3 / 4 partitions missing, starting partial re-computation
26/01/08 16:49:04 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 13 (MapPartitionsRDD[43] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c and executionId 74 (first 15 tasks are for partitions Vector(1, 2, 3))
26/01/08 16:49:04 INFO TaskSchedulerImpl: Adding task set 13.0 with 3 tasks resource profile 0
26/01/08 16:49:04 INFO TaskSetManager: TaskSet 13.0 using PreferredLocationsV1
26/01/08 16:49:04 INFO FairSchedulableBuilder: Added task set TaskSet_13.0 tasks to pool 1767888395088
26/01/08 16:49:04 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 15) (10.139.64.4,executor driver, partition 1, PROCESS_LOCAL, 
26/01/08 16:49:04 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 16) (10.139.64.4,executor driver, partition 2, PROCESS_LOCAL, 
26/01/08 16:49:04 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 17) (10.139.64.4,executor driver, partition 3, PROCESS_LOCAL, 
26/01/08 16:49:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 165.9 KiB, free 4.2 GiB)
26/01/08 16:49:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 4.2 GiB)
26/01/08 16:49:04 INFO SparkContext: Created broadcast 13 from broadcast at TaskSetManager.scala:848
26/01/08 16:49:04 INFO Executor: Running task 0.0 in stage 13.0 (TID 15)
26/01/08 16:49:04 INFO Executor: Running task 1.0 in stage 13.0 (TID 16)
26/01/08 16:49:04 INFO Executor: Running task 2.0 in stage 13.0 (TID 17)
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:49:04 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:49:04 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:49:04 INFO PythonRunner: Times: total = 9, boot = -41, init = 50, finish = 0
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 16:49:04 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 16:49:04 INFO Executor: Finished task 0.0 in stage 13.0 (TID 15). 2598 bytes result sent to driver
26/01/08 16:49:04 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 15) in 134 ms on 10.139.64.4 (executor driver) (1/3)
26/01/08 16:49:04 INFO Executor: Finished task 2.0 in stage 13.0 (TID 17). 2750 bytes result sent to driver
26/01/08 16:49:04 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 17) in 203 ms on 10.139.64.4 (executor driver) (2/3)
26/01/08 16:49:04 INFO PythonRunner: Times: total = 212, boot = 93, init = 119, finish = 0
26/01/08 16:49:04 INFO Executor: Finished task 1.0 in stage 13.0 (TID 16). 2598 bytes result sent to driver
26/01/08 16:49:04 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 16) in 253 ms on 10.139.64.4 (executor driver) (3/3)
26/01/08 16:49:04 INFO TaskSchedulerImpl: Removed TaskSet 13.0 whose tasks have all completed, from pool 1767888395088
26/01/08 16:49:04 INFO DAGScheduler: ResultStage 13 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 266 ms
26/01/08 16:49:04 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 16:49:04 INFO TaskSchedulerImpl: Canceling stage 13
26/01/08 16:49:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Result stage finished
26/01/08 16:49:04 INFO DAGScheduler: Job 13 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 269.429348 ms
26/01/08 16:49:04 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.073079 ms.
26/01/08 16:49:04 INFO CodeGenerator: Code generated in 7.799424 ms
26/01/08 16:49:04 INFO ClusterLoadMonitor: Removed query with execution ID:74. Current active queries:0
26/01/08 16:49:04 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 16:49:04 INFO QueryProfileListener: Query profile sent to logger, seq number: 52, app id: local-1767888418069
26/01/08 16:49:04 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:49:04 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 16:49:04 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 16:49:04 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 16:49:04 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:49:04 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 16:49:04 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 16:49:04 INFO DispatcherImpl: Grpc session requested for grpc-session-9d2e9cda-653b-4439-b335-f49b9778346d: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@50787eec),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-887a6404fc174b1d823abbb9a5331487),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 16:49:04 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 16:49:04 INFO DispatcherImpl: Sandbox grpc-session-9d2e9cda-653b-4439-b335-f49b9778346d - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 16:49:06 WARN JupyterKernelListener: Received Jupyter debug message with unknown command: null
26/01/08 16:49:07 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
26/01/08 16:49:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6840caa3 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:49:07 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM System WHERE 1=0
26/01/08 16:49:10 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
26/01/08 16:49:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:49:29 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at jdk.internal.reflect.GeneratedMethodAccessor314.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 16:49:29 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 16:49:29 INFO DispatcherImpl: Grpc session: grpc-session-1730a83c-4880-498c-b139-4d29d3221d77 closing exc is None
26/01/08 16:49:30 INFO QueryPlanningTracker: Query phase analysis took 161s before execution.
26/01/08 16:49:36 INFO ApiClient: Connection with key Some(4755913544928166118) has been finalized and removed.
26/01/08 16:49:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@5bef34f0 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:50:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4874b5d5 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:50:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:50:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/f6976978-a7ba-497b-8abb-83b51e423961] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 16:50:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/f6976978-a7ba-497b-8abb-83b51e423961] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-296392db3a2755fd method=PUT
26/01/08 16:50:34 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/f6976978-a7ba-497b-8abb-83b51e423961] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 16:50:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@692a70ca size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:51:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7d1aa9e size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:51:18 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326698
26/01/08 16:51:37 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@253b5620 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 16:51:41 INFO ProgressReporter$: [580 occurrences] Reporting partial results for running commands: 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c
26/01/08 16:51:55 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:51:55 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:51:55 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:51:55 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:51:55 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:51:55 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 224 milliseconds)
26/01/08 16:51:55 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 16:51:55 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 16:51:55 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 16:51:55 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 16:51:55 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 16:51:55 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 244 milliseconds)
26/01/08 16:52:04 INFO DAGScheduler: Asked to cancel job group 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c with cancelFutureJobs=false
26/01/08 16:52:04 WARN DAGScheduler: Failed to cancel job group 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c. Cannot find active jobs for it.
26/01/08 16:52:04 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 16:52:04 INFO WSFSDriverManager$: Received response with error tip message from WSFS: {"errorTipMessage":"[Trace ID: 00-ce07885327144810a95f733090695005-96d02b508918543f-00]"}
26/01/08 16:52:04 INFO ProgressReporter$: Removed result fetcher for 1767888395088_8717174494027153236_c01d24fed1204afcb900cd060a74a72c
26/01/08 16:52:04 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767888395088,None,Some(c01d24fed1204afcb900cd060a74a72c)).
26/01/08 16:52:07 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@480c854f size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
