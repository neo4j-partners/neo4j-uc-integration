26/01/08 17:00:10 INFO ApiClient: Connection with key Some(-2041798106024369361) has been finalized and removed.
26/01/08 17:00:10 INFO ApiClient: Connection with key Some(7473513101796526069) has been finalized and removed.
26/01/08 17:00:10 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: UNKNOWN: Application error processing RPC
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 17:00:10 INFO DispatcherImpl: Grpc session: grpc-session-1b78c609-9b02-49bd-a8e1-d9e57b635d5d closing exc is None
26/01/08 17:00:10 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 17:00:10 INFO QueryPlanningTracker: Query phase analysis took 14s before execution.
26/01/08 17:00:10 INFO ClusterLoadMonitor: Added query with execution ID:38. Current active queries:1
26/01/08 17:00:10 INFO ClusterLoadMonitor: Removed query with execution ID:38. Current active queries:0
26/01/08 17:00:10 INFO DBCEventLoggingListener: Rolling event log; numTimesRolledOver = 1
26/01/08 17:00:10 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.lang.RuntimeException: Connection was closed before the operation completed.
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
	at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
	at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
		at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
		at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
		at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:796)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:762)
		at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:803)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 17:00:10 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 17:00:10 INFO DBCEventLoggingListener: Rolled active log file /databricks/driver/eventlogs/8138648883106984160/eventlog to /databricks/driver/eventlogs/8138648883106984160/eventlog-2026-01-08--17-00, size = 37142859
26/01/08 17:00:10 INFO DBCEventLoggingListener: Logging events to eventlogs/8138648883106984160/eventlog
26/01/08 17:00:11 INFO ProgressReporter$: Removed result fetcher for 1767891361529_4674959935455548321_060d79e880cc4dbc9a0ed20082961499
26/01/08 17:00:11 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767891361529_4674959935455548321_060d79e880cc4dbc9a0ed20082961499
26/01/08 17:00:11 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:00:11 INFO ProgressReporter$: Added result fetcher for 1767891361529_4898901885208476493_060d79e880cc4dbc9a0ed20082961499
26/01/08 17:00:11 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/eb721074-5e95-44a6-b328-31e0616b0756] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:00:11 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:00:11 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/eb721074-5e95-44a6-b328-31e0616b0756] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e60701 method=PUT
26/01/08 17:00:11 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/eb721074-5e95-44a6-b328-31e0616b0756] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:00:11 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:00:11 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 17:00:11 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 17:00:11 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:00:11 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:00:11 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 17:00:11 INFO DispatcherImpl: Grpc session requested for grpc-session-f27a0b8f-b370-4e20-82db-e52adec05f18: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@114138be),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 17:00:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 17:00:11 INFO DispatcherImpl: Sandbox grpc-session-f27a0b8f-b370-4e20-82db-e52adec05f18 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 17:00:11 INFO DBCEventLoggingListener: Compressed rolled file /databricks/driver/eventlogs/8138648883106984160/eventlog-2026-01-08--17-00 to /databricks/driver/eventlogs/8138648883106984160/eventlog-2026-01-08--17-00.gz in 888ms, size = 5537729
26/01/08 17:00:11 INFO DBCEventLoggingListener: Deleted rolled file eventlogs/8138648883106984160/eventlog-2026-01-08--17-00
26/01/08 17:00:14 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT COUNT(*) AS flight_count FROM Flight) SPARK_GEN_SUBQ_69 WHERE 1=0
26/01/08 17:00:16 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x9ecb6f50]'
26/01/08 17:00:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@57d65558 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:00:30 INFO ApiClient: Connection with key Some(2331746202971920657) has been finalized and removed.
26/01/08 17:00:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:00:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@545f67d3 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:01:01 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x6b68d8f9]'
26/01/08 17:01:15 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:01:15 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:01:15 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:01:15 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:01:15 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:01:15 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 265 milliseconds)
26/01/08 17:01:15 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:01:15 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:01:15 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:01:16 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:01:16 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:01:16 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 203 milliseconds)
26/01/08 17:01:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@41bcbff7 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:01:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:01:37 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 17:01:37 INFO InstanceMetadataServiceHelper$: Failed to read AWS instance metadata service document. Assuming not in AWS.
com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/dynamic/instance-identity/document
	at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:108)
	at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:70)
	at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:81)
	at com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:62)
	at com.databricks.s3a.logging.InstanceMetadataServiceHelper$.$anonfun$isAws$1(InstanceMetadataServiceHelper.scala:24)
	at scala.util.Try$.apply(Try.scala:217)
	at com.databricks.s3a.logging.InstanceMetadataServiceHelper$.isAws$lzycompute(InstanceMetadataServiceHelper.scala:22)
	at com.databricks.s3a.logging.InstanceMetadataServiceHelper$.isAws(InstanceMetadataServiceHelper.scala:18)
	at com.databricks.backend.common.util.HadoopFSUtil$.setDefaultS3Configuration(HadoopFSUtil.scala:165)
	at com.databricks.backend.common.util.HadoopFSUtil$.createConfiguration(HadoopFSUtil.scala:138)
	at com.databricks.backend.common.util.HadoopFSUtil$.createConfiguration(HadoopFSUtil.scala:55)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2Factory.getHadoopConfiguration(DatabricksFileSystemV2Factory.scala:184)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2Factory.createFileSystem(DatabricksFileSystemV2Factory.scala:54)
	at com.databricks.backend.daemon.data.filesystem.MountEntryResolver.$anonfun$getOrCreateFileSystem$1(MountEntryResolver.scala:109)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.common.util.locks.LoggedLock$.withAttributionContext(LoggedLock.scala:89)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.common.util.locks.LoggedLock$.withAttributionTags(LoggedLock.scala:89)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.common.util.locks.LoggedLock$.recordOperationWithResultTags(LoggedLock.scala:89)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.common.util.locks.LoggedLock$.recordOperation(LoggedLock.scala:89)
	at com.databricks.common.util.locks.LoggedLock$.withLock(LoggedLock.scala:162)
	at com.databricks.common.util.locks.PerKeyLock.withLock(PerKeyLock.scala:42)
	at com.databricks.backend.daemon.data.filesystem.MountEntryResolver.getOrCreateFileSystem(MountEntryResolver.scala:106)
	at com.databricks.backend.daemon.data.filesystem.MountEntryResolver.createFileSystem(MountEntryResolver.scala:133)
	at com.databricks.backend.daemon.data.client.DBFSV2.resolveAndGetFileSystem(DatabricksFileSystemV2.scala:158)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.resolve(DatabricksFileSystemV2.scala:798)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1225)
	at com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)
	at com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$4(LokiFileSystem.scala:317)
	at com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:297)
	at com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$1(LokiFileSystem.scala:317)
	at com.databricks.sql.io.LokiFileSystem$.withWrappedExceptions(LokiFileSystem.scala:156)
	at com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:317)
	at com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:323)
	at com.databricks.backend.daemon.driver.DriverCorral.$anonfun$new$2(DriverCorral.scala:345)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$3(NamedTimer.scala:139)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:127)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$2(NamedTimer.scala:136)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)
	at com.databricks.threading.NamedTimer$$anon$1.$anonfun$run$1(NamedTimer.scala:134)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:109)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:134)
	at java.base/java.util.TimerThread.mainLoop(Timer.java:566)
	at java.base/java.util.TimerThread.run(Timer.java:516)
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:01:37 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 17:01:37 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 674
26/01/08 17:01:37 INFO DriverCorral: DBFS health check ok
26/01/08 17:01:37 INFO HiveMetaStore: 1: get_database: default
26/01/08 17:01:37 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 17:01:37 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
26/01/08 17:01:37 INFO ObjectStore: ObjectStore, initialize called
26/01/08 17:01:37 INFO ObjectStore: Initialized ObjectStore
26/01/08 17:01:37 INFO DriverCorral: Metastore health check ok
26/01/08 17:01:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@673cc55f size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:02:00 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x57dece0f]'
26/01/08 17:02:12 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 370926
 Duration: 31
26/01/08 17:02:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2252075 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:02:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:02:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2d6bbe1d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:03:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@98cff40 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:03:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:03:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 153611837] onWebSocketClose with statusCode:1001 Endpoint unavailable, reason: java.util.concurrent.TimeoutException: Idle timeout expired: 300000/300000 ms
26/01/08 17:03:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 153611837] onWebSocketComplete
26/01/08 17:03:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 153611837] Stop ArmeriaMessageSendTask
26/01/08 17:03:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1203560648] onWebSocketSubscribe with headers: Map(db-outgoing-buffer-throttler-burst -> List(60000000), db-outgoing-buffer-throttler-steady-rate -> List(6000000), db-outgoing-buffer-throttler-warning-interval-sec -> List(60))
26/01/08 17:03:54 INFO OutgoingDirectNotebookBufferRateLimiter: OutgoingDirectNotebookBufferRateLimiter initialized with clusterBurst=60000000, clusterSteadyRate=6000000 
26/01/08 17:03:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 1203560648] Start ArmeriaMessageSendTask
26/01/08 17:03:55 INFO ProgressReporter$: [584 occurrences] Reporting partial results for running commands: 1767891361529_4898901885208476493_060d79e880cc4dbc9a0ed20082961499
26/01/08 17:03:56 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xa76e0847]'
26/01/08 17:03:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@81bc513 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:04:18 INFO DAGScheduler: Asked to cancel job group 1767891361529_4898901885208476493_060d79e880cc4dbc9a0ed20082961499 with cancelFutureJobs=false
26/01/08 17:04:18 WARN DAGScheduler: Failed to cancel job group 1767891361529_4898901885208476493_060d79e880cc4dbc9a0ed20082961499. Cannot find active jobs for it.
26/01/08 17:04:18 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 17:04:19 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767891361529,None,Some(060d79e880cc4dbc9a0ed20082961499)).
26/01/08 17:04:19 INFO ProgressReporter$: Removed result fetcher for 1767891361529_4898901885208476493_060d79e880cc4dbc9a0ed20082961499
26/01/08 17:04:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@23166aed size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:04:32 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:04:32 INFO ProgressReporter$: Added result fetcher for 1767891361529_6342340328683867167_b82ef69849144f20b3014f26b68743b3
26/01/08 17:04:32 INFO ProgressReporter$: [22 occurrences] Reporting progress for running commands: 1767891361529_6342340328683867167_b82ef69849144f20b3014f26b68743b3
26/01/08 17:04:32 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 17:04:32 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 17:04:32 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 17:04:32 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 17:04:32 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:04:32 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:04:32 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 0 ms
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 17:04:32 INFO DispatcherImpl: Grpc session requested for grpc-session-f6d01121-1895-485d-8495-9d41622fad6d: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@114138be),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 17:04:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 17:04:32 INFO DispatcherImpl: Sandbox grpc-session-f6d01121-1895-485d-8495-9d41622fad6d - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 17:04:36 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1 AS test) SPARK_GEN_SUBQ_70 WHERE 1=0
26/01/08 17:04:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:04:48 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 17:04:48 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 17:04:48 INFO DispatcherImpl: Grpc session: grpc-session-f27a0b8f-b370-4e20-82db-e52adec05f18 closing exc is None
26/01/08 17:04:49 INFO QueryPlanningTracker: Query phase analysis took 277s before execution.
26/01/08 17:04:50 INFO ApiClient: Connection with key Some(7992950099699629446) has been finalized and removed.
26/01/08 17:04:50 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: UNKNOWN: Application error processing RPC
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 17:04:50 INFO DispatcherImpl: Grpc session: grpc-session-f6d01121-1895-485d-8495-9d41622fad6d closing exc is None
26/01/08 17:04:50 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 17:04:50 INFO QueryPlanningTracker: Query phase analysis took 18s before execution.
26/01/08 17:04:50 INFO ClusterLoadMonitor: Added query with execution ID:39. Current active queries:1
26/01/08 17:04:50 INFO ClusterLoadMonitor: Removed query with execution ID:39. Current active queries:0
26/01/08 17:04:50 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 17:04:50 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.lang.RuntimeException: Connection was closed before the operation completed.
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
	at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
	at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
		at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
		at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
		at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
		at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
		at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:796)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:762)
		at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:803)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 17:04:51 INFO ProgressReporter$: Removed result fetcher for 1767891361529_6342340328683867167_b82ef69849144f20b3014f26b68743b3
26/01/08 17:04:51 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767891361529_6342340328683867167_b82ef69849144f20b3014f26b68743b3
26/01/08 17:04:51 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/514a065b-2642-4a99-81b1-c00852f52f9b] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:04:51 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/514a065b-2642-4a99-81b1-c00852f52f9b] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e610bf method=PUT
26/01/08 17:04:51 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088077/514a065b-2642-4a99-81b1-c00852f52f9b] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:04:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@53fda3c4 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:05:02 INFO SandboxWarmpool: Shutting down sandbox pool due to inactivity for more than 5 minutes.
26/01/08 17:05:02 INFO SandboxMemoryHolderImpl: MemoryConsumer released 377487360 bytes for WARMPOOL, currently used: 242691
26/01/08 17:05:10 INFO ApiClient: Connection with key Some(2212045901080738934) has been finalized and removed.
26/01/08 17:05:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3d652004 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:05:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:05:41 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:05:41 INFO ProgressReporter$: Added result fetcher for 1767891361529_7816731782284353625_a2fa3e467dc74ed785f8624fcf0b71c2
26/01/08 17:05:41 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:05:41 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:05:41 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 17:05:42 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 17:05:42 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:05:42 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:05:42 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 17:05:42 INFO DispatcherImpl: Grpc session requested for grpc-session-bc13f691-dec3-480c-bb4e-9a44952cbd37: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@114138be),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 17:05:42 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 17:05:42 INFO DispatcherImpl: Sandbox grpc-session-bc13f691-dec3-480c-bb4e-9a44952cbd37 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 17:05:45 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1 AS test) SPARK_GEN_SUBQ_71 WHERE 1=0
26/01/08 17:05:50 WARN JdbcConnectClient: Closing connection due to exception
java.sql.SQLException: UNKNOWN: Application error processing RPC
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$2(GrpcExceptionWrapper.scala:65)
	at scala.Option.getOrElse(Option.scala:201)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:65)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient$$anon$1.onError(JdbcConnectClient.scala:121)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 17:05:50 INFO DispatcherImpl: Grpc session: grpc-session-bc13f691-dec3-480c-bb4e-9a44952cbd37 closing exc is None
26/01/08 17:05:50 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 17:05:50 INFO QueryPlanningTracker: Query phase analysis took 8s before execution.
26/01/08 17:05:50 INFO ClusterLoadMonitor: Added query with execution ID:40. Current active queries:1
26/01/08 17:05:50 INFO ClusterLoadMonitor: Removed query with execution ID:40. Current active queries:0
26/01/08 17:05:50 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 17:05:50 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
java.lang.RuntimeException: Connection was closed before the operation completed.
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
	at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
	at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
	at scala.util.Using$.resource(Using.scala:296)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected(JdbcConnectClient.scala:96)
		at com.databricks.safespark.jdbc.grpc_client.JdbcGetRowsClient.fetchMetadata(JdbcGetRowsClient.scala:102)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata$lzycompute(GrpcResultSet.scala:27)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.metadata(GrpcResultSet.scala:26)
		at com.databricks.safespark.jdbc.driver.GrpcResultSet.getMetaData(GrpcResultSet.scala:100)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:299)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$3(JDBCRDD.scala:101)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$getQueryOutputSchema$2(JDBCRDD.scala:99)
		at scala.util.Using$.resource(Using.scala:296)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:97)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:73)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 17:05:51 INFO ProgressReporter$: Removed result fetcher for 1767891361529_7816731782284353625_a2fa3e467dc74ed785f8624fcf0b71c2
26/01/08 17:05:51 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767891361529_7816731782284353625_a2fa3e467dc74ed785f8624fcf0b71c2
26/01/08 17:05:51 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/1b29907c-ce45-446c-95b9-361b4f9559b6] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:05:51 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/1b29907c-ce45-446c-95b9-361b4f9559b6] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e6127d method=PUT
26/01/08 17:05:51 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088075/1b29907c-ce45-446c-95b9-361b4f9559b6] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:05:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6b8059e6 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:06:00 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:06:00 INFO ProgressReporter$: Added result fetcher for 1767891361529_8875153340510326977_f0201255cfc54432b5e2415eeb174d67
26/01/08 17:06:00 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:06:00 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:06:00 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 17:06:00 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 17:06:00 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:06:00 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:06:00 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 17:06:00 INFO DispatcherImpl: Grpc session requested for grpc-session-bfead15a-739d-4dc6-9d06-00a8bac161d2: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@114138be),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 17:06:00 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 17:06:00 INFO DispatcherImpl: Sandbox grpc-session-bfead15a-739d-4dc6-9d06-00a8bac161d2 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 17:06:04 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test) SPARK_GEN_SUBQ_72 WHERE 1=0
26/01/08 17:06:04 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 17:06:04 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 17:06:04 INFO DispatcherImpl: Grpc session: grpc-session-bfead15a-739d-4dc6-9d06-00a8bac161d2 closing exc is None
26/01/08 17:06:04 INFO QueryPlanningTracker: Query phase analysis took 3s before execution.
26/01/08 17:06:04 INFO ClusterLoadMonitor: Added query with execution ID:41. Current active queries:1
26/01/08 17:06:04 INFO ClusterLoadMonitor: Removed query with execution ID:41. Current active queries:0
26/01/08 17:06:04 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:
org.apache.spark.SparkException: [JDBC_EXTERNAL_ENGINE_SYNTAX_ERROR.DURING_OUTPUT_SCHEMA_RESOLUTION] JDBC external engine syntax error. The error was caused by the query SELECT * FROM (/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test) SPARK_GEN_SUBQ_72 WHERE 1=0. error: syntax error or access rule violation - invalid syntax. The error occurred during output schema resolution. SQLSTATE: 42000
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
	at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)
	at org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)
	at org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)
	at org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)
	at org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)
	at org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$1(JDBCRDD.scala:82)
		at com.databricks.spark.sql.execution.datasources.jdbc.JdbcUtilsEdge$.withSafeSQLQueryCheck(JdbcUtilsEdge.scala:314)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.$anonfun$resolveTable$2(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1385)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
		at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
		at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
		at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
		at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
		at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
		at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
		at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
		at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
		at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
		at py4j.Gateway.invoke(Gateway.java:306)
		at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
		at py4j.commands.CallCommand.execute(CallCommand.java:79)
		at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
		at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
		at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.sql.SQLException: error: syntax error or access rule violation - invalid syntax
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:62)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$$anonfun$$nestedInanonfun$unwrapToSQLException$1$1.applyOrElse(GrpcExceptionWrapper.scala:59)
	at scala.collection.IterableOnceOps.collectFirst(IterableOnce.scala:1256)
	at scala.collection.IterableOnceOps.collectFirst$(IterableOnce.scala:1248)
	at scala.collection.AbstractIterable.collectFirst(Iterable.scala:935)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.$anonfun$unwrapToSQLException$1(GrpcExceptionWrapper.scala:59)
	at scala.Option.flatMap(Option.scala:283)
	at com.databricks.safespark.jdbc.common.GrpcExceptionWrapper$.unwrapToSQLException(GrpcExceptionWrapper.scala:58)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.unwrapToSQLException(JdbcConnectClient.scala:72)
	at com.databricks.safespark.jdbc.grpc_client.JdbcPrepareStatementClient$$anon$1.onError(JdbcPrepareStatementClient.scala:33)
	at grpc_shaded.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:481)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:564)
	at grpc_shaded.io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:72)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:729)
	at grpc_shaded.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:710)
	at grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

26/01/08 17:06:04 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 17:06:04 INFO ProgressReporter$: Removed result fetcher for 1767891361529_8875153340510326977_f0201255cfc54432b5e2415eeb174d67
26/01/08 17:06:04 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767891361529_8875153340510326977_f0201255cfc54432b5e2415eeb174d67
26/01/08 17:06:04 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/e6f2620f-16aa-4d97-b1cd-5962175f8dbd] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:06:04 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/e6f2620f-16aa-4d97-b1cd-5962175f8dbd] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e612f4 method=PUT
26/01/08 17:06:04 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088076/e6f2620f-16aa-4d97-b1cd-5962175f8dbd] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:06:10 INFO ApiClient: Connection with key Some(5585111475974535934) has been finalized and removed.
26/01/08 17:06:10 INFO ApiClient: Connection with key Some(1485379392504518110) has been finalized and removed.
26/01/08 17:06:11 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:06:11 INFO ProgressReporter$: Added result fetcher for 1767891361529_7721265931122371493_b4d4e21822fe43a99a2a0fa89425014d
26/01/08 17:06:11 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 17:06:11 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 17:06:11 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 17:06:11 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 17:06:11 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:06:11 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:06:11 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 17:06:11 INFO DispatcherImpl: Grpc session requested for grpc-session-8a407c09-e8ed-4726-9b9b-ec7b82fed27e: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@114138be),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 17:06:11 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 17:06:11 INFO DispatcherImpl: Sandbox grpc-session-8a407c09-e8ed-4726-9b9b-ec7b82fed27e - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 17:06:14 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1 AS test) SPARK_GEN_SUBQ_73 WHERE 1=0
26/01/08 17:06:15 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x85110791]'
26/01/08 17:06:16 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:06:16 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:06:16 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:06:16 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:06:16 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:06:16 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 266 milliseconds)
26/01/08 17:06:16 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:06:16 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:06:16 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:06:16 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:06:16 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:06:16 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 242 milliseconds)
26/01/08 17:06:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@7b511779 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:06:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:06:37 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:06:37 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 17:06:37 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 674
26/01/08 17:06:37 INFO DriverCorral: DBFS health check ok
26/01/08 17:06:37 INFO HiveMetaStore: 1: get_database: default
26/01/08 17:06:37 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 17:06:37 INFO DriverCorral: Metastore health check ok
26/01/08 17:06:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@450c6854 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:07:15 INFO DAGScheduler: Asked to cancel job group 1767891361529_7721265931122371493_b4d4e21822fe43a99a2a0fa89425014d with cancelFutureJobs=false
26/01/08 17:07:15 WARN DAGScheduler: Failed to cancel job group 1767891361529_7721265931122371493_b4d4e21822fe43a99a2a0fa89425014d. Cannot find active jobs for it.
26/01/08 17:07:15 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 17:07:15 INFO ProgressReporter$: Removed result fetcher for 1767891361529_7721265931122371493_b4d4e21822fe43a99a2a0fa89425014d
26/01/08 17:07:15 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767891361529,None,Some(b4d4e21822fe43a99a2a0fa89425014d)).
26/01/08 17:07:25 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:07:25 INFO ProgressReporter$: Added result fetcher for 1767891361529_7111057086918530125_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:25 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:07:25 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:07:25 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM (SELECT 1) SPARK_GEN_SUBQ_74 WHERE 1=0
26/01/08 17:07:25 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 0 DFA states in the parser. Total cached DFA states: 300in the parser. Driver memory: 7888437248.
26/01/08 17:07:25 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.015343 ms.
26/01/08 17:07:25 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:07:25 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.095966 ms.
26/01/08 17:07:25 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:07:25 INFO ClusterLoadMonitor: Added query with execution ID:42. Current active queries:1
26/01/08 17:07:25 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 3, computed in 0 ms.
26/01/08 17:07:25 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:07:25 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 17:07:25 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 17:07:25 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 17:07:25 INFO JDBCDatabaseMetadata: closed connection during metadata fetch
26/01/08 17:07:25 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 17:07:25 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 17:07:25 INFO CodeGenerator: Code generated in 17.875564 ms
26/01/08 17:07:25 INFO SparkContext: Starting job: wrapper at /root/.ipykernel/1703/command-7730399508088080-1602692301:112
26/01/08 17:07:25 INFO DAGScheduler: Got job 6 (wrapper at /root/.ipykernel/1703/command-7730399508088080-1602692301:112) with 1 output partitions
26/01/08 17:07:25 INFO DAGScheduler: Final stage: ResultStage 6 (wrapper at /root/.ipykernel/1703/command-7730399508088080-1602692301:112)
26/01/08 17:07:25 INFO DAGScheduler: Parents of final stage: List()
26/01/08 17:07:25 INFO DAGScheduler: Missing parents: List()
26/01/08 17:07:25 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[18] at wrapper at /root/.ipykernel/1703/command-7730399508088080-1602692301:112), which has no missing parents
26/01/08 17:07:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[18] at wrapper at /root/.ipykernel/1703/command-7730399508088080-1602692301:112) with jobGroupId 1767891361529_7111057086918530125_b0db2b1d65774bd19ae55f4ac4c09084 and executionId 42 (first 15 tasks are for partitions Vector(0))
26/01/08 17:07:25 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
26/01/08 17:07:25 INFO TaskSetManager: TaskSet 6.0 using PreferredLocationsV1
26/01/08 17:07:25 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767891361529, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767891361529. Created 1767891361529 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 17:07:25 INFO FairSchedulableBuilder: Added task set TaskSet_6.0 tasks to pool 1767891361529
26/01/08 17:07:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 17:07:25 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 163.0 KiB, free 4.2 GiB)
26/01/08 17:07:25 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 49.8 KiB, free 4.2 GiB)
26/01/08 17:07:25 INFO SparkContext: Created broadcast 6 from broadcast at TaskSetManager.scala:848
26/01/08 17:07:25 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
26/01/08 17:07:25 INFO JDBCRDD: Generated JDBC query to fetch data: SELECT "1" FROM (SELECT 1) SPARK_GEN_SUBQ_74     LIMIT 1 
26/01/08 17:07:25 INFO TransportClientFactory: Found inactive connection to /10.139.64.4:46481, creating a new one.
26/01/08 17:07:25 INFO TransportClientFactory: Successfully created connection to /10.139.64.4:46481 after 0 ms (0 ms spent in bootstraps)
26/01/08 17:07:25 INFO CodeGenerator: Code generated in 24.205091 ms
26/01/08 17:07:25 INFO JDBCRDD: closed connection
26/01/08 17:07:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1438 bytes result sent to driver
26/01/08 17:07:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 263 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 17:07:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0 whose tasks have all completed, from pool 1767891361529
26/01/08 17:07:25 INFO DAGScheduler: ResultStage 6 (wrapper at /root/.ipykernel/1703/command-7730399508088080-1602692301:112) finished in 269 ms
26/01/08 17:07:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 17:07:25 INFO TaskSchedulerImpl: Canceling stage 6
26/01/08 17:07:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Result stage finished
26/01/08 17:07:25 INFO DAGScheduler: Job 6 finished: wrapper at /root/.ipykernel/1703/command-7730399508088080-1602692301:112, took 275.386924 ms
26/01/08 17:07:25 INFO ClusterLoadMonitor: Removed query with execution ID:42. Current active queries:0
26/01/08 17:07:25 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 17:07:25 INFO QueryProfileListener: Query profile sent to logger, seq number: 36, app id: local-1767891381204
26/01/08 17:07:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@51fd05f1 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:07:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:07:45 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryIsolatedJDBCExtension.loadTable(RemoteQueryIsolatedJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$3(RemoteQueryTVFAnalysis.scala:104)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.$anonfun$applyOrElse$2(RemoteQueryTVFAnalysis.scala:102)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryConnectorExtension.withClassifyQueryContainsSyntaxError(RemoteQueryConnectorExtension.scala:56)
	at com.databricks.sql.execution.datasources.remotequery.connectorextension.RemoteQueryJDBCExtension.withClassifyQueryContainsSyntaxError(RemoteQueryJDBCExtension.scala:76)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:101)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis$$anonfun$rewrite$1.applyOrElse(RemoteQueryTVFAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1373)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1372)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:57)
	at com.databricks.sql.execution.datasources.remotequery.RemoteQueryTVFAnalysis.rewrite(RemoteQueryTVFAnalysis.scala:43)
	at com.databricks.sql.optimizer.DatabricksEdgeRule.apply(DatabricksEdgeRule.scala:36)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:796)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:762)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:803)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 17:07:45 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 17:07:45 INFO DispatcherImpl: Grpc session: grpc-session-8a407c09-e8ed-4726-9b9b-ec7b82fed27e closing exc is None
26/01/08 17:07:45 INFO QueryPlanningTracker: Query phase analysis took 93s before execution.
26/01/08 17:07:48 INFO ProgressReporter$: Removed result fetcher for 1767891361529_7111057086918530125_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767891361529_7111057086918530125_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:07:48 INFO ProgressReporter$: Added result fetcher for 1767891361529_7807960748947336278_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 INFO ProgressReporter$: Removed result fetcher for 1767891361529_7807960748947336278_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767891361529_7807960748947336278_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:07:48 INFO ProgressReporter$: Added result fetcher for 1767891361529_6002842490225599486_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 INFO ProgressReporter$: Removed result fetcher for 1767891361529_6002842490225599486_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 INFO DatabricksStreamingQueryListener: Command execution complete on Driver, no active Streaming query runs associated with command 1767891361529_6002842490225599486_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:07:48 INFO ProgressReporter$: Added result fetcher for 1767891361529_8480039558839362345_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:07:48 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:07:48 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:07:48 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 17:07:48 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 17:07:48 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:07:48 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:07:48 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 17:07:48 INFO DispatcherImpl: Grpc session requested for grpc-session-6cf0391e-8ca4-452d-b980-2af0eca0e1d1: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@114138be),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 17:07:48 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 17:07:48 INFO DispatcherImpl: Sandbox grpc-session-6cf0391e-8ca4-452d-b980-2af0eca0e1d1 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 17:07:50 INFO ApiClient: Connection with key Some(7552644512770528784) has been finalized and removed.
26/01/08 17:07:51 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM MaintenanceEvent WHERE 1=0
26/01/08 17:07:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@38db3941 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:08:29 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4b421bfc size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:08:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:08:39 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x4883efea]'
26/01/08 17:08:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1203560648] onWebSocketClose with statusCode:1001 Endpoint unavailable, reason: java.util.concurrent.TimeoutException: Idle timeout expired: 300000/300000 ms
26/01/08 17:08:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1203560648] onWebSocketComplete
26/01/08 17:08:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 1203560648] Stop ArmeriaMessageSendTask
26/01/08 17:08:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1766745224] onWebSocketSubscribe with headers: Map(db-outgoing-buffer-throttler-burst -> List(60000000), db-outgoing-buffer-throttler-steady-rate -> List(6000000), db-outgoing-buffer-throttler-warning-interval-sec -> List(60))
26/01/08 17:08:54 INFO OutgoingDirectNotebookBufferRateLimiter: OutgoingDirectNotebookBufferRateLimiter initialized with clusterBurst=60000000, clusterSteadyRate=6000000 
26/01/08 17:08:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 1766745224] Start ArmeriaMessageSendTask
26/01/08 17:08:55 INFO ProgressReporter$: [412 occurrences] Reporting partial results for running commands: 1767891361529_8480039558839362345_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:08:59 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6d2a8408 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:09:14 INFO DAGScheduler: Asked to cancel job group 1767891361529_8480039558839362345_b0db2b1d65774bd19ae55f4ac4c09084 with cancelFutureJobs=false
26/01/08 17:09:14 WARN DAGScheduler: Failed to cancel job group 1767891361529_8480039558839362345_b0db2b1d65774bd19ae55f4ac4c09084. Cannot find active jobs for it.
26/01/08 17:09:14 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 17:09:14 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767891361529,None,Some(b0db2b1d65774bd19ae55f4ac4c09084)).
26/01/08 17:09:14 INFO ProgressReporter$: Removed result fetcher for 1767891361529_8480039558839362345_b0db2b1d65774bd19ae55f4ac4c09084
26/01/08 17:09:19 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:09:19 INFO ProgressReporter$: Added result fetcher for 1767891361529_8036950882120344225_7d71314ad22b491489736cab725a961c
26/01/08 17:09:19 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:09:19 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:09:19 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 17:09:19 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 17:09:19 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:09:19 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:09:19 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 0 ms
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 17:09:19 INFO DispatcherImpl: Grpc session requested for grpc-session-433c82bd-7fef-4f1a-bb2b-c875bf5c6c43: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@114138be),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 17:09:19 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 17:09:19 INFO DispatcherImpl: Sandbox grpc-session-433c82bd-7fef-4f1a-bb2b-c875bf5c6c43 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 17:09:23 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM Flight WHERE 1=0
26/01/08 17:09:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@54c1c6d1 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:09:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:09:44 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 17:09:44 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 17:09:44 INFO DispatcherImpl: Grpc session: grpc-session-6cf0391e-8ca4-452d-b980-2af0eca0e1d1 closing exc is None
26/01/08 17:09:44 INFO QueryPlanningTracker: Query phase analysis took 116s before execution.
26/01/08 17:09:50 INFO ApiClient: Connection with key Some(-5902276975406864081) has been finalized and removed.
26/01/08 17:10:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@35b1fc39 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:10:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6ef78c88 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:10:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:10:44 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x9d391b37]'
26/01/08 17:10:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/ec96d122-5ccc-45a4-aeba-7bb1c20db693] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:10:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/ec96d122-5ccc-45a4-aeba-7bb1c20db693] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e61ced method=PUT
26/01/08 17:10:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/ec96d122-5ccc-45a4-aeba-7bb1c20db693] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:11:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@57444f9a size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:11:16 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:11:16 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:11:16 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:11:16 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:11:16 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:11:16 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 214 milliseconds)
26/01/08 17:11:16 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:11:16 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:11:16 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:11:17 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:11:17 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:11:17 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 210 milliseconds)
26/01/08 17:11:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@d69eced size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:11:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:11:37 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:11:37 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 17:11:37 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 674
26/01/08 17:11:37 INFO DriverCorral: DBFS health check ok
26/01/08 17:11:37 INFO HiveMetaStore: 1: get_database: default
26/01/08 17:11:37 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 17:11:37 INFO DriverCorral: Metastore health check ok
26/01/08 17:12:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3703894d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:12:10 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 968932
 Duration: 40
26/01/08 17:12:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6542d0e9 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:12:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:12:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/36f7f6ad-c648-4042-bb2e-e14b94c07994] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:12:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/36f7f6ad-c648-4042-bb2e-e14b94c07994] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e6211e method=PUT
26/01/08 17:12:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/36f7f6ad-c648-4042-bb2e-e14b94c07994] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:13:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@108f6b44 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:13:03 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0xfa57a894]'
26/01/08 17:13:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@52c3e33c size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:13:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:13:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1766745224] onWebSocketClose with statusCode:1001 Endpoint unavailable, reason: java.util.concurrent.TimeoutException: Idle timeout expired: 300000/300000 ms
26/01/08 17:13:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1766745224] onWebSocketComplete
26/01/08 17:13:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 1766745224] Stop ArmeriaMessageSendTask
26/01/08 17:13:54 ERROR LoggingService: [sreqId=bd33025c, chanId=c149fa3b, caddr=10.31.67.198, raddr=10.139.0.4:37174, laddr=10.139.64.4:6062][h1://0104-134656-pvz3m00y-10-139-64-4/#GET] Request: {startTime=2026-01-08T17:08:54.682Z(1767892134682000), length=0B, duration=300s(300111088794ns), scheme=none+h1, name=GET, headers=[:method=GET, :path=/]}
26/01/08 17:13:54 ERROR LoggingService: [sreqId=bd33025c, chanId=c149fa3b, caddr=10.31.67.198, raddr=10.139.0.4:37174, laddr=10.139.64.4:6062][h1://0104-134656-pvz3m00y-10-139-64-4/#GET] Response: {startTime=2026-01-08T17:08:54.683Z(1767892134683000), length=24B, duration=300s(300110780138ns), totalDuration=300s(300112315464ns), cause=grpc_shaded.io.netty.handler.ssl.SslClosedEngineException: SSLEngine closed already, headers=[:status=101]}
grpc_shaded.io.netty.handler.ssl.SslClosedEngineException: SSLEngine closed already
	at grpc_shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:923)
	at grpc_shaded.io.netty.handler.ssl.SslHandler.wrapAndFlush(SslHandler.java:827)
	at grpc_shaded.io.netty.handler.ssl.SslHandler.flush(SslHandler.java:808)
	at grpc_shaded.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:941)
	at grpc_shaded.io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:921)
	at grpc_shaded.io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:907)
	at grpc_shaded.com.linecorp.armeria.internal.common.TrafficLoggingHandler.flush(TrafficLoggingHandler.java:49)
	at grpc_shaded.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:937)
	at grpc_shaded.io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:921)
	at grpc_shaded.io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:907)
	at grpc_shaded.io.netty.channel.DefaultChannelPipeline.flush(DefaultChannelPipeline.java:916)
	at grpc_shaded.io.netty.channel.AbstractChannel.flush(AbstractChannel.java:253)
	at grpc_shaded.com.linecorp.armeria.internal.common.Http1ObjectEncoder.doWriteUnsplitData(Http1ObjectEncoder.java:206)
	at grpc_shaded.com.linecorp.armeria.internal.common.Http1ObjectEncoder.doWriteData(Http1ObjectEncoder.java:183)
	at grpc_shaded.com.linecorp.armeria.internal.common.HttpObjectEncoder.writeData(HttpObjectEncoder.java:72)
	at grpc_shaded.com.linecorp.armeria.server.AbstractHttpResponseSubscriber.onNext(AbstractHttpResponseSubscriber.java:215)
	at grpc_shaded.com.linecorp.armeria.server.AbstractHttpResponseSubscriber.onNext(AbstractHttpResponseSubscriber.java:57)
	at grpc_shaded.com.linecorp.armeria.internal.common.stream.RecoverableStreamMessage$RecoverableSubscriber.onNext(RecoverableStreamMessage.java:227)
	at grpc_shaded.com.linecorp.armeria.common.stream.DeferredStreamMessage$ForwardingSubscriber.onNext(DeferredStreamMessage.java:514)
	at grpc_shaded.com.linecorp.armeria.common.stream.PublisherBasedStreamMessage$AbortableSubscriber.onNext0(PublisherBasedStreamMessage.java:344)
	at grpc_shaded.com.linecorp.armeria.common.stream.PublisherBasedStreamMessage$AbortableSubscriber.onNext(PublisherBasedStreamMessage.java:330)
	at grpc_shaded.com.linecorp.armeria.internal.common.stream.SurroundingPublisher$SurroundingSubscriber.publishDownstream(SurroundingPublisher.java:348)
	at grpc_shaded.com.linecorp.armeria.internal.common.stream.SurroundingPublisher$SurroundingSubscriber.onNext(SurroundingPublisher.java:384)
	at grpc_shaded.com.linecorp.armeria.common.stream.FuseableStreamMessage$FuseableSubscriber.onNext(FuseableStreamMessage.java:288)
	at grpc_shaded.com.linecorp.armeria.internal.common.stream.RecoverableStreamMessage$RecoverableSubscriber.onNext(RecoverableStreamMessage.java:227)
	at grpc_shaded.com.linecorp.armeria.common.stream.DefaultStreamMessage.notifySubscriberWithElements(DefaultStreamMessage.java:412)
	at grpc_shaded.com.linecorp.armeria.common.stream.DefaultStreamMessage.notifySubscriber0(DefaultStreamMessage.java:390)
	at grpc_shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
	at grpc_shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
	at grpc_shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at grpc_shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:405)
	at grpc_shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)
	at grpc_shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at grpc_shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 17:13:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1397835776] onWebSocketSubscribe with headers: Map(db-outgoing-buffer-throttler-burst -> List(60000000), db-outgoing-buffer-throttler-steady-rate -> List(6000000), db-outgoing-buffer-throttler-warning-interval-sec -> List(60))
26/01/08 17:13:54 INFO OutgoingDirectNotebookBufferRateLimiter: OutgoingDirectNotebookBufferRateLimiter initialized with clusterBurst=60000000, clusterSteadyRate=6000000 
26/01/08 17:13:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 1397835776] Start ArmeriaMessageSendTask
26/01/08 17:13:55 INFO ProgressReporter$: [587 occurrences] Reporting partial results for running commands: 1767891361529_8036950882120344225_7d71314ad22b491489736cab725a961c
26/01/08 17:14:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@79a141c8 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:14:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@a97138d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:14:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:14:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/4c1f6733-6d7f-4ba1-b219-a1320e352bbb] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:14:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/4c1f6733-6d7f-4ba1-b219-a1320e352bbb] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e6254d method=PUT
26/01/08 17:14:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/4c1f6733-6d7f-4ba1-b219-a1320e352bbb] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:15:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@61299cd0 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:15:12 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x1e401742]'
26/01/08 17:15:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@2cbe59ca size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:15:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:16:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@5f62e294 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:16:17 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:16:17 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:16:17 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:16:17 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:16:17 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:16:17 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 242 milliseconds)
26/01/08 17:16:17 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:16:17 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:16:17 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:16:17 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:16:17 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:16:17 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 239 milliseconds)
26/01/08 17:16:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6d5b898e size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:16:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:16:37 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:16:37 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 17:16:37 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 674
26/01/08 17:16:37 INFO DriverCorral: DBFS health check ok
26/01/08 17:16:37 INFO HiveMetaStore: 1: get_database: default
26/01/08 17:16:37 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 17:16:37 INFO DriverCorral: Metastore health check ok
26/01/08 17:16:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/f8f8100e-43d2-49e6-9ff2-b7b631a4593d] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:16:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/f8f8100e-43d2-49e6-9ff2-b7b631a4593d] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e6297e method=PUT
26/01/08 17:16:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/f8f8100e-43d2-49e6-9ff2-b7b631a4593d] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:17:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@403947e9 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:17:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@1e17362d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:17:34 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x4e313369]'
26/01/08 17:17:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:18:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@56a90139 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:18:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@678ff59b size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:18:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:18:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/ea90621c-b3ae-425b-9fe9-89348140a0b6] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:18:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/ea90621c-b3ae-425b-9fe9-89348140a0b6] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e62dac method=PUT
26/01/08 17:18:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/ea90621c-b3ae-425b-9fe9-89348140a0b6] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:18:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1397835776] onWebSocketClose with statusCode:1001 Endpoint unavailable, reason: java.util.concurrent.TimeoutException: Idle timeout expired: 300000/300000 ms
26/01/08 17:18:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1397835776] onWebSocketComplete
26/01/08 17:18:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 1397835776] Stop ArmeriaMessageSendTask
26/01/08 17:18:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 946931964] onWebSocketSubscribe with headers: Map(db-outgoing-buffer-throttler-burst -> List(60000000), db-outgoing-buffer-throttler-steady-rate -> List(6000000), db-outgoing-buffer-throttler-warning-interval-sec -> List(60))
26/01/08 17:18:54 INFO OutgoingDirectNotebookBufferRateLimiter: OutgoingDirectNotebookBufferRateLimiter initialized with clusterBurst=60000000, clusterSteadyRate=6000000 
26/01/08 17:18:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 946931964] Start ArmeriaMessageSendTask
26/01/08 17:18:55 INFO ProgressReporter$: [599 occurrences] Reporting partial results for running commands: 1767891361529_8036950882120344225_7d71314ad22b491489736cab725a961c
26/01/08 17:19:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6d03e08c size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:19:29 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x8772c7d8]'
26/01/08 17:19:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4a02155f size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:19:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:20:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@543c65ef size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:20:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@4c8761de size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:20:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:20:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/9e35c18d-a907-448b-b163-b882faaba15d] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:20:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/9e35c18d-a907-448b-b163-b882faaba15d] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e631d2 method=PUT
26/01/08 17:20:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088084/9e35c18d-a907-448b-b163-b882faaba15d] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:21:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@3ef2ff42 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:21:17 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:21:17 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:21:17 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:21:17 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:21:17 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:21:17 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 223 milliseconds)
26/01/08 17:21:17 INFO DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://consolidated-eastusc3-prod-metastore-0.mysql.database.azure.com:9207/organization1098933906466604?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt
26/01/08 17:21:17 INFO HikariDataSource: metastore-monitor - Starting...
26/01/08 17:21:17 INFO HikariDataSource: metastore-monitor - Start completed.
26/01/08 17:21:17 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
26/01/08 17:21:17 INFO HikariDataSource: metastore-monitor - Shutdown completed.
26/01/08 17:21:17 INFO MetastoreMonitor: PoPProxy healthcheck successful (connection duration = 209 milliseconds)
26/01/08 17:21:21 WARN SparkContext: Requesting executors is not supported by current scheduler.
26/01/08 17:21:21 INFO DeadlockDetector: Requested deadlock detection caused by: DAG_SCHEDULER_NO_ACTIVE_JOB
26/01/08 17:21:21 INFO HangingThreadDetector: Requested hanging thread detection caused by: DAG_SCHEDULER_NO_ACTIVE_JOB
26/01/08 17:21:22 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x486f7a4d]'
26/01/08 17:21:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6480ad2e size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:21:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:21:37 INFO DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasbs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme r2. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO HadoopFSUtil$: Installing the old filesystem implementation for scheme wasb. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.sql.io.LokiFileSystem.
26/01/08 17:21:37 INFO DatabricksFileSystemV2Factory: Creating abfss file system for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net
26/01/08 17:21:37 INFO AzureBlobFileSystem:V3: Initializing AzureBlobFileSystem for abfss://REDACTED_CREDENTIALS(63a9f0ea)@dbstorageolrfaaff6x6nu.dfs.core.windows.net/1098933906466604 with credential = FixedSASTokenProvider with jvmId = 674
26/01/08 17:21:37 INFO DriverCorral: DBFS health check ok
26/01/08 17:21:37 INFO HiveMetaStore: 1: get_database: default
26/01/08 17:21:37 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
26/01/08 17:21:37 INFO DriverCorral: Metastore health check ok
26/01/08 17:22:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@5324af4c size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:22:20 INFO MemoryUsageTracker: GC notification:
 Name: PS Scavenge,
 Action: end of minor GC,
 Cause: Allocation Failure
 StartTime: 1579235
 Duration: 34
26/01/08 17:22:26 INFO DAGScheduler: Asked to cancel job group 1767891361529_8036950882120344225_7d71314ad22b491489736cab725a961c with cancelFutureJobs=false
26/01/08 17:22:26 WARN DAGScheduler: Failed to cancel job group 1767891361529_8036950882120344225_7d71314ad22b491489736cab725a961c. Cannot find active jobs for it.
26/01/08 17:22:26 INFO JdbcConnectClient: Initiating graceful connection shutdown: notifying server
26/01/08 17:22:26 INFO PythonDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1767891361529,None,Some(7d71314ad22b491489736cab725a961c)).
26/01/08 17:22:26 INFO ProgressReporter$: Removed result fetcher for 1767891361529_8036950882120344225_7d71314ad22b491489736cab725a961c
26/01/08 17:22:30 INFO SparkAdapter: Successfully added uid 0 to driver local context
26/01/08 17:22:30 INFO ProgressReporter$: Added result fetcher for 1767891361529_8447117816278344522_fbc01f8fd20144d3860af3fd2838422c
26/01/08 17:22:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@245aad67 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:22:30 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.015779 ms.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 1 DFA states in the parser. Total cached DFA states: 301in the parser. Driver memory: 7888437248.
26/01/08 17:22:30 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.554606 ms.
26/01/08 17:22:30 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.00364 ms.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:30 INFO CurrentQueryContext: Thread Thread[Thread-190,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.CreateViewCommand).
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:30 INFO ClusterLoadMonitor: Added query with execution ID:43. Current active queries:1
26/01/08 17:22:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 0, computed in 0 ms.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=true, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 17:22:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 17:22:30 INFO DBCEventLoggingListener: Rolling event log; numTimesRolledOver = 2
26/01/08 17:22:30 INFO DBCEventLoggingListener: Rolled active log file /databricks/driver/eventlogs/8138648883106984160/eventlog to /databricks/driver/eventlogs/8138648883106984160/eventlog-2026-01-08--17-10, size = 361638
26/01/08 17:22:30 INFO DBCEventLoggingListener: Logging events to eventlogs/8138648883106984160/eventlog
26/01/08 17:22:30 INFO DBCEventLoggingListener: Compressed rolled file /databricks/driver/eventlogs/8138648883106984160/eventlog-2026-01-08--17-10 to /databricks/driver/eventlogs/8138648883106984160/eventlog-2026-01-08--17-10.gz in 11ms, size = 71751
26/01/08 17:22:30 INFO DBCEventLoggingListener: Deleted rolled file eventlogs/8138648883106984160/eventlog-2026-01-08--17-10
26/01/08 17:22:30 INFO ClusterLoadMonitor: Removed query with execution ID:43. Current active queries:0
26/01/08 17:22:30 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 17:22:30 INFO CurrentQueryContext: Thread Thread[Thread-190,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED).
26/01/08 17:22:30 INFO QueryProfileListener: Query profile sent to logger, seq number: 37, app id: local-1767891381204
26/01/08 17:22:30 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 54 DFA states in the parser. Total cached DFA states: 355in the parser. Driver memory: 7888437248.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 17:22:30 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 16.803367 ms.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:30 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.129683 ms.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:30 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.102145 ms.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:30 INFO ClusterLoadMonitor: Added query with execution ID:44. Current active queries:1
26/01/08 17:22:30 INFO ProgressReporter$: [8 occurrences] Reporting progress for running commands: 1767891361529_8447117816278344522_fbc01f8fd20144d3860af3fd2838422c
26/01/08 17:22:30 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 31, computed in 0 ms.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:30 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 17:22:30 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 17:22:30 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 17:22:30 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
26/01/08 17:22:30 INFO CodeGenerator: Code generated in 19.020192 ms
26/01/08 17:22:30 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 17:22:30 INFO DAGScheduler: Got job 7 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 17:22:30 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 17:22:30 INFO DAGScheduler: Parents of final stage: List()
26/01/08 17:22:30 INFO DAGScheduler: Missing parents: List()
26/01/08 17:22:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[24] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 17:22:30 INFO DAGScheduler: submitMissingTasks(ResultStage 7): 1 / 4 partitions missing, starting partial re-computation
26/01/08 17:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[24] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767891361529_8447117816278344522_fbc01f8fd20144d3860af3fd2838422c and executionId 44 (first 15 tasks are for partitions Vector(0))
26/01/08 17:22:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
26/01/08 17:22:30 INFO TaskSetManager: TaskSet 7.0 using PreferredLocationsV1
26/01/08 17:22:30 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 1767891361529, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 1767891361529. Created 1767891361529 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1
26/01/08 17:22:30 INFO FairSchedulableBuilder: Added task set TaskSet_7.0 tasks to pool 1767891361529
26/01/08 17:22:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 17:22:30 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 166.8 KiB, free 4.2 GiB)
26/01/08 17:22:30 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 51.3 KiB, free 4.2 GiB)
26/01/08 17:22:30 INFO SparkContext: Created broadcast 7 from broadcast at TaskSetManager.scala:848
26/01/08 17:22:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 17:22:30 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.unityCatalog.enforce.permissions from SQLConf: false.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableOnDedicatedCluster from default: true.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.requireUnityCatalog from default: true.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.unityCatalog.enabled from SQLConf: true.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.maxRetries from default: 10.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.retryInitialBackoffMs from default: 1000.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.retryMaxBackoffMs from default: 30000.
26/01/08 17:22:30 INFO SafeSparkConf$SafeSpark: Retrieved conf value for sspark.databricks.safespark.service.credential.server.timeoutMs from default: 60000.
26/01/08 17:22:30 INFO SafeSparkCredentialServerManager: Creating credential server for socket /databricks/sparkconnect/creds/shared-credential-servercd85b5e0-9d12-4c7d-a84b-154b5aa28d72.sock
26/01/08 17:22:30 INFO SafeSparkCredentialServerManager: Creating credential server (attempt 1) for socket /databricks/sparkconnect/creds/shared-credential-servercd85b5e0-9d12-4c7d-a84b-154b5aa28d72.sock
26/01/08 17:22:30 INFO SafeSparkCredentialRetrievalServer: Adjusted permissions on socket file at: /databricks/sparkconnect/creds/shared-credential-servercd85b5e0-9d12-4c7d-a84b-154b5aa28d72.sock
26/01/08 17:22:30 INFO SandboxCredentialInfoRegistry: Registered sandbox info for UID 0
26/01/08 17:22:30 INFO SafeSparkCredentialRetrievalServer: Registered sandbox for UID 0
26/01/08 17:22:30 INFO PythonWorkerFactory: Waiting for python envs to be ready: List(None, Some(/local_disk0/.ephemeral_nfs/cluster_libraries/python))
26/01/08 17:22:30 INFO DatabricksUtils: Environment directory found at /local_disk0/.ephemeral_nfs/cluster_libraries/python after 1 attempts
26/01/08 17:22:31 WARN WsfsHttpClient: Host http://databricks.node.host.local does not exist: java.net.UnknownHostException: databricks.node.host.local: Name or service not known
26/01/08 17:22:31 WARN WsfsHttpClient: Host http://node.host.local does not exist: java.net.UnknownHostException: node.host.local: Name or service not known
26/01/08 17:22:31 INFO TransportClientFactory: Found inactive connection to /10.139.64.4:46481, creating a new one.
26/01/08 17:22:31 INFO TransportClientFactory: Successfully created connection to /10.139.64.4:46481 after 0 ms (0 ms spent in bootstraps)
26/01/08 17:22:31 INFO CodeGenerator: Code generated in 31.797502 ms
26/01/08 17:22:31 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2877 bytes result sent to driver
26/01/08 17:22:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1138 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 17:22:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool 1767891361529
26/01/08 17:22:31 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42807
26/01/08 17:22:31 INFO DAGScheduler: ResultStage 7 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 1163 ms
26/01/08 17:22:31 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 17:22:31 INFO TaskSchedulerImpl: Canceling stage 7
26/01/08 17:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Result stage finished
26/01/08 17:22:31 INFO DAGScheduler: Job 7 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 1173.574428 ms
26/01/08 17:22:31 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.040616 ms.
26/01/08 17:22:31 INFO CodeGenerator: Code generated in 6.116528 ms
26/01/08 17:22:31 INFO ClusterLoadMonitor: Removed query with execution ID:44. Current active queries:0
26/01/08 17:22:31 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 17:22:31 INFO AbstractParser$ParserCaches: EXPERIMENTAL: Query cached 6 DFA states in the parser. Total cached DFA states: 361in the parser. Driver memory: 7888437248.
26/01/08 17:22:31 INFO QueryPlanningTracker: Query phase parsing took 0s before execution.
26/01/08 17:22:31 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.109248 ms.
26/01/08 17:22:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:31 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.127751 ms.
26/01/08 17:22:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:31 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.157625 ms.
26/01/08 17:22:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:31 INFO ClusterLoadMonitor: Added query with execution ID:45. Current active queries:1
26/01/08 17:22:31 INFO QueryAnalyzedPlanSizeLogger$: Total number of expressions in the analyzed plan: 24, computed in 0 ms.
26/01/08 17:22:31 INFO QueryPlanningTracker: Query phase analysis took 0s before execution.
26/01/08 17:22:31 INFO QueryProfileListener: Query profile sent to logger, seq number: 38, app id: local-1767891381204
26/01/08 17:22:31 INFO InjectShuffleJoinPlaceholder: HBO Gate Checks: joinCount=0 (threshold=50), totalBytes=0 (threshold=1073741824), allWhitelisted=false, withinJoinThreshold=true, aboveMinScanSize=false. Check passes: false.
26/01/08 17:22:31 INFO InjectShuffleJoinPlaceholder: Skipping InjectShuffleJoinPlaceholder rule for plan, as it did not pass all HBO gate checks.
26/01/08 17:22:31 INFO QueryPlanningTracker: Query phase optimization took 0s before execution.
26/01/08 17:22:31 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 17:22:31 INFO QueryPlanningTracker: Query phase planning took 0s before execution.
26/01/08 17:22:31 INFO CodeGenerator: Code generated in 22.728368 ms
26/01/08 17:22:31 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 17:22:32 INFO DAGScheduler: Got job 8 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 1 output partitions
26/01/08 17:22:32 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 17:22:32 INFO DAGScheduler: Parents of final stage: List()
26/01/08 17:22:32 INFO DAGScheduler: Missing parents: List()
26/01/08 17:22:32 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 17:22:32 INFO DAGScheduler: submitMissingTasks(ResultStage 8): 1 / 4 partitions missing, starting partial re-computation
26/01/08 17:22:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767891361529_8447117816278344522_fbc01f8fd20144d3860af3fd2838422c and executionId 45 (first 15 tasks are for partitions Vector(0))
26/01/08 17:22:32 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
26/01/08 17:22:32 INFO TaskSetManager: TaskSet 8.0 using PreferredLocationsV1
26/01/08 17:22:32 INFO FairSchedulableBuilder: Added task set TaskSet_8.0 tasks to pool 1767891361529
26/01/08 17:22:32 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (10.139.64.4,executor driver, partition 0, PROCESS_LOCAL, 
26/01/08 17:22:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 166.1 KiB, free 4.2 GiB)
26/01/08 17:22:32 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 4.2 GiB)
26/01/08 17:22:32 INFO SparkContext: Created broadcast 8 from broadcast at TaskSetManager.scala:848
26/01/08 17:22:32 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 17:22:32 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 17:22:32 INFO CodeGenerator: Code generated in 23.70253 ms
26/01/08 17:22:32 INFO PythonRunner: Times: total = 166, boot = 47, init = 119, finish = 0
26/01/08 17:22:32 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2598 bytes result sent to driver
26/01/08 17:22:32 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 205 ms on 10.139.64.4 (executor driver) (1/1)
26/01/08 17:22:32 INFO TaskSchedulerImpl: Removed TaskSet 8.0 whose tasks have all completed, from pool 1767891361529
26/01/08 17:22:32 INFO DAGScheduler: ResultStage 8 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 213 ms
26/01/08 17:22:32 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 17:22:32 INFO TaskSchedulerImpl: Canceling stage 8
26/01/08 17:22:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Result stage finished
26/01/08 17:22:32 INFO DAGScheduler: Job 8 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 217.257233 ms
26/01/08 17:22:32 INFO SparkContext: Starting job: $anonfun$withAction$4 at LexicalThreadLocal.scala:63
26/01/08 17:22:32 INFO DAGScheduler: Got job 9 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) with 3 output partitions
26/01/08 17:22:32 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63)
26/01/08 17:22:32 INFO DAGScheduler: Parents of final stage: List()
26/01/08 17:22:32 INFO DAGScheduler: Missing parents: List()
26/01/08 17:22:32 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[25] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63), which has no missing parents
26/01/08 17:22:32 INFO DAGScheduler: submitMissingTasks(ResultStage 9): 3 / 4 partitions missing, starting partial re-computation
26/01/08 17:22:32 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[25] at $anonfun$withAction$4 at LexicalThreadLocal.scala:63) with jobGroupId 1767891361529_8447117816278344522_fbc01f8fd20144d3860af3fd2838422c and executionId 45 (first 15 tasks are for partitions Vector(1, 2, 3))
26/01/08 17:22:32 INFO TaskSchedulerImpl: Adding task set 9.0 with 3 tasks resource profile 0
26/01/08 17:22:32 INFO TaskSetManager: TaskSet 9.0 using PreferredLocationsV1
26/01/08 17:22:32 INFO FairSchedulableBuilder: Added task set TaskSet_9.0 tasks to pool 1767891361529
26/01/08 17:22:32 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (10.139.64.4,executor driver, partition 1, PROCESS_LOCAL, 
26/01/08 17:22:32 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 10) (10.139.64.4,executor driver, partition 2, PROCESS_LOCAL, 
26/01/08 17:22:32 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 11) (10.139.64.4,executor driver, partition 3, PROCESS_LOCAL, 
26/01/08 17:22:32 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 166.1 KiB, free 4.2 GiB)
26/01/08 17:22:32 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 4.2 GiB)
26/01/08 17:22:32 INFO SparkContext: Created broadcast 9 from broadcast at TaskSetManager.scala:848
26/01/08 17:22:32 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
26/01/08 17:22:32 INFO Executor: Running task 2.0 in stage 9.0 (TID 11)
26/01/08 17:22:32 INFO Executor: Running task 1.0 in stage 9.0 (TID 10)
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 17:22:32 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 17:22:32 INFO PythonRunner: Times: total = 11, boot = -62, init = 73, finish = 0
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 17:22:32 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.useEnginePySpark.forceForTesting from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.pyspark.udf.isolation.faulthandler.enable from default: true.
26/01/08 17:22:32 INFO PythonWorkerUtils: Using cluster-wide virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python
26/01/08 17:22:32 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2598 bytes result sent to driver
26/01/08 17:22:32 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 137 ms on 10.139.64.4 (executor driver) (1/3)
26/01/08 17:22:32 INFO PythonRunner: Times: total = 149, boot = 49, init = 100, finish = 0
26/01/08 17:22:32 INFO Executor: Finished task 1.0 in stage 9.0 (TID 10). 2598 bytes result sent to driver
26/01/08 17:22:32 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 10) in 191 ms on 10.139.64.4 (executor driver) (2/3)
26/01/08 17:22:32 INFO Executor: Finished task 2.0 in stage 9.0 (TID 11). 2750 bytes result sent to driver
26/01/08 17:22:32 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 11) in 256 ms on 10.139.64.4 (executor driver) (3/3)
26/01/08 17:22:32 INFO TaskSchedulerImpl: Removed TaskSet 9.0 whose tasks have all completed, from pool 1767891361529
26/01/08 17:22:32 INFO DAGScheduler: ResultStage 9 ($anonfun$withAction$4 at LexicalThreadLocal.scala:63) finished in 285 ms
26/01/08 17:22:32 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
26/01/08 17:22:32 INFO TaskSchedulerImpl: Canceling stage 9
26/01/08 17:22:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Result stage finished
26/01/08 17:22:32 INFO DAGScheduler: Job 9 finished: $anonfun$withAction$4 at LexicalThreadLocal.scala:63, took 304.097739 ms
26/01/08 17:22:32 INFO HiveUnityCatalogCheckRule: Collecting SQL actions took 0.060291 ms.
26/01/08 17:22:32 INFO CodeGenerator: Code generated in 8.304589 ms
26/01/08 17:22:32 INFO ClusterLoadMonitor: Removed query with execution ID:45. Current active queries:0
26/01/08 17:22:32 INFO SQLExecution:  0 QueryExecution(s) are running
26/01/08 17:22:32 INFO QueryProfileListener: Query profile sent to logger, seq number: 39, app id: local-1767891381204
26/01/08 17:22:32 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:22:32 WARN DataSource: org.apache.spark.sql.sources.DataSourceRegister: com.google.cloud.spark.bigquery.BigQueryRelationProvider Unable to get public no-arg constructor
26/01/08 17:22:32 INFO DataSourceUtils: getOrInstallEnvironment: fetching connection MessageWithContext(neo4j_connection,,{})
26/01/08 17:22:32 INFO DataSourceUtils: getOrInstallEnvironment: found environment
26/01/08 17:22:32 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:22:32 INFO UDFEnvironmentInstallerFactory: Trying to get installer at com.databricks.backend.daemon.driver.UDFEnvUtils
26/01/08 17:22:32 INFO InstallUDFEnvironments: Time taken to install UDF environment for datasource: 1 ms
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.xmx.mib from default: 300.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.jvm.maxMetaspace.mib from default: 48.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.jdbcSandbox.size.default.mib from default: 400.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.idle from default: -1.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.count from default: -1.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.keepAlive.interval from default: -1.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.writeTimeout from default: -1.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.sql.externalUDF.tcp.userTimeout from default: -1.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.gvisor.version.testOnly from default: none.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachWriter.enabled from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.connect.scala.isolated.foreachBatch.enabled from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.archive.artifact.unpack.disabled from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dev.shm.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.service.credential.server.enableForNonUcUdfs from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.logging.enabled from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.std.logging.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.gvisorDebugFlags from default: .
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.skipUnreachableFuseMounts from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.monitoring.useOomCounter from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled from default: false.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib from default: 2147483647.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.metrics.enabled from default: true.
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.safespark.sandbox.dispatcher.sandboxapi.traffic.enabled from default: false.
26/01/08 17:22:32 INFO DispatcherImpl: Grpc session requested for grpc-session-24ae4660-28c4-47f3-bccd-1334f677d6e6: RawConnectionSpec(SparkContextDelegate(org.apache.spark.SparkContext@114138be),null,List(),Map(payload_format -> JDBC_SERVICE, DB_XMX_MIB -> 300, DB_MAX_METASPACE_MIB -> 48, DB_JDBC_DRIVER_CP -> /local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad/jars/neo4j-jdbc-full-bundle-6.10.4-SNAPSHOT.jar),List(/local_disk0/.ephemeral_nfs/udf_envs/scalaUDFEnv-baea115309d237045c11e395ab7bdfad),List(),9999,1000,None,None,Duration.Inf,HashMap(spark.connect.scala.isolated.foreachWriter.enabled -> false, spark.databricks.safespark.gvisor.version.testOnly -> none, spark.databricks.streaming.safespark.foreachBatch.python.isolation.enabled -> false, spark.connect.scala.isolated.foreachBatch.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.idle -> -1, spark.databricks.safespark.service.credential.server.enabled -> true, spark.databricks.safespark.sandbox.skipUnreachableFuseMounts -> true, spark.databricks.sql.externalUDF.tcp.keepAlive.interval -> -1, spark.databricks.safespark.archive.artifact.unpack.disabled -> false, spark.databricks.safespark.sandbox.metrics.enabled -> true, spark.databricks.streaming.safespark.foreachBatch.python.memory.limit.mib -> 2147483647, spark.databricks.sql.externalUDF.tcp.userTimeout -> -1, spark.databricks.safespark.sandbox.logging.enabled -> false, spark.databricks.sql.externalUDF.tcp.writeTimeout -> -1, spark.databricks.safespark.sandbox.dev.shm.enabled -> true, spark.databricks.safespark.sandbox.std.logging.enabled -> true, spark.databricks.safespark.sandbox.monitoring.useOomCounter -> true, spark.databricks.safespark.sandbox.gvisorDebugFlags -> , spark.databricks.sql.externalUDF.tcp.keepAlive.enabled -> true, spark.databricks.safespark.sandbox.wipeClientImageRootfsOnFailure.enabled -> false, spark.databricks.sql.externalUDF.tcp.keepAlive.count -> -1, spark.databricks.safespark.service.credential.server.enableForNonUcUdfs -> true),false,Some(400),None,Some(ISOLATED_DATA_SOURCE_DRIVER),true)
26/01/08 17:22:32 INFO SafeSparkConf$SafeSpark: Retrieved conf value for spark.databricks.internal.backgroundcompute.safespark.isolation.enableDiskBasedTmpDir from default: false.
26/01/08 17:22:32 INFO DispatcherImpl: Sandbox grpc-session-24ae4660-28c4-47f3-bccd-1334f677d6e6 - using apiType = ApiAdapter and executionType = Some(ISOLATED_DATA_SOURCE_DRIVER).
26/01/08 17:22:33 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
26/01/08 17:22:35 INFO JDBCRDD: Generated JDBC query to get scan output schema: SELECT * FROM System WHERE 1=0
26/01/08 17:22:36 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
26/01/08 17:22:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:22:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/7b66eb6b-c0f4-486a-b9a5-78d36653c6ec] Presigned URL: Started uploading stream using AzureSasUri
26/01/08 17:22:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/7b66eb6b-c0f4-486a-b9a5-78d36653c6ec] executeHttpRequest finished with status code 201; numRetries = 0 upload_type=AzureSasUri operation_id=PresignedUrlUpload-4969451767e6360a method=PUT
26/01/08 17:22:50 INFO PresignedUrlClientUtils$: FS_OP_CREATE FILE[https://dbstorageolrfaaff6x6nu.blob.core.windows.net/jobs/1098933906466604/command-results/7730399508088085/7b66eb6b-c0f4-486a-b9a5-78d36653c6ec] Presigned URL: Successfully uploaded stream using AzureSasUri
26/01/08 17:22:56 WARN JdbcConnectClient: Failed to wait for server cleanup
java.util.concurrent.TimeoutException: Future timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
	at org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:60)
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:538)
	at com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.close(JdbcConnectClient.scala:210)
	at com.databricks.safespark.jdbc.driver.JdbcConnection.close(JdbcConnection.scala:38)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1387)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:90)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.$anonfun$createRelation$1(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.metric.SQLMetrics$.withTimingNs(SQLMetrics.scala:490)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)
	at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:140)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:306)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:117)
	at java.base/java.lang.Thread.run(Thread.java:840)
26/01/08 17:22:56 INFO JdbcConnectClient: Closing connection gracefully
26/01/08 17:22:56 INFO DispatcherImpl: Grpc session: grpc-session-433c82bd-7fef-4f1a-bb2b-c875bf5c6c43 closing exc is None
26/01/08 17:22:56 INFO QueryPlanningTracker: Query phase analysis took 817s before execution.
26/01/08 17:23:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@d7b4e41 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:23:10 INFO ApiClient: Connection with key Some(8060697406175997593) has been finalized and removed.
26/01/08 17:23:26 WARN Bootstrap: Unknown channel option 'SO_KEEPALIVE' for channel '[id: 0x14791065]'
26/01/08 17:23:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6cf37b1d size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:23:37 INFO DatabricksILoop$: Received SAFEr configs with version 1767888326700
26/01/08 17:23:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 946931964] onWebSocketClose with statusCode:1001 Endpoint unavailable, reason: java.util.concurrent.TimeoutException: Idle timeout expired: 300000/300000 ms
26/01/08 17:23:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 946931964] onWebSocketComplete
26/01/08 17:23:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 946931964] Stop ArmeriaMessageSendTask
26/01/08 17:23:54 INFO ArmeriaCommChannelWebSocketHandler: [session: 1821141676] onWebSocketSubscribe with headers: Map(db-outgoing-buffer-throttler-burst -> List(60000000), db-outgoing-buffer-throttler-steady-rate -> List(6000000), db-outgoing-buffer-throttler-warning-interval-sec -> List(60))
26/01/08 17:23:54 INFO OutgoingDirectNotebookBufferRateLimiter: OutgoingDirectNotebookBufferRateLimiter initialized with clusterBurst=60000000, clusterSteadyRate=6000000 
26/01/08 17:23:54 INFO ArmeriaOutgoingDirectNotebookMessageBuffer: [session: 1821141676] Start ArmeriaMessageSendTask
26/01/08 17:23:55 INFO ProgressReporter$: [590 occurrences] Reporting partial results for running commands: 1767891361529_8447117816278344522_fbc01f8fd20144d3860af3fd2838422c
26/01/08 17:24:00 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@1a5be700 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
26/01/08 17:24:30 INFO Storelet[]:: [Storelet.scala:1347] [every=30s] Received performOperations for namespace unified-softstore-metadata and key <ByteString@6b9b50b9 size=46 contents="SoftstoreSyncer-v1-1098933906466604-publishers">
