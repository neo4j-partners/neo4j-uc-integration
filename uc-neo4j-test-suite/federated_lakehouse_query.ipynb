{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# Federated Lakehouse Query: Neo4j + Databricks\n",
        "\n",
        "Demonstrates querying both Databricks Delta lakehouse tables and Neo4j graph data\n",
        "in unified federated queries — combining time-series sensor analytics with graph-based\n",
        "maintenance events, flight operations, and component topology.\n",
        "\n",
        "This notebook implements the same dual-source pattern used in AgentBricks (Lab 6),\n",
        "but with direct SQL federation instead of AI agent routing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "architecture",
      "metadata": {},
      "source": [
        "## Architecture\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────────┐\n",
        "│                        Spark SQL Engine                            │\n",
        "│                     (Federated Query Layer)                        │\n",
        "├────────────────────────────┬────────────────────────────────────────┤\n",
        "│    Delta Lakehouse         │         Neo4j Knowledge Graph         │\n",
        "│    (Unity Catalog)         │         (UC JDBC / Spark Connector)   │\n",
        "│                           │                                        │\n",
        "│  ┌──────────────────┐     │     ┌───────────────────────────┐      │\n",
        "│  │ sensor_readings  │     │     │ MaintenanceEvent nodes    │      │\n",
        "│  │ (345,600 rows)   │     │     │ Flight nodes              │      │\n",
        "│  │ sensors (160)    │     │     │ Component topology        │      │\n",
        "│  │ systems (80)     │     │     │ Airport relationships     │      │\n",
        "│  │ aircraft (20)    │     │     │ Delay events              │      │\n",
        "│  └──────────────────┘     │     └───────────────────────────┘      │\n",
        "│                           │                                        │\n",
        "│  Best for:                │     Best for:                          │\n",
        "│  - Time-series analytics  │     - Relationship traversals          │\n",
        "│  - Statistical aggregates │     - Maintenance correlation          │\n",
        "│  - Sensor trend analysis  │     - Flight/route topology            │\n",
        "└────────────────────────────┴────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**Federation Methods Used:**\n",
        "1. `remote_query()` — UC JDBC table-valued function for Neo4j aggregate queries (no cluster library needed)\n",
        "2. Neo4j Spark Connector — Row-level Neo4j data loaded into temp views for rich JOINs with Delta tables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prerequisites",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "1. **Lakehouse tables** already exist in your catalog (created by lab setup):\n",
        "   `aircraft`, `systems`, `sensors`, `sensor_readings`\n",
        "\n",
        "2. **Neo4j UC JDBC connection** configured per [GUIDE_NEO4J_UC.md](../GUIDE_NEO4J_UC.md)\n",
        "\n",
        "3. **Cluster configuration:**\n",
        "   - SafeSpark memory settings applied (see guide)\n",
        "   - Neo4j Spark Connector installed as cluster library (`org.neo4j:neo4j-connector-apache-spark`)\n",
        "   - `neo4j-uc-creds` secret scope configured via `setup.sh`\n",
        "\n",
        "4. **Databricks preview features** enabled:\n",
        "   - Custom JDBC on UC Compute\n",
        "   - `remote_query` table-valued function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "SCOPE_NAME = \"neo4j-uc-creds\"\n",
        "\n",
        "# Lakehouse configuration — update to match your environment\n",
        "LAKEHOUSE_CATALOG = \"aws-databricks-neo4j-lab\"   # Your Unity Catalog name\n",
        "LAKEHOUSE_SCHEMA = \"lakehouse\"                    # Schema containing Delta tables\n",
        "\n",
        "# Neo4j credentials from Databricks Secrets\n",
        "NEO4J_HOST = dbutils.secrets.get(SCOPE_NAME, \"host\")\n",
        "NEO4J_USER = dbutils.secrets.get(SCOPE_NAME, \"user\")\n",
        "NEO4J_PASSWORD = dbutils.secrets.get(SCOPE_NAME, \"password\")\n",
        "try:\n",
        "    NEO4J_DATABASE = dbutils.secrets.get(SCOPE_NAME, \"database\")\n",
        "except Exception:\n",
        "    NEO4J_DATABASE = \"neo4j\"\n",
        "\n",
        "UC_CONNECTION_NAME = dbutils.secrets.get(SCOPE_NAME, \"connection_name\")\n",
        "NEO4J_BOLT_URI = f\"neo4j+s://{NEO4J_HOST}\"\n",
        "\n",
        "# Set catalog and schema context for Delta table queries\n",
        "spark.sql(f\"USE CATALOG `{LAKEHOUSE_CATALOG}`\")\n",
        "spark.sql(f\"USE SCHEMA `{LAKEHOUSE_SCHEMA}`\")\n",
        "\n",
        "print(f\"Lakehouse: {LAKEHOUSE_CATALOG}.{LAKEHOUSE_SCHEMA}\")\n",
        "print(f\"Neo4j Host: {NEO4J_HOST}\")\n",
        "print(f\"Neo4j Bolt URI: {NEO4J_BOLT_URI}\")\n",
        "print(f\"UC Connection: {UC_CONNECTION_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Verify Data Sources\n",
        "\n",
        "Confirm both the lakehouse tables and Neo4j UC connection are accessible before running federated queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify-lakehouse",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Delta lakehouse tables\n",
        "print(\"=\" * 60)\n",
        "print(\"DELTA LAKEHOUSE TABLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for table in [\"aircraft\", \"systems\", \"sensors\", \"sensor_readings\"]:\n",
        "    count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {table}\").collect()[0][\"cnt\"]\n",
        "    print(f\"  {table}: {count:,} rows\")\n",
        "\n",
        "print(\"\\nSample aircraft data:\")\n",
        "spark.sql(\"\"\"\n",
        "    SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
        "    FROM aircraft LIMIT 5\n",
        "\"\"\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify-neo4j",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Neo4j UC JDBC connection with aggregate queries\n",
        "print(\"=\" * 60)\n",
        "print(\"NEO4J KNOWLEDGE GRAPH (via UC JDBC)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "neo4j_counts = {\n",
        "    \"Aircraft\": \"SELECT COUNT(*) AS cnt FROM Aircraft\",\n",
        "    \"MaintenanceEvent\": \"SELECT COUNT(*) AS cnt FROM MaintenanceEvent\",\n",
        "    \"Flight\": \"SELECT COUNT(*) AS cnt FROM Flight\",\n",
        "}\n",
        "\n",
        "for label, query in neo4j_counts.items():\n",
        "    result = spark.sql(f\"\"\"\n",
        "        SELECT * FROM remote_query('{UC_CONNECTION_NAME}', query => '{query}')\n",
        "    \"\"\").collect()\n",
        "    print(f\"  {label}: {result[0]['cnt']:,} nodes\")\n",
        "\n",
        "# Test graph traversal\n",
        "traversal = spark.sql(f\"\"\"\n",
        "    SELECT * FROM remote_query('{UC_CONNECTION_NAME}',\n",
        "        query => 'SELECT COUNT(*) AS cnt FROM Flight f NATURAL JOIN DEPARTS_FROM r NATURAL JOIN Airport a')\n",
        "\"\"\").collect()\n",
        "print(f\"  Flight→Airport relationships: {traversal[0]['cnt']:,}\")\n",
        "\n",
        "print(\"\\nBoth data sources verified.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "federated-1-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Federated Query — Fleet Summary\n",
        "\n",
        "Combines Neo4j graph metrics with Delta sensor analytics using `remote_query()` for a\n",
        "fleet-wide overview. This approach uses **pure SQL** and requires no cluster libraries\n",
        "beyond the UC JDBC connection.\n",
        "\n",
        "**Pattern:** `remote_query()` returns a table that can be CROSS JOINed with Delta\n",
        "table aggregates in a single SQL statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "federated-fleet-summary",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Federated Fleet Summary: remote_query() for Neo4j + Delta sensor analytics\n",
        "# No Spark Connector needed — pure SQL federation via UC JDBC\n",
        "\n",
        "result = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        neo4j.total_maintenance_events,\n",
        "        neo4j.critical_events,\n",
        "        neo4j.total_flights,\n",
        "        neo4j.flight_airport_connections,\n",
        "        ROUND(sensor.avg_egt, 1) AS avg_egt_celsius,\n",
        "        ROUND(sensor.avg_vibration, 4) AS avg_vibration_ips,\n",
        "        ROUND(sensor.avg_fuel_flow, 2) AS avg_fuel_flow_kgs,\n",
        "        ROUND(sensor.avg_n1_speed, 0) AS avg_n1_speed_rpm,\n",
        "        sensor.total_readings\n",
        "    FROM (\n",
        "        SELECT\n",
        "            maint.cnt AS total_maintenance_events,\n",
        "            crit.cnt AS critical_events,\n",
        "            flights.cnt AS total_flights,\n",
        "            deps.cnt AS flight_airport_connections\n",
        "        FROM\n",
        "            remote_query('{UC_CONNECTION_NAME}',\n",
        "                query => 'SELECT COUNT(*) AS cnt FROM MaintenanceEvent') AS maint\n",
        "        CROSS JOIN\n",
        "            remote_query('{UC_CONNECTION_NAME}',\n",
        "                query => 'SELECT COUNT(*) AS cnt FROM MaintenanceEvent WHERE severity = ''CRITICAL''') AS crit\n",
        "        CROSS JOIN\n",
        "            remote_query('{UC_CONNECTION_NAME}',\n",
        "                query => 'SELECT COUNT(*) AS cnt FROM Flight') AS flights\n",
        "        CROSS JOIN\n",
        "            remote_query('{UC_CONNECTION_NAME}',\n",
        "                query => 'SELECT COUNT(*) AS cnt FROM Flight f NATURAL JOIN DEPARTS_FROM r NATURAL JOIN Airport a') AS deps\n",
        "    ) neo4j\n",
        "    CROSS JOIN (\n",
        "        SELECT\n",
        "            AVG(CASE WHEN sen.type = 'EGT' THEN r.value END) AS avg_egt,\n",
        "            AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END) AS avg_vibration,\n",
        "            AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END) AS avg_fuel_flow,\n",
        "            AVG(CASE WHEN sen.type = 'N1Speed' THEN r.value END) AS avg_n1_speed,\n",
        "            COUNT(*) AS total_readings\n",
        "        FROM sensor_readings r\n",
        "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
        "    ) sensor\n",
        "\"\"\")\n",
        "\n",
        "print(\"Fleet Summary — Neo4j Graph Metrics + Delta Sensor Analytics\")\n",
        "print(\"Neo4j: remote_query() via UC JDBC | Delta: sensor_readings + sensors\")\n",
        "print(\"=\" * 80)\n",
        "result.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "federated-2-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Federated Query — Sensor Health + Maintenance Correlation\n",
        "\n",
        "Uses the **Neo4j Spark Connector** to load maintenance events as a temp view, then\n",
        "JOINs with Delta lakehouse sensor data to correlate sensor health with maintenance\n",
        "activity per aircraft.\n",
        "\n",
        "**Key insight:** Aircraft with higher sensor readings (EGT, vibration) may correlate\n",
        "with more frequent maintenance events — this query reveals that relationship across\n",
        "both data sources.\n",
        "\n",
        "**Why Spark Connector?** UC JDBC aggregates don't support GROUP BY (Spark wraps\n",
        "queries in subqueries for schema inference). The Spark Connector gives us row-level\n",
        "graph data that we can freely aggregate and JOIN in Spark SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-maintenance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load maintenance events from Neo4j via Spark Connector\n",
        "neo4j_maintenance = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
        "    .option(\"url\", NEO4J_BOLT_URI) \\\n",
        "    .option(\"authentication.type\", \"basic\") \\\n",
        "    .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
        "    .option(\"authentication.basic.password\", NEO4J_PASSWORD) \\\n",
        "    .option(\"labels\", \"MaintenanceEvent\") \\\n",
        "    .load()\n",
        "\n",
        "neo4j_maintenance.createOrReplaceTempView(\"neo4j_maintenance\")\n",
        "\n",
        "print(f\"Loaded {neo4j_maintenance.count()} maintenance events from Neo4j\")\n",
        "print(\"\\nSample maintenance events:\")\n",
        "neo4j_maintenance.select(\n",
        "    \"aircraft_id\", \"fault\", \"severity\", \"corrective_action\"\n",
        ").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "federated-sensor-maintenance",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Federated Query: Sensor Health + Maintenance Correlation\n",
        "# Delta: sensor_readings, sensors, systems, aircraft\n",
        "# Neo4j: MaintenanceEvent nodes (via Spark Connector temp view)\n",
        "\n",
        "result = spark.sql(\"\"\"\n",
        "    WITH aircraft_ref AS (\n",
        "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
        "        FROM aircraft\n",
        "    ),\n",
        "    sensor_health AS (\n",
        "        SELECT\n",
        "            sys.aircraft_id,\n",
        "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
        "            ROUND(MAX(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS max_egt,\n",
        "            ROUND(AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS avg_vibration,\n",
        "            ROUND(MAX(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS max_vibration\n",
        "        FROM sensor_readings r\n",
        "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
        "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
        "        GROUP BY sys.aircraft_id\n",
        "    ),\n",
        "    maintenance_summary AS (\n",
        "        SELECT\n",
        "            aircraft_id,\n",
        "            COUNT(*) AS total_events,\n",
        "            SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical,\n",
        "            SUM(CASE WHEN severity = 'MAJOR' THEN 1 ELSE 0 END) AS major,\n",
        "            SUM(CASE WHEN severity = 'MINOR' THEN 1 ELSE 0 END) AS minor\n",
        "        FROM neo4j_maintenance\n",
        "        GROUP BY aircraft_id\n",
        "    )\n",
        "    SELECT\n",
        "        a.tail_number,\n",
        "        a.model,\n",
        "        a.operator,\n",
        "        COALESCE(m.total_events, 0) AS maint_events,\n",
        "        COALESCE(m.critical, 0) AS critical,\n",
        "        COALESCE(m.major, 0) AS major,\n",
        "        COALESCE(m.minor, 0) AS minor,\n",
        "        s.avg_egt AS avg_egt_c,\n",
        "        s.max_egt AS max_egt_c,\n",
        "        s.avg_vibration AS avg_vib_ips,\n",
        "        s.max_vibration AS max_vib_ips\n",
        "    FROM aircraft_ref a\n",
        "    LEFT JOIN maintenance_summary m ON a.aircraft_id = m.aircraft_id\n",
        "    LEFT JOIN sensor_health s ON a.aircraft_id = s.aircraft_id\n",
        "    ORDER BY m.total_events DESC NULLS LAST\n",
        "\"\"\")\n",
        "\n",
        "print(\"Sensor Health + Maintenance Correlation\")\n",
        "print(\"Delta: sensor_readings, sensors, systems, aircraft\")\n",
        "print(\"Neo4j: MaintenanceEvent nodes (Spark Connector)\")\n",
        "print(\"=\" * 100)\n",
        "result.show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "federated-3-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Federated Query — Flight Operations + Engine Performance\n",
        "\n",
        "Loads flight data from Neo4j and correlates with engine sensor performance from the\n",
        "Delta lakehouse. Shows how aircraft utilization (flight frequency, route coverage)\n",
        "relates to engine health metrics (EGT, fuel flow, N1 speed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-flights",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load flight data from Neo4j via Spark Connector\n",
        "neo4j_flights = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
        "    .option(\"url\", NEO4J_BOLT_URI) \\\n",
        "    .option(\"authentication.type\", \"basic\") \\\n",
        "    .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
        "    .option(\"authentication.basic.password\", NEO4J_PASSWORD) \\\n",
        "    .option(\"labels\", \"Flight\") \\\n",
        "    .load()\n",
        "\n",
        "neo4j_flights.createOrReplaceTempView(\"neo4j_flights\")\n",
        "\n",
        "print(f\"Loaded {neo4j_flights.count()} flights from Neo4j\")\n",
        "print(\"\\nSample flight data:\")\n",
        "neo4j_flights.select(\n",
        "    \"aircraft_id\", \"flight_number\", \"operator\", \"origin\", \"destination\"\n",
        ").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "federated-flights-engine",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Federated Query: Flight Operations + Engine Performance\n",
        "# Delta: sensor_readings (engine sensors only), sensors, systems, aircraft\n",
        "# Neo4j: Flight nodes (via Spark Connector temp view)\n",
        "\n",
        "result = spark.sql(\"\"\"\n",
        "    WITH aircraft_ref AS (\n",
        "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, operator\n",
        "        FROM aircraft\n",
        "    ),\n",
        "    flight_activity AS (\n",
        "        SELECT\n",
        "            aircraft_id,\n",
        "            COUNT(*) AS total_flights,\n",
        "            COUNT(DISTINCT origin) AS unique_origins,\n",
        "            COUNT(DISTINCT destination) AS unique_destinations\n",
        "        FROM neo4j_flights\n",
        "        GROUP BY aircraft_id\n",
        "    ),\n",
        "    engine_health AS (\n",
        "        SELECT\n",
        "            sys.aircraft_id,\n",
        "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
        "            ROUND(AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END), 2) AS avg_fuel_flow,\n",
        "            ROUND(AVG(CASE WHEN sen.type = 'N1Speed' THEN r.value END), 0) AS avg_n1_speed\n",
        "        FROM sensor_readings r\n",
        "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
        "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
        "        WHERE sys.type = 'Engine'\n",
        "        GROUP BY sys.aircraft_id\n",
        "    )\n",
        "    SELECT\n",
        "        a.tail_number,\n",
        "        a.model,\n",
        "        a.operator,\n",
        "        f.total_flights,\n",
        "        f.unique_origins AS origins,\n",
        "        f.unique_destinations AS destinations,\n",
        "        e.avg_egt AS avg_egt_c,\n",
        "        e.avg_fuel_flow AS fuel_kgs,\n",
        "        e.avg_n1_speed AS n1_rpm\n",
        "    FROM aircraft_ref a\n",
        "    JOIN flight_activity f ON a.aircraft_id = f.aircraft_id\n",
        "    JOIN engine_health e ON a.aircraft_id = e.aircraft_id\n",
        "    ORDER BY f.total_flights DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"Flight Operations + Engine Performance\")\n",
        "print(\"Delta: sensor_readings (Engine sensors), sensors, systems, aircraft\")\n",
        "print(\"Neo4j: Flight nodes (Spark Connector)\")\n",
        "print(\"=\" * 90)\n",
        "result.show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "federated-4-header",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: Federated Query — Fleet Health Dashboard\n",
        "\n",
        "The most comprehensive federated query, combining **all data sources** into a single\n",
        "fleet health view:\n",
        "\n",
        "- **Delta lakehouse**: Sensor readings aggregated per aircraft (EGT, vibration, fuel flow)\n",
        "- **Neo4j (Spark Connector)**: Maintenance events and flight counts per aircraft\n",
        "- **Neo4j (remote_query)**: Graph relationship traversal counting flight→airport connections\n",
        "\n",
        "This demonstrates using **both federation methods** in a single analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "federated-dashboard",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Fleet Health Dashboard\n",
        "# Combines all data sources: Delta tables + Neo4j Spark Connector + remote_query()\n",
        "\n",
        "# Graph traversal metric via remote_query (UC JDBC)\n",
        "departure_count = spark.sql(f\"\"\"\n",
        "    SELECT * FROM remote_query('{UC_CONNECTION_NAME}',\n",
        "        query => 'SELECT COUNT(*) AS cnt FROM Flight f NATURAL JOIN DEPARTS_FROM r NATURAL JOIN Airport a')\n",
        "\"\"\").collect()[0][\"cnt\"]\n",
        "print(f\"Graph traversal (Flight)-[:DEPARTS_FROM]->(Airport): {departure_count:,} connections\")\n",
        "\n",
        "# Full fleet health dashboard\n",
        "result = spark.sql(\"\"\"\n",
        "    WITH aircraft_ref AS (\n",
        "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
        "        FROM aircraft\n",
        "    ),\n",
        "    sensor_stats AS (\n",
        "        SELECT\n",
        "            sys.aircraft_id,\n",
        "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
        "            ROUND(AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS avg_vib,\n",
        "            ROUND(AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END), 2) AS avg_fuel,\n",
        "            COUNT(*) AS reading_count\n",
        "        FROM sensor_readings r\n",
        "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
        "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
        "        GROUP BY sys.aircraft_id\n",
        "    ),\n",
        "    maint AS (\n",
        "        SELECT aircraft_id, COUNT(*) AS events,\n",
        "               SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical\n",
        "        FROM neo4j_maintenance\n",
        "        GROUP BY aircraft_id\n",
        "    ),\n",
        "    flights AS (\n",
        "        SELECT aircraft_id, COUNT(*) AS flight_count\n",
        "        FROM neo4j_flights\n",
        "        GROUP BY aircraft_id\n",
        "    )\n",
        "    SELECT\n",
        "        a.tail_number,\n",
        "        a.model,\n",
        "        a.operator,\n",
        "        COALESCE(f.flight_count, 0) AS flights,\n",
        "        COALESCE(m.events, 0) AS maint_events,\n",
        "        COALESCE(m.critical, 0) AS critical,\n",
        "        s.avg_egt AS egt_c,\n",
        "        s.avg_vib AS vib_ips,\n",
        "        s.avg_fuel AS fuel_kgs,\n",
        "        s.reading_count AS readings\n",
        "    FROM aircraft_ref a\n",
        "    LEFT JOIN flights f ON a.aircraft_id = f.aircraft_id\n",
        "    LEFT JOIN maint m ON a.aircraft_id = m.aircraft_id\n",
        "    LEFT JOIN sensor_stats s ON a.aircraft_id = s.aircraft_id\n",
        "    ORDER BY COALESCE(m.critical, 0) DESC, COALESCE(m.events, 0) DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nFleet Health Dashboard\")\n",
        "print(\"Delta: sensor_readings, sensors, systems, aircraft\")\n",
        "print(\"Neo4j: MaintenanceEvent + Flight (Spark Connector), graph traversal (remote_query)\")\n",
        "print(\"=\" * 100)\n",
        "result.show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated three federated query patterns combining Neo4j graph data\n",
        "with Databricks Delta lakehouse tables:\n",
        "\n",
        "| Section | Pattern | Neo4j Method | What It Shows |\n",
        "|---------|---------|-------------|---------------|\n",
        "| Fleet Summary | Aggregate federation | `remote_query()` (UC JDBC) | Fleet-wide metrics from both sources in pure SQL |\n",
        "| Sensor + Maintenance | Row-level federation | Spark Connector → temp view | Per-aircraft correlation across data sources |\n",
        "| Flight Ops + Engine | Row-level federation | Spark Connector → temp view | Utilization vs engine health |\n",
        "| Fleet Health Dashboard | Hybrid | Both methods combined | Comprehensive multi-source analytics |\n",
        "\n",
        "### Integration Methods Comparison\n",
        "\n",
        "| Method | Pros | Cons |\n",
        "|--------|------|------|\n",
        "| `remote_query()` | Pure SQL, no cluster library, UC governed | Aggregate-only (no GROUP BY, ORDER BY) |\n",
        "| Spark Connector | Full Cypher support, row-level data | Requires cluster library, no UC governance |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Delta Lakehouse** is the source of truth for time-series sensor analytics (345K+ readings)\n",
        "- **Neo4j** is the source of truth for graph relationships (maintenance → components → systems → aircraft)\n",
        "- **Federated queries** combine both via Spark SQL temp views and `remote_query()`\n",
        "- The same dual-source pattern from AgentBricks (Lab 6) works directly in SQL without AI agents\n",
        "\n",
        "### References\n",
        "\n",
        "- [GUIDE_NEO4J_UC.md](../GUIDE_NEO4J_UC.md) — Full UC JDBC integration guide\n",
        "- [Neo4j JDBC SQL2Cypher](https://neo4j.com/docs/jdbc-manual/current/sql2cypher/) — SQL translation rules\n",
        "- [Databricks remote_query()](https://docs.databricks.com/sql/language-manual/functions/remote_query) — Table-valued function reference\n",
        "- [Databricks Lakehouse Federation](https://docs.databricks.com/query-federation/) — Federation overview\n",
        "- [Neo4j Spark Connector](https://neo4j.com/docs/spark/current/) — Spark Connector docs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}