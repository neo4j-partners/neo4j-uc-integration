{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Metadata Sync: Materialized Delta Tables\n\nThis notebook materializes Neo4j node labels and relationship types as **managed Delta tables**\nin Unity Catalog. When data is written as a Delta table, UC automatically registers the full\nschema metadata — column names, types, nullability, row counts, and statistics — making it\nbrowsable in **Catalog Explorer** and queryable via `INFORMATION_SCHEMA`.\n\n**What this proves:** Neo4j graph schema can be synchronized into Unity Catalog with zero\ncustom API calls. The Spark Connector infers the schema, and `saveAsTable()` does the rest.\n\n### Steps\n\n1. Load configuration from Databricks Secrets\n2. Verify Neo4j connectivity\n3. Discover all node labels and their properties\n4. Create target UC schemas (catalog must already exist — see README)\n5. Materialize a single label as a Delta table (test)\n6. Verify metadata in `INFORMATION_SCHEMA`\n7. Materialize all discovered labels\n8. Materialize relationship types\n9. Final verification and summary\n\n### Prerequisites\n\n- **Target catalog `neo4j_metadata` must already exist** — see `METADATA_SYNC_README.md`\n- `neo4j-uc-creds` secret scope configured via `setup.sh`\n- Neo4j Spark Connector installed on cluster (`org.neo4j:neo4j-connector-apache-spark_2.12:5.4.0_for_spark_3`)\n- Neo4j Python driver installed (`neo4j`)\n- **Single user** access mode cluster (required by Spark Connector)",
   "id": "intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration"
   ],
   "id": "config-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION — Loaded from Databricks Secrets\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "SCOPE_NAME = \"neo4j-uc-creds\"\n",
    "\n",
    "# Neo4j credentials\n",
    "NEO4J_HOST = dbutils.secrets.get(SCOPE_NAME, \"host\")\n",
    "NEO4J_USER = dbutils.secrets.get(SCOPE_NAME, \"user\")\n",
    "NEO4J_PASSWORD = dbutils.secrets.get(SCOPE_NAME, \"password\")\n",
    "\n",
    "try:\n",
    "    NEO4J_DATABASE = dbutils.secrets.get(SCOPE_NAME, \"database\")\n",
    "except Exception:\n",
    "    NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# Derived URLs\n",
    "NEO4J_BOLT_URI = f\"neo4j+s://{NEO4J_HOST}\"\n",
    "\n",
    "# Target catalog and schema for materialized tables\n",
    "# Change these to match your environment\n",
    "TARGET_CATALOG = \"neo4j_metadata\"\n",
    "NODES_SCHEMA = \"nodes\"\n",
    "RELATIONSHIPS_SCHEMA = \"relationships\"\n",
    "\n",
    "# Initialize cross-cell variables to avoid NameError if cells are skipped\n",
    "discovered_labels = defaultdict(list)\n",
    "discovered_relationships = defaultdict(list)\n",
    "label_results = []\n",
    "rel_results = []\n",
    "table_name = None\n",
    "full_table = None\n",
    "\n",
    "# Set Neo4j credentials at session level so they don't appear in Spark UI query plans\n",
    "spark.conf.set(\"neo4j.url\", NEO4J_BOLT_URI)\n",
    "spark.conf.set(\"neo4j.authentication.type\", \"basic\")\n",
    "spark.conf.set(\"neo4j.authentication.basic.username\", NEO4J_USER)\n",
    "spark.conf.set(\"neo4j.authentication.basic.password\", NEO4J_PASSWORD)\n",
    "spark.conf.set(\"neo4j.database\", NEO4J_DATABASE)\n",
    "\n",
    "print(\"Configuration loaded from Databricks Secrets:\")\n",
    "print(f\"  Secret Scope: {SCOPE_NAME}\")\n",
    "print(f\"  Neo4j Host: {NEO4J_HOST}\")\n",
    "print(f\"  Bolt URI: {NEO4J_BOLT_URI}\")\n",
    "print(f\"  Database: {NEO4J_DATABASE}\")\n",
    "print(f\"  Target Catalog: {TARGET_CATALOG}\")\n",
    "print(f\"  Nodes Schema: {NODES_SCHEMA}\")\n",
    "print(f\"  Relationships Schema: {RELATIONSHIPS_SCHEMA}\")\n",
    "print(f\"  Neo4j credentials: set at session level (not per-query)\")"
   ],
   "id": "config",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Verify Neo4j Connectivity"
   ],
   "id": "connectivity-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VERIFY NEO4J CONNECTIVITY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFY NEO4J CONNECTIVITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    with GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:\n",
    "        driver.verify_connectivity()\n",
    "        print(\"\\n[PASS] Driver connectivity verified\")\n",
    "\n",
    "        with driver.session(database=NEO4J_DATABASE) as session:\n",
    "            result = session.run(\"RETURN 1 AS test\")\n",
    "            record = result.single()\n",
    "            print(f\"[PASS] Query executed: RETURN 1 = {record['test']}\")\n",
    "\n",
    "            result = session.run(\"CALL dbms.components() YIELD name, versions RETURN name, versions\")\n",
    "            for record in result:\n",
    "                print(f\"[INFO] Connected to: {record['name']} {record['versions']}\")\n",
    "\n",
    "    print(\"\\nStatus: PASS\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Connection failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ],
   "id": "connectivity",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Discover Node Labels and Properties"
   ],
   "id": "discover-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# DISCOVER NEO4J SCHEMA\n",
    "# =============================================================================\n",
    "# Uses db.schema.nodeTypeProperties() and db.schema.relTypeProperties()\n",
    "# Built-in procedures, no APOC required.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISCOVER NEO4J SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "multi_label_skipped = 0\n",
    "\n",
    "try:\n",
    "    with GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:\n",
    "        with driver.session(database=NEO4J_DATABASE) as session:\n",
    "            # Discover node label properties\n",
    "            print(\"\\n[INFO] Running CALL db.schema.nodeTypeProperties()...\")\n",
    "            result = session.run(\"CALL db.schema.nodeTypeProperties()\")\n",
    "            for record in result:\n",
    "                # Skip properties with null name (labels with no properties)\n",
    "                if record[\"propertyName\"] is None:\n",
    "                    continue\n",
    "                labels = record[\"nodeLabels\"]\n",
    "                if len(labels) == 1:\n",
    "                    label = labels[0]\n",
    "                    discovered_labels[label].append({\n",
    "                        \"name\": record[\"propertyName\"],\n",
    "                        \"types\": record[\"propertyTypes\"],\n",
    "                        \"mandatory\": record[\"mandatory\"]\n",
    "                    })\n",
    "                else:\n",
    "                    multi_label_skipped += 1\n",
    "\n",
    "            # Discover relationship type properties\n",
    "            print(\"[INFO] Running CALL db.schema.relTypeProperties()...\")\n",
    "            result = session.run(\"CALL db.schema.relTypeProperties()\")\n",
    "            for record in result:\n",
    "                # Parse relType: format is `:`REL_TYPE`` — remove leading :` and trailing `\n",
    "                raw = record[\"relType\"]\n",
    "                rel_type = re.sub(r'^:`|`$', '', raw)\n",
    "                discovered_relationships[rel_type].append({\n",
    "                    \"name\": record[\"propertyName\"],\n",
    "                    \"types\": record[\"propertyTypes\"],\n",
    "                    \"mandatory\": record[\"mandatory\"]\n",
    "                })\n",
    "\n",
    "    # Display discovered schema\n",
    "    print(f\"\\nNODE LABELS ({len(discovered_labels)} discovered):\")\n",
    "    print(\"-\" * 50)\n",
    "    for label, props in sorted(discovered_labels.items()):\n",
    "        print(f\"  {label}: {len(props)} properties\")\n",
    "        for p in props[:5]:\n",
    "            types_str = \", \".join(p[\"types\"])\n",
    "            mandatory_str = \" [mandatory]\" if p[\"mandatory\"] else \"\"\n",
    "            print(f\"    - {p['name']}: {types_str}{mandatory_str}\")\n",
    "        if len(props) > 5:\n",
    "            print(f\"    ... and {len(props) - 5} more\")\n",
    "\n",
    "    if multi_label_skipped > 0:\n",
    "        print(f\"\\n[WARN] Skipped {multi_label_skipped} multi-label node type entries\")\n",
    "        print(\"  Multi-label nodes (e.g., :Person:Employee) are not materialized separately.\")\n",
    "        print(\"  Their properties appear under each individual label.\")\n",
    "\n",
    "    print(f\"\\nRELATIONSHIP TYPES ({len(discovered_relationships)} discovered):\")\n",
    "    print(\"-\" * 50)\n",
    "    for rel_type, props in sorted(discovered_relationships.items()):\n",
    "        prop_count = len([p for p in props if p[\"name\"] is not None])\n",
    "        print(f\"  {rel_type}: {prop_count} properties\")\n",
    "\n",
    "    print(f\"\\n[PASS] Schema discovery complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Schema discovery failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "id": "discover-schema",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Step 3: Create Target Schemas\n\nThe target catalog (`neo4j_metadata`) must already exist. See the README for setup instructions.\nThis step creates the `nodes` and `relationships` schemas within that catalog.",
   "id": "create-catalog-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# CREATE TARGET SCHEMAS\n# =============================================================================\n# The catalog must already exist — see METADATA_SYNC_README.md for setup.\n\nprint(\"=\" * 60)\nprint(\"CREATE TARGET SCHEMAS\")\nprint(\"=\" * 60)\n\n# Verify catalog exists\ntry:\n    spark.sql(f\"USE CATALOG `{TARGET_CATALOG}`\")\n    print(f\"\\n[PASS] Catalog '{TARGET_CATALOG}' exists\")\nexcept Exception as e:\n    print(f\"\\n[FAIL] Catalog '{TARGET_CATALOG}' not found: {e}\")\n    print(\"[INFO] Create it first — see METADATA_SYNC_README.md for instructions.\")\n\ntry:\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{TARGET_CATALOG}`.`{NODES_SCHEMA}`\")\n    print(f\"[PASS] Schema '{TARGET_CATALOG}.{NODES_SCHEMA}' exists\")\nexcept Exception as e:\n    print(f\"[FAIL] Could not create schema: {e}\")\n\ntry:\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{TARGET_CATALOG}`.`{RELATIONSHIPS_SCHEMA}`\")\n    print(f\"[PASS] Schema '{TARGET_CATALOG}.{RELATIONSHIPS_SCHEMA}' exists\")\nexcept Exception as e:\n    print(f\"[FAIL] Could not create schema: {e}\")\n\n# Verify\nprint(f\"\\n[INFO] Target structure:\")\nprint(f\"  {TARGET_CATALOG}\")\nprint(f\"  ├── {NODES_SCHEMA}\")\nprint(f\"  └── {RELATIONSHIPS_SCHEMA}\")",
   "id": "create-catalog",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Materialize One Label (Single Label Test)\n",
    "\n",
    "Read the first discovered label from Neo4j via the Spark Connector and write it as a\n",
    "managed Delta table. This validates the full pipeline before running it for all labels."
   ],
   "id": "single-label-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# MATERIALIZE ONE LABEL AS A DELTA TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MATERIALIZE SINGLE LABEL (TEST)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not discovered_labels:\n",
    "    print(\"\\n[FAIL] No labels discovered — cannot proceed.\")\n",
    "    print(\"[INFO] Check the schema discovery cell above for errors.\")\n",
    "else:\n",
    "    test_label = sorted(discovered_labels.keys())[0]\n",
    "    table_name = test_label.lower()\n",
    "    full_table = f\"`{TARGET_CATALOG}`.`{NODES_SCHEMA}`.`{table_name}`\"\n",
    "\n",
    "    print(f\"\\n[INFO] Label: {test_label}\")\n",
    "    print(f\"[INFO] Target table: {full_table}\")\n",
    "\n",
    "    try:\n",
    "        # Read from Neo4j via Spark Connector\n",
    "        # Credentials are set at session level in the config cell\n",
    "        df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "            .option(\"labels\", f\":{test_label}\") \\\n",
    "            .load()\n",
    "\n",
    "        print(f\"\\n[INFO] Inferred schema for :{test_label}:\")\n",
    "        df.printSchema()\n",
    "\n",
    "        print(f\"[INFO] Sample data (5 rows):\")\n",
    "        df.show(5, truncate=False)\n",
    "\n",
    "        col_count = len(df.columns)\n",
    "\n",
    "        # Write as managed Delta table with overwriteSchema for idempotent reruns\n",
    "        df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(full_table)\n",
    "\n",
    "        # Get row count from the written table (avoids double-scanning Neo4j)\n",
    "        row_count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {full_table}\").collect()[0][\"cnt\"]\n",
    "\n",
    "        print(f\"\\n[PASS] Materialized :{test_label} → {full_table}\")\n",
    "        print(f\"  Rows: {row_count}\")\n",
    "        print(f\"  Columns: {col_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FAIL] Materialization failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ],
   "id": "single-label",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Verify Metadata in INFORMATION_SCHEMA\n",
    "\n",
    "Confirm that the materialized table and its columns are now visible in Unity Catalog's\n",
    "`INFORMATION_SCHEMA`. This is the key proof that metadata sync worked — the table appears\n",
    "in Catalog Explorer with full column definitions."
   ],
   "id": "verify-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VERIFY METADATA IN INFORMATION_SCHEMA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFY METADATA IN INFORMATION_SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if table_name is None:\n",
    "    print(\"\\n[SKIP] No table was materialized — skipping verification.\")\n",
    "else:\n",
    "    # Check tables\n",
    "    print(f\"\\n[INFO] Tables in {TARGET_CATALOG}.{NODES_SCHEMA}:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT table_name, table_type, comment, created\n",
    "        FROM `{TARGET_CATALOG}`.information_schema.tables\n",
    "        WHERE table_schema = '{NODES_SCHEMA}'\n",
    "        ORDER BY table_name\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    # Check columns for the test table\n",
    "    print(f\"\\n[INFO] Columns in {TARGET_CATALOG}.{NODES_SCHEMA}.{table_name}:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT ordinal_position, column_name, data_type, is_nullable\n",
    "        FROM `{TARGET_CATALOG}`.information_schema.columns\n",
    "        WHERE table_schema = '{NODES_SCHEMA}'\n",
    "          AND table_name = '{table_name}'\n",
    "        ORDER BY ordinal_position\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    # Verify row count via SQL\n",
    "    count_result = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {full_table}\").collect()[0][\"cnt\"]\n",
    "    print(f\"[PASS] Table {full_table} has {count_result} rows\")\n",
    "    print(f\"[PASS] Metadata is visible in INFORMATION_SCHEMA and Catalog Explorer\")"
   ],
   "id": "verify-metadata",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Materialize All Discovered Labels\n",
    "\n",
    "Loop through all discovered node labels and materialize each one as a managed Delta table."
   ],
   "id": "all-labels-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# MATERIALIZE ALL NODE LABELS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"MATERIALIZE ALL NODE LABELS ({len(discovered_labels)} labels)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "label_results = []\n",
    "\n",
    "for label in sorted(discovered_labels.keys()):\n",
    "    tbl_name = label.lower()\n",
    "    full_tbl = f\"`{TARGET_CATALOG}`.`{NODES_SCHEMA}`.`{tbl_name}`\"\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "            .option(\"labels\", f\":{label}\") \\\n",
    "            .load()\n",
    "\n",
    "        col_count = len(df.columns)\n",
    "\n",
    "        df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(full_tbl)\n",
    "\n",
    "        # Get row count from written table (avoids double Neo4j scan)\n",
    "        row_count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {full_tbl}\").collect()[0][\"cnt\"]\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        label_results.append({\n",
    "            \"label\": label,\n",
    "            \"table\": tbl_name,\n",
    "            \"rows\": row_count,\n",
    "            \"columns\": col_count,\n",
    "            \"time_s\": round(elapsed, 1),\n",
    "            \"status\": \"PASS\"\n",
    "        })\n",
    "        print(f\"  [PASS] :{label} → {full_tbl} ({row_count} rows, {col_count} cols, {elapsed:.1f}s)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        error_msg = str(e).split('\\n')[0][:80]\n",
    "        label_results.append({\n",
    "            \"label\": label,\n",
    "            \"table\": tbl_name,\n",
    "            \"rows\": 0,\n",
    "            \"columns\": 0,\n",
    "            \"time_s\": round(elapsed, 1),\n",
    "            \"status\": f\"FAIL: {error_msg}\"\n",
    "        })\n",
    "        print(f\"  [FAIL] :{label} — {error_msg}\")\n",
    "\n",
    "# Summary\n",
    "passed = [r for r in label_results if r[\"status\"] == \"PASS\"]\n",
    "failed = [r for r in label_results if r[\"status\"] != \"PASS\"]\n",
    "total_rows = sum(r[\"rows\"] for r in passed)\n",
    "total_cols = sum(r[\"columns\"] for r in passed)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"NODE LABELS SUMMARY\")\n",
    "print(f\"  Passed: {len(passed)}/{len(label_results)}\")\n",
    "print(f\"  Total rows: {total_rows:,}\")\n",
    "print(f\"  Total columns: {total_cols}\")\n",
    "if failed:\n",
    "    print(f\"  Failed: {', '.join(r['label'] for r in failed)}\")"
   ],
   "id": "all-labels",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Step 7: Materialize Relationship Types\n\nRead relationship types from Neo4j and write them as Delta tables in the `relationships` schema.\nThe Spark Connector's `relationship` option requires `relationship.source.labels` and\n`relationship.target.labels`, so we first discover the actual patterns via a `MATCH` query.",
   "id": "relationships-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# MATERIALIZE RELATIONSHIP TYPES\n# =============================================================================\n\nprint(\"=\" * 60)\nprint(\"MATERIALIZE RELATIONSHIP TYPES\")\nprint(\"=\" * 60)\n\n# Discover relationship patterns (source label, type, target label)\n# db.schema.relTypeProperties() does NOT return source/target labels,\n# so we use a MATCH query to discover actual patterns from the data.\nrel_patterns = []\ntry:\n    with GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:\n        with driver.session(database=NEO4J_DATABASE) as session:\n            result = session.run(\"\"\"\n                MATCH (src)-[r]->(tgt)\n                WITH type(r) AS relType, labels(src) AS srcLabels, labels(tgt) AS tgtLabels\n                RETURN DISTINCT relType, srcLabels[0] AS sourceLabel, tgtLabels[0] AS targetLabel\n                ORDER BY relType\n            \"\"\")\n            for record in result:\n                rel_patterns.append({\n                    \"type\": record[\"relType\"],\n                    \"source\": record[\"sourceLabel\"],\n                    \"target\": record[\"targetLabel\"]\n                })\n\n    print(f\"\\n[INFO] Found {len(rel_patterns)} relationship patterns\")\n    for p in rel_patterns:\n        print(f\"  (:{p['source']})-[:{p['type']}]->(:{p['target']})\")\n\nexcept Exception as e:\n    print(f\"\\n[FAIL] Could not discover relationship patterns: {e}\")\n    print(\"[INFO] Cannot proceed without source/target labels — the Spark Connector requires them.\")\n    import traceback\n    traceback.print_exc()\n\nrel_results = []\n\nfor pattern in rel_patterns:\n    rel_type = pattern[\"type\"]\n    source_label = pattern[\"source\"]\n    target_label = pattern[\"target\"]\n    tbl_name = rel_type.lower()\n    full_tbl = f\"`{TARGET_CATALOG}`.`{RELATIONSHIPS_SCHEMA}`.`{tbl_name}`\"\n\n    start = time.time()\n    try:\n        df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n            .option(\"relationship\", rel_type) \\\n            .option(\"relationship.source.labels\", f\":{source_label}\") \\\n            .option(\"relationship.target.labels\", f\":{target_label}\") \\\n            .load()\n\n        col_count = len(df.columns)\n\n        df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(full_tbl)\n\n        # Get row count from written table\n        row_count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {full_tbl}\").collect()[0][\"cnt\"]\n\n        elapsed = time.time() - start\n        rel_results.append({\n            \"type\": rel_type,\n            \"pattern\": f\"(:{source_label})-[:{rel_type}]->(:{target_label})\",\n            \"table\": tbl_name,\n            \"rows\": row_count,\n            \"columns\": col_count,\n            \"time_s\": round(elapsed, 1),\n            \"status\": \"PASS\"\n        })\n        print(f\"  [PASS] [:{rel_type}] → {full_tbl} ({row_count} rows, {col_count} cols, {elapsed:.1f}s)\")\n\n    except Exception as e:\n        elapsed = time.time() - start\n        error_msg = str(e).split('\\n')[0][:80]\n        rel_results.append({\n            \"type\": rel_type,\n            \"pattern\": f\"(:{source_label})-[:{rel_type}]->(:{target_label})\",\n            \"table\": tbl_name,\n            \"rows\": 0,\n            \"columns\": 0,\n            \"time_s\": round(elapsed, 1),\n            \"status\": f\"FAIL: {error_msg}\"\n        })\n        print(f\"  [FAIL] [:{rel_type}] — {error_msg}\")\n\n# Summary\npassed_rels = [r for r in rel_results if r[\"status\"] == \"PASS\"]\nfailed_rels = [r for r in rel_results if r[\"status\"] != \"PASS\"]\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(f\"RELATIONSHIP TYPES SUMMARY\")\nprint(f\"  Passed: {len(passed_rels)}/{len(rel_results)}\")\nprint(f\"  Total rows: {sum(r['rows'] for r in passed_rels):,}\")\nif failed_rels:\n    print(f\"  Failed: {', '.join(r['type'] for r in failed_rels)}\")",
   "id": "relationships",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Final Verification and Summary\n",
    "\n",
    "Query `INFORMATION_SCHEMA` to show all tables created across both schemas. This is the\n",
    "final proof that Neo4j metadata is synchronized into Unity Catalog."
   ],
   "id": "summary-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# FINAL VERIFICATION AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# All tables in the catalog\n",
    "print(f\"\\n[INFO] All tables in {TARGET_CATALOG}:\")\n",
    "all_tables_df = spark.sql(f\"\"\"\n",
    "    SELECT table_schema, table_name, table_type\n",
    "    FROM `{TARGET_CATALOG}`.information_schema.tables\n",
    "    WHERE table_schema IN ('{NODES_SCHEMA}', '{RELATIONSHIPS_SCHEMA}')\n",
    "    ORDER BY table_schema, table_name\n",
    "\"\"\")\n",
    "all_tables_df.show(50, truncate=False)\n",
    "\n",
    "# Column counts per table\n",
    "print(f\"\\n[INFO] Column counts per table:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT table_schema, table_name, COUNT(*) AS column_count\n",
    "    FROM `{TARGET_CATALOG}`.information_schema.columns\n",
    "    WHERE table_schema IN ('{NODES_SCHEMA}', '{RELATIONSHIPS_SCHEMA}')\n",
    "    GROUP BY table_schema, table_name\n",
    "    ORDER BY table_schema, table_name\n",
    "\"\"\").show(50, truncate=False)\n",
    "\n",
    "# Overall summary\n",
    "total_tables = all_tables_df.count()\n",
    "node_tables = len([r for r in label_results if r[\"status\"] == \"PASS\"])\n",
    "rel_tables_count = len([r for r in rel_results if r[\"status\"] == \"PASS\"])\n",
    "total_data_rows = (sum(r['rows'] for r in label_results if r['status'] == 'PASS') +\n",
    "                   sum(r['rows'] for r in rel_results if r['status'] == 'PASS'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METADATA SYNC SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Target Catalog: {TARGET_CATALOG}\")\n",
    "print(f\"  Node label tables: {node_tables} (in {NODES_SCHEMA})\")\n",
    "print(f\"  Relationship tables: {rel_tables_count} (in {RELATIONSHIPS_SCHEMA})\")\n",
    "print(f\"  Total tables: {total_tables}\")\n",
    "print(f\"  Total data rows: {total_data_rows:,}\")\n",
    "print(f\"\\n  All tables are:\")\n",
    "print(f\"    - Browsable in Catalog Explorer\")\n",
    "print(f\"    - Visible in INFORMATION_SCHEMA\")\n",
    "print(f\"    - Queryable via standard SQL\")\n",
    "print(f\"    - Governed by UC permissions\")\n",
    "print(f\"\\n[PASS] Metadata synchronization complete\")"
   ],
   "id": "summary",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}