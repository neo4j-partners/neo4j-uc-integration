{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Sync: Materialized Delta Tables (Approach 3)\n",
    "\n",
    "This notebook materializes Neo4j node labels and relationship types as **managed Delta tables**\n",
    "in Unity Catalog. When data is written as a Delta table, UC automatically registers the full\n",
    "schema metadata — column names, types, nullability, row counts, and statistics — making it\n",
    "browsable in **Catalog Explorer** and queryable via `INFORMATION_SCHEMA`.\n",
    "\n",
    "**What this proves:** Neo4j graph schema can be synchronized into Unity Catalog with zero\n",
    "custom API calls. The Spark Connector infers the schema, and `saveAsTable()` does the rest.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Load configuration from Databricks Secrets\n",
    "2. Verify Neo4j connectivity\n",
    "3. Discover all node labels and their properties\n",
    "4. Create target UC catalog and schema\n",
    "5. Materialize a single label as a Delta table (test)\n",
    "6. Verify metadata in `INFORMATION_SCHEMA`\n",
    "7. Materialize all discovered labels\n",
    "8. Materialize relationship types\n",
    "9. Final verification and summary\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- `neo4j-uc-creds` secret scope configured via `setup.sh`\n",
    "- Neo4j Spark Connector installed on cluster (`org.neo4j:neo4j-connector-apache-spark_2.12:5.3.10_for_spark_3`)\n",
    "- Neo4j Python driver installed (`neo4j`)\n",
    "- **Single user** access mode cluster (required by Spark Connector)"
   ],
   "id": "intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration"
   ],
   "id": "config-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION — Loaded from Databricks Secrets\n",
    "# =============================================================================\n",
    "\n",
    "SCOPE_NAME = \"neo4j-uc-creds\"\n",
    "\n",
    "# Neo4j credentials\n",
    "NEO4J_HOST = dbutils.secrets.get(SCOPE_NAME, \"host\")\n",
    "NEO4J_USER = dbutils.secrets.get(SCOPE_NAME, \"user\")\n",
    "NEO4J_PASSWORD = dbutils.secrets.get(SCOPE_NAME, \"password\")\n",
    "\n",
    "try:\n",
    "    NEO4J_DATABASE = dbutils.secrets.get(SCOPE_NAME, \"database\")\n",
    "except Exception:\n",
    "    NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# Derived URLs\n",
    "NEO4J_BOLT_URI = f\"neo4j+s://{NEO4J_HOST}\"\n",
    "\n",
    "# Target catalog and schema for materialized tables\n",
    "# Change these to match your environment\n",
    "TARGET_CATALOG = \"neo4j_metadata\"\n",
    "NODES_SCHEMA = \"nodes\"\n",
    "RELATIONSHIPS_SCHEMA = \"relationships\"\n",
    "\n",
    "print(\"Configuration loaded from Databricks Secrets:\")\n",
    "print(f\"  Secret Scope: {SCOPE_NAME}\")\n",
    "print(f\"  Neo4j Host: {NEO4J_HOST}\")\n",
    "print(f\"  Bolt URI: {NEO4J_BOLT_URI}\")\n",
    "print(f\"  Database: {NEO4J_DATABASE}\")\n",
    "print(f\"  Target Catalog: {TARGET_CATALOG}\")\n",
    "print(f\"  Nodes Schema: {NODES_SCHEMA}\")\n",
    "print(f\"  Relationships Schema: {RELATIONSHIPS_SCHEMA}\")"
   ],
   "id": "config",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Verify Neo4j Connectivity"
   ],
   "id": "connectivity-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VERIFY NEO4J CONNECTIVITY\n",
    "# =============================================================================\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFY NEO4J CONNECTIVITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    driver.verify_connectivity()\n",
    "    print(\"\\n[PASS] Driver connectivity verified\")\n",
    "\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        result = session.run(\"RETURN 1 AS test\")\n",
    "        record = result.single()\n",
    "        print(f\"[PASS] Query executed: RETURN 1 = {record['test']}\")\n",
    "\n",
    "        result = session.run(\"CALL dbms.components() YIELD name, versions RETURN name, versions\")\n",
    "        for record in result:\n",
    "            print(f\"[INFO] Connected to: {record['name']} {record['versions']}\")\n",
    "\n",
    "    driver.close()\n",
    "    print(\"\\nStatus: PASS\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Connection failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ],
   "id": "connectivity",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Discover Node Labels and Properties"
   ],
   "id": "discover-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# DISCOVER NEO4J SCHEMA\n",
    "# =============================================================================\n",
    "# Uses db.schema.nodeTypeProperties() — built-in, no APOC required.\n",
    "# Returns label names, property names, property types, and mandatory flags.\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISCOVER NEO4J SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "discovered_labels = defaultdict(list)\n",
    "discovered_relationships = defaultdict(list)\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        # Discover node label properties\n",
    "        print(\"\\n[INFO] Running CALL db.schema.nodeTypeProperties()...\")\n",
    "        result = session.run(\"CALL db.schema.nodeTypeProperties()\")\n",
    "        for record in result:\n",
    "            labels = record[\"nodeLabels\"]\n",
    "            if len(labels) == 1:\n",
    "                label = labels[0]\n",
    "                discovered_labels[label].append({\n",
    "                    \"name\": record[\"propertyName\"],\n",
    "                    \"types\": record[\"propertyTypes\"],\n",
    "                    \"mandatory\": record[\"mandatory\"]\n",
    "                })\n",
    "\n",
    "        # Discover relationship type properties\n",
    "        print(\"[INFO] Running CALL db.schema.relTypeProperties()...\")\n",
    "        result = session.run(\"CALL db.schema.relTypeProperties()\")\n",
    "        for record in result:\n",
    "            rel_type = record[\"relType\"].strip(\":`\")\n",
    "            discovered_relationships[rel_type].append({\n",
    "                \"name\": record[\"propertyName\"],\n",
    "                \"types\": record[\"propertyTypes\"],\n",
    "                \"mandatory\": record[\"mandatory\"]\n",
    "            })\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    # Display discovered schema\n",
    "    print(f\"\\nNODE LABELS ({len(discovered_labels)} discovered):\")\n",
    "    print(\"-\" * 50)\n",
    "    for label, props in sorted(discovered_labels.items()):\n",
    "        print(f\"  {label}: {len(props)} properties\")\n",
    "        for p in props[:5]:\n",
    "            types_str = \", \".join(p[\"types\"])\n",
    "            mandatory_str = \" [mandatory]\" if p[\"mandatory\"] else \"\"\n",
    "            print(f\"    - {p['name']}: {types_str}{mandatory_str}\")\n",
    "        if len(props) > 5:\n",
    "            print(f\"    ... and {len(props) - 5} more\")\n",
    "\n",
    "    print(f\"\\nRELATIONSHIP TYPES ({len(discovered_relationships)} discovered):\")\n",
    "    print(\"-\" * 50)\n",
    "    for rel_type, props in sorted(discovered_relationships.items()):\n",
    "        prop_count = len([p for p in props if p[\"name\"] is not None])\n",
    "        print(f\"  {rel_type}: {prop_count} properties\")\n",
    "\n",
    "    print(f\"\\n[PASS] Schema discovery complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Schema discovery failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "id": "discover-schema",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Create Target Catalog and Schema"
   ],
   "id": "create-catalog-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CREATE TARGET CATALOG AND SCHEMAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATE TARGET CATALOG AND SCHEMAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{TARGET_CATALOG}`\")\n",
    "    print(f\"\\n[PASS] Catalog '{TARGET_CATALOG}' exists\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Could not create catalog: {e}\")\n",
    "    print(\"[INFO] You may need CREATE CATALOG privilege. Ask your admin.\")\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{TARGET_CATALOG}`.`{NODES_SCHEMA}`\")\n",
    "    print(f\"[PASS] Schema '{TARGET_CATALOG}.{NODES_SCHEMA}' exists\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Could not create schema: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{TARGET_CATALOG}`.`{RELATIONSHIPS_SCHEMA}`\")\n",
    "    print(f\"[PASS] Schema '{TARGET_CATALOG}.{RELATIONSHIPS_SCHEMA}' exists\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Could not create schema: {e}\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\n[INFO] Target structure:\")\n",
    "print(f\"  {TARGET_CATALOG}\")\n",
    "print(f\"  ├── {NODES_SCHEMA}\")\n",
    "print(f\"  └── {RELATIONSHIPS_SCHEMA}\")"
   ],
   "id": "create-catalog",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Materialize One Label (Single Label Test)\n",
    "\n",
    "Read the first discovered label from Neo4j via the Spark Connector and write it as a\n",
    "managed Delta table. This validates the full pipeline before running it for all labels."
   ],
   "id": "single-label-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# MATERIALIZE ONE LABEL AS A DELTA TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MATERIALIZE SINGLE LABEL (TEST)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pick the first discovered label\n",
    "test_label = sorted(discovered_labels.keys())[0] if discovered_labels else \"Aircraft\"\n",
    "table_name = test_label.lower()\n",
    "full_table = f\"`{TARGET_CATALOG}`.`{NODES_SCHEMA}`.`{table_name}`\"\n",
    "\n",
    "print(f\"\\n[INFO] Label: {test_label}\")\n",
    "print(f\"[INFO] Target table: {full_table}\")\n",
    "\n",
    "try:\n",
    "    # Read from Neo4j via Spark Connector\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .option(\"url\", NEO4J_BOLT_URI) \\\n",
    "        .option(\"authentication.type\", \"basic\") \\\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"database\", NEO4J_DATABASE) \\\n",
    "        .option(\"labels\", f\":{test_label}\") \\\n",
    "        .load()\n",
    "\n",
    "    print(f\"\\n[INFO] Inferred schema for :{test_label}:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    print(f\"[INFO] Sample data (5 rows):\")\n",
    "    df.show(5, truncate=False)\n",
    "\n",
    "    row_count = df.count()\n",
    "    col_count = len(df.columns)\n",
    "\n",
    "    # Write as managed Delta table\n",
    "    df.write.mode(\"overwrite\").saveAsTable(full_table)\n",
    "\n",
    "    print(f\"\\n[PASS] Materialized :{test_label} → {full_table}\")\n",
    "    print(f\"  Rows: {row_count}\")\n",
    "    print(f\"  Columns: {col_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Materialization failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "id": "single-label",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Verify Metadata in INFORMATION_SCHEMA\n",
    "\n",
    "Confirm that the materialized table and its columns are now visible in Unity Catalog's\n",
    "`INFORMATION_SCHEMA`. This is the key proof that metadata sync worked — the table appears\n",
    "in Catalog Explorer with full column definitions."
   ],
   "id": "verify-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VERIFY METADATA IN INFORMATION_SCHEMA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFY METADATA IN INFORMATION_SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check tables\n",
    "print(f\"\\n[INFO] Tables in {TARGET_CATALOG}.{NODES_SCHEMA}:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT table_name, table_type, comment, created\n",
    "    FROM `{TARGET_CATALOG}`.information_schema.tables\n",
    "    WHERE table_schema = '{NODES_SCHEMA}'\n",
    "    ORDER BY table_name\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Check columns for the test table\n",
    "print(f\"\\n[INFO] Columns in {TARGET_CATALOG}.{NODES_SCHEMA}.{table_name}:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT ordinal_position, column_name, data_type, is_nullable\n",
    "    FROM `{TARGET_CATALOG}`.information_schema.columns\n",
    "    WHERE table_schema = '{NODES_SCHEMA}'\n",
    "      AND table_name = '{table_name}'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Verify row count via SQL\n",
    "count_result = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {full_table}\").collect()[0][\"cnt\"]\n",
    "print(f\"[PASS] Table {full_table} has {count_result} rows\")\n",
    "print(f\"[PASS] Metadata is visible in INFORMATION_SCHEMA and Catalog Explorer\")"
   ],
   "id": "verify-metadata",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Materialize All Discovered Labels\n",
    "\n",
    "Loop through all discovered node labels and materialize each one as a managed Delta table."
   ],
   "id": "all-labels-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# MATERIALIZE ALL NODE LABELS\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"MATERIALIZE ALL NODE LABELS ({len(discovered_labels)} labels)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "label_results = []\n",
    "\n",
    "for label in sorted(discovered_labels.keys()):\n",
    "    tbl_name = label.lower()\n",
    "    full_tbl = f\"`{TARGET_CATALOG}`.`{NODES_SCHEMA}`.`{tbl_name}`\"\n",
    "\n",
    "    try:\n",
    "        start = time.time()\n",
    "\n",
    "        df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "            .option(\"url\", NEO4J_BOLT_URI) \\\n",
    "            .option(\"authentication.type\", \"basic\") \\\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASSWORD) \\\n",
    "            .option(\"database\", NEO4J_DATABASE) \\\n",
    "            .option(\"labels\", f\":{label}\") \\\n",
    "            .load()\n",
    "\n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "\n",
    "        df.write.mode(\"overwrite\").saveAsTable(full_tbl)\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        label_results.append({\n",
    "            \"label\": label,\n",
    "            \"table\": tbl_name,\n",
    "            \"rows\": row_count,\n",
    "            \"columns\": col_count,\n",
    "            \"time_s\": round(elapsed, 1),\n",
    "            \"status\": \"PASS\"\n",
    "        })\n",
    "        print(f\"  [PASS] :{label} → {full_tbl} ({row_count} rows, {col_count} cols, {elapsed:.1f}s)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        error_msg = str(e).split('\\n')[0][:80]\n",
    "        label_results.append({\n",
    "            \"label\": label,\n",
    "            \"table\": tbl_name,\n",
    "            \"rows\": 0,\n",
    "            \"columns\": 0,\n",
    "            \"time_s\": round(elapsed, 1),\n",
    "            \"status\": f\"FAIL: {error_msg}\"\n",
    "        })\n",
    "        print(f\"  [FAIL] :{label} — {error_msg}\")\n",
    "\n",
    "# Summary\n",
    "passed = [r for r in label_results if r[\"status\"] == \"PASS\"]\n",
    "failed = [r for r in label_results if r[\"status\"] != \"PASS\"]\n",
    "total_rows = sum(r[\"rows\"] for r in passed)\n",
    "total_cols = sum(r[\"columns\"] for r in passed)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"NODE LABELS SUMMARY\")\n",
    "print(f\"  Passed: {len(passed)}/{len(label_results)}\")\n",
    "print(f\"  Total rows: {total_rows:,}\")\n",
    "print(f\"  Total columns: {total_cols}\")\n",
    "if failed:\n",
    "    print(f\"  Failed: {', '.join(r['label'] for r in failed)}\")"
   ],
   "id": "all-labels",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Materialize Relationship Types\n",
    "\n",
    "Read relationship types from Neo4j and write them as Delta tables in the `relationships` schema.\n",
    "The Spark Connector's `relationship` option reads relationship data including source/target node IDs."
   ],
   "id": "relationships-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# MATERIALIZE RELATIONSHIP TYPES\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MATERIALIZE RELATIONSHIP TYPES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, discover relationship patterns with source/target labels\n",
    "rel_patterns = []\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        result = session.run(\"\"\"\n",
    "            CALL db.schema.visualization() YIELD nodes, relationships\n",
    "            UNWIND relationships AS rel\n",
    "            RETURN DISTINCT type(rel) AS rel_type,\n",
    "                   labels(startNode(rel))[0] AS source_label,\n",
    "                   labels(endNode(rel))[0] AS target_label\n",
    "        \"\"\")\n",
    "        for record in result:\n",
    "            rel_patterns.append({\n",
    "                \"type\": record[\"rel_type\"],\n",
    "                \"source\": record[\"source_label\"],\n",
    "                \"target\": record[\"target_label\"]\n",
    "            })\n",
    "    driver.close()\n",
    "    print(f\"\\n[INFO] Found {len(rel_patterns)} relationship patterns\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[WARN] Could not discover relationship patterns: {e}\")\n",
    "    print(\"[INFO] Falling back to discovered_relationships keys\")\n",
    "    for rel_type in discovered_relationships:\n",
    "        rel_patterns.append({\"type\": rel_type, \"source\": \"*\", \"target\": \"*\"})\n",
    "\n",
    "rel_results = []\n",
    "\n",
    "for pattern in rel_patterns:\n",
    "    rel_type = pattern[\"type\"]\n",
    "    source_label = pattern[\"source\"]\n",
    "    target_label = pattern[\"target\"]\n",
    "    tbl_name = rel_type.lower()\n",
    "    full_tbl = f\"`{TARGET_CATALOG}`.`{RELATIONSHIPS_SCHEMA}`.`{tbl_name}`\"\n",
    "\n",
    "    try:\n",
    "        start = time.time()\n",
    "\n",
    "        reader = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "            .option(\"url\", NEO4J_BOLT_URI) \\\n",
    "            .option(\"authentication.type\", \"basic\") \\\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASSWORD) \\\n",
    "            .option(\"database\", NEO4J_DATABASE) \\\n",
    "            .option(\"relationship\", rel_type)\n",
    "\n",
    "        if source_label and source_label != \"*\":\n",
    "            reader = reader.option(\"relationship.source.labels\", f\":{source_label}\")\n",
    "        if target_label and target_label != \"*\":\n",
    "            reader = reader.option(\"relationship.target.labels\", f\":{target_label}\")\n",
    "\n",
    "        df = reader.load()\n",
    "\n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "\n",
    "        df.write.mode(\"overwrite\").saveAsTable(full_tbl)\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        rel_results.append({\n",
    "            \"type\": rel_type,\n",
    "            \"pattern\": f\"(:{source_label})-[:{rel_type}]->(:{target_label})\",\n",
    "            \"table\": tbl_name,\n",
    "            \"rows\": row_count,\n",
    "            \"columns\": col_count,\n",
    "            \"time_s\": round(elapsed, 1),\n",
    "            \"status\": \"PASS\"\n",
    "        })\n",
    "        print(f\"  [PASS] [:{rel_type}] → {full_tbl} ({row_count} rows, {col_count} cols, {elapsed:.1f}s)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        error_msg = str(e).split('\\n')[0][:80]\n",
    "        rel_results.append({\n",
    "            \"type\": rel_type,\n",
    "            \"pattern\": f\"(:{source_label})-[:{rel_type}]->(:{target_label})\",\n",
    "            \"table\": tbl_name,\n",
    "            \"rows\": 0,\n",
    "            \"columns\": 0,\n",
    "            \"time_s\": round(elapsed, 1),\n",
    "            \"status\": f\"FAIL: {error_msg}\"\n",
    "        })\n",
    "        print(f\"  [FAIL] [:{rel_type}] — {error_msg}\")\n",
    "\n",
    "# Summary\n",
    "passed_rels = [r for r in rel_results if r[\"status\"] == \"PASS\"]\n",
    "failed_rels = [r for r in rel_results if r[\"status\"] != \"PASS\"]\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"RELATIONSHIP TYPES SUMMARY\")\n",
    "print(f\"  Passed: {len(passed_rels)}/{len(rel_results)}\")\n",
    "print(f\"  Total rows: {sum(r['rows'] for r in passed_rels):,}\")\n",
    "if failed_rels:\n",
    "    print(f\"  Failed: {', '.join(r['type'] for r in failed_rels)}\")"
   ],
   "id": "relationships",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Final Verification and Summary\n",
    "\n",
    "Query `INFORMATION_SCHEMA` to show all tables created across both schemas. This is the\n",
    "final proof that Neo4j metadata is synchronized into Unity Catalog."
   ],
   "id": "summary-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# FINAL VERIFICATION AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# All tables in the catalog\n",
    "print(f\"\\n[INFO] All tables in {TARGET_CATALOG}:\")\n",
    "all_tables_df = spark.sql(f\"\"\"\n",
    "    SELECT table_schema, table_name, table_type\n",
    "    FROM `{TARGET_CATALOG}`.information_schema.tables\n",
    "    WHERE table_schema IN ('{NODES_SCHEMA}', '{RELATIONSHIPS_SCHEMA}')\n",
    "    ORDER BY table_schema, table_name\n",
    "\"\"\")\n",
    "all_tables_df.show(50, truncate=False)\n",
    "\n",
    "# Column counts per table\n",
    "print(f\"\\n[INFO] Column counts per table:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT table_schema, table_name, COUNT(*) AS column_count\n",
    "    FROM `{TARGET_CATALOG}`.information_schema.columns\n",
    "    WHERE table_schema IN ('{NODES_SCHEMA}', '{RELATIONSHIPS_SCHEMA}')\n",
    "    GROUP BY table_schema, table_name\n",
    "    ORDER BY table_schema, table_name\n",
    "\"\"\").show(50, truncate=False)\n",
    "\n",
    "# Overall summary\n",
    "total_tables = all_tables_df.count()\n",
    "node_tables = len(passed) if 'passed' in dir() else 0\n",
    "rel_tables = len(passed_rels) if 'passed_rels' in dir() else 0\n",
    "total_data_rows = (sum(r['rows'] for r in label_results if r['status'] == 'PASS') +\n",
    "                   sum(r['rows'] for r in rel_results if r['status'] == 'PASS'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METADATA SYNC SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Target Catalog: {TARGET_CATALOG}\")\n",
    "print(f\"  Node label tables: {node_tables} (in {NODES_SCHEMA})\")\n",
    "print(f\"  Relationship tables: {rel_tables} (in {RELATIONSHIPS_SCHEMA})\")\n",
    "print(f\"  Total tables: {total_tables}\")\n",
    "print(f\"  Total data rows: {total_data_rows:,}\")\n",
    "print(f\"\\n  All tables are:\")\n",
    "print(f\"    - Browsable in Catalog Explorer\")\n",
    "print(f\"    - Visible in INFORMATION_SCHEMA\")\n",
    "print(f\"    - Queryable via standard SQL\")\n",
    "print(f\"    - Governed by UC permissions\")\n",
    "print(f\"\\n[PASS] Metadata synchronization complete\")"
   ],
   "id": "summary",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
