{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-issue-summary",
   "metadata": {},
   "source": [
    "# Neo4j JDBC Unity Catalog Connection - Support Ticket\n",
    "\n",
    "## Issue Summary\n",
    "\n",
    "**Problem**: Unity Catalog JDBC connection to Neo4j fails with `Connection was closed before the operation completed` error, despite:\n",
    "- Network connectivity working (TCP test passes)\n",
    "- Neo4j Python driver working\n",
    "- Neo4j Spark Connector working\n",
    "\n",
    "**Error Location**: `com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected`\n",
    "\n",
    "This notebook provides a systematic test progression to isolate the failure point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": "---\n\n## Configuration\n\n**Prerequisites**: Run `setup.sh` to configure Databricks secrets before running this notebook.\n\nThe setup script reads credentials from `.env` and stores them in the `neo4j-uc-creds` secret scope:\n- `host` - Neo4j host\n- `user` - Neo4j username\n- `password` - Neo4j password\n- `connection_name` - Unity Catalog connection name\n- `jdbc_jar_path` - Path to Neo4j JDBC full bundle JAR in UC Volume\n- `cleaner_jar_path` - Path to Neo4j JDBC Spark cleaner JAR in UC Volume\n- `database` - Neo4j database (optional, defaults to \"neo4j\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-vars",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION - Loaded from Databricks Secrets\n# =============================================================================\n# Secrets are configured using setup.sh which creates scope \"neo4j-uc-creds\"\n# with secrets: host, user, password, connection_name, jdbc_jar_path, cleaner_jar_path, database\n\nSCOPE_NAME = \"neo4j-uc-creds\"\n\n# Aura Connection Details (from secrets)\nNEO4J_HOST = dbutils.secrets.get(SCOPE_NAME, \"host\")\nNEO4J_USER = dbutils.secrets.get(SCOPE_NAME, \"user\")\nNEO4J_PASSWORD = dbutils.secrets.get(SCOPE_NAME, \"password\")\n\n# Database defaults to \"neo4j\" if not set\ntry:\n    NEO4J_DATABASE = dbutils.secrets.get(SCOPE_NAME, \"database\")\nexcept:\n    NEO4J_DATABASE = \"neo4j\"\n\n# Unity Catalog Resources (from secrets)\nJDBC_JAR_PATH = dbutils.secrets.get(SCOPE_NAME, \"jdbc_jar_path\")\nCLEANER_JAR_PATH = dbutils.secrets.get(SCOPE_NAME, \"cleaner_jar_path\")\nUC_CONNECTION_NAME = dbutils.secrets.get(SCOPE_NAME, \"connection_name\")\n\n# Combined java_dependencies for CREATE CONNECTION\nJAVA_DEPENDENCIES = f'[\"{JDBC_JAR_PATH}\", \"{CLEANER_JAR_PATH}\"]'\n\n# Derived URLs (no need to edit)\nNEO4J_BOLT_URI = f\"neo4j+s://{NEO4J_HOST}\"\nNEO4J_JDBC_URL = f\"jdbc:neo4j+s://{NEO4J_HOST}:7687/{NEO4J_DATABASE}\"\nNEO4J_JDBC_URL_SQL = f\"{NEO4J_JDBC_URL}?enableSQLTranslation=true\"\n\nprint(\"Configuration loaded from Databricks Secrets:\")\nprint(f\"  Secret Scope: {SCOPE_NAME}\")\nprint(f\"  Neo4j Host: {NEO4J_HOST}\")\nprint(f\"  Bolt URI: {NEO4J_BOLT_URI}\")\nprint(f\"  JDBC URL: {NEO4J_JDBC_URL}\")\nprint(f\"  Connection Name: {UC_CONNECTION_NAME}\")\nprint(f\"  JDBC JAR Path: {JDBC_JAR_PATH}\")\nprint(f\"  Cleaner JAR Path: {CLEANER_JAR_PATH}\")\nprint(f\"  Java Dependencies: {JAVA_DEPENDENCIES}\")"
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Information\n",
    "\n",
    "Capture cluster and runtime details for support context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-info",
   "metadata": {},
   "outputs": [],
   "source": "# Collect environment information\nprint(\"=\" * 60)\nprint(\"ENVIRONMENT INFORMATION\")\nprint(\"=\" * 60)\n\n# Spark version\nprint(f\"\\nSpark Version: {spark.version}\")\n\n# Databricks Runtime\ntry:\n    dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")\n    print(f\"Databricks Runtime: {dbr_version}\")\nexcept:\n    print(\"Databricks Runtime: Unable to determine\")\n\n# Python version\nimport sys\nprint(f\"Python Version: {sys.version}\")\n\n# Check neo4j package\ntry:\n    import neo4j\n    print(f\"Neo4j Python Driver: {neo4j.__version__}\")\nexcept ImportError:\n    print(\"Neo4j Python Driver: NOT INSTALLED\")\n\n# Check JAR files exist\nprint(f\"\\nJDBC JAR Path: {JDBC_JAR_PATH}\")\nprint(f\"Cleaner JAR Path: {CLEANER_JAR_PATH}\")\ntry:\n    files = dbutils.fs.ls(JDBC_JAR_PATH.rsplit('/', 1)[0])\n    file_names = [f.name for f in files]\n    jdbc_jar_found = JDBC_JAR_PATH.split('/')[-1] in file_names\n    cleaner_jar_found = CLEANER_JAR_PATH.split('/')[-1] in file_names\n    print(f\"JDBC JAR File Exists: {jdbc_jar_found}\")\n    print(f\"Cleaner JAR File Exists: {cleaner_jar_found}\")\nexcept Exception as e:\n    print(f\"JAR File Check Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Network Connectivity Test (TCP Layer)\n",
    "\n",
    "**Expected Result**: PASS - Proves network path is open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-test",
   "metadata": {},
   "outputs": [],
   "source": "# TCP connectivity test using netcat\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Network Connectivity (TCP)\")\nprint(\"=\" * 60)\nprint(f\"\\nTarget: {NEO4J_HOST}:7687 (Bolt protocol port)\")\nprint(\"Testing: Can Databricks reach Neo4j at the network level?\")\n\nspark.sql(\"\"\"\nCREATE OR REPLACE TEMPORARY FUNCTION connectionTest(host STRING, port STRING)\nRETURNS STRING\nLANGUAGE PYTHON AS $$\nimport subprocess\nimport time\ntry:\n    start = time.time()\n    command = ['nc', '-zv', host, str(port)]\n    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=10)\n    elapsed = (time.time() - start) * 1000  # ms\n    output = result.stdout.decode() + result.stderr.decode()\n    if result.returncode == 0:\n        status = \"SUCCESS\"\n        message = f\"TCP connection established in {elapsed:.1f}ms\"\n    else:\n        status = \"FAILURE\"\n        message = f\"Cannot reach {host}:{port} - check firewall rules\"\n    return f\"{status}|{elapsed:.1f}|{message}|{output.strip()}\"\nexcept Exception as e:\n    return f\"FAILURE|0|Error: {str(e)}|\"\n$$\n\"\"\")\n\nstart_time = time.time()\nresult = spark.sql(f\"SELECT connectionTest('{NEO4J_HOST}', '7687') AS result\").collect()[0]['result']\ntotal_time = (time.time() - start_time) * 1000\n\nparts = result.split('|')\nstatus = parts[0]\nlatency = parts[1]\nmessage = parts[2]\ndetails = parts[3] if len(parts) > 3 else \"\"\n\nif status == \"SUCCESS\":\n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> CONNECTIVITY VERIFIED <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] {message}\")\n    print(f\"\\nConnection Details:\")\n    print(f\"  - Host: {NEO4J_HOST}\")\n    print(f\"  - Port: 7687 (Bolt)\")\n    print(f\"  - TCP Latency: {latency}ms\")\n    print(f\"  - Total Test Time: {total_time:.1f}ms\")\n    if details:\n        print(f\"  - Raw Output: {details}\")\n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Network path to Neo4j is OPEN\")\n    print(\"        Firewall rules allow Bolt protocol traffic\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\nelse:\n    print(f\"\\n[FAIL] {message}\")\n    print(f\"Details: {details}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "markdown",
   "id": "python-driver-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Neo4j Python Driver Test\n",
    "\n",
    "**Expected Result**: PASS - Proves credentials work and Neo4j is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "python-driver-test",
   "metadata": {},
   "outputs": [],
   "source": "# Test Neo4j Python driver connectivity\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Neo4j Python Driver\")\nprint(\"=\" * 60)\nprint(f\"\\nTarget: {NEO4J_BOLT_URI}\")\nprint(\"Testing: Can we authenticate and execute queries via Bolt protocol?\")\n\nfrom neo4j import GraphDatabase\n\ntry:\n    start_time = time.time()\n    driver = GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n    \n    # Verify connectivity\n    driver.verify_connectivity()\n    connect_time = (time.time() - start_time) * 1000\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> AUTHENTICATION SUCCESSFUL <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] Driver connected and authenticated in {connect_time:.1f}ms\")\n    \n    # Test simple query\n    with driver.session() as session:\n        query_start = time.time()\n        result = session.run(\"RETURN 1 AS test\")\n        record = result.single()\n        query_time = (time.time() - query_start) * 1000\n        print(f\"[PASS] Query executed: RETURN 1 = {record['test']} ({query_time:.1f}ms)\")\n        \n        # Get Neo4j version\n        result = session.run(\"CALL dbms.components() YIELD name, versions RETURN name, versions\")\n        neo4j_info = []\n        for record in result:\n            neo4j_info.append(f\"{record['name']} {record['versions']}\")\n    \n    total_time = (time.time() - start_time) * 1000\n    driver.close()\n    \n    print(f\"\\nConnection Details:\")\n    print(f\"  - URI: {NEO4J_BOLT_URI}\")\n    print(f\"  - User: {NEO4J_USER}\")\n    print(f\"  - Database: {NEO4J_DATABASE}\")\n    print(f\"  - Neo4j Server: {', '.join(neo4j_info)}\")\n    print(f\"  - Connection Time: {connect_time:.1f}ms\")\n    print(f\"  - Total Test Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Neo4j Python Driver connection WORKING\")\n    print(\"        Credentials valid, Bolt protocol functional\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Connection failed: {e}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "markdown",
   "id": "spark-connector-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Neo4j Spark Connector (Working Baseline)\n",
    "\n",
    "**Expected Result**: PASS - This is our working baseline that proves Spark can communicate with Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark-connector-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Neo4j Spark Connector (known working method)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Neo4j Spark Connector (org.neo4j.spark.DataSource)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .option(\"url\", NEO4J_BOLT_URI) \\\n",
    "        .option(\"authentication.type\", \"basic\") \\\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"RETURN 'Spark Connector Works!' AS message, 1 AS value\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Spark Connector query executed successfully:\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Spark Connector failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-jdbc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Direct JDBC Tests (Bypassing Unity Catalog)\n",
    "\n",
    "These tests use the Neo4j JDBC driver directly with Spark, **without** Unity Catalog's SafeSpark wrapper.\n",
    "\n",
    "**Note**: Requires the JDBC JAR to be installed as a cluster library (not just in a UC Volume).\n",
    "\n",
    "**Limitation Discovered**: Spark's JDBC driver wraps `query` option queries in a subquery for schema inference:\n",
    "```sql\n",
    "SELECT * FROM (your_query) SPARK_GEN_SUBQ_N WHERE 1=0\n",
    "```\n",
    "This breaks native Cypher even with `FORCE_CYPHER` hint (hint is inside subquery, outer wrapper is still SQL).\n",
    "\n",
    "**Schema Inference Issue**: When using `dbtable` option, Spark's schema inference returns `NullType()` for all columns from Neo4j JDBC. This causes `No column has been read prior to this call` error when reading data. **Fix**: Use `customSchema` option to explicitly specify column types.\n",
    "\n",
    "**Workarounds**:\n",
    "1. Use `dbtable` option with `customSchema` (required to avoid NullType inference)\n",
    "2. Use `query` option with `customSchema` for SQL queries\n",
    "3. Use Neo4j Spark Connector instead of JDBC (Section 4 - works without customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-jdbc-no-sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct JDBC - Using dbtable (reads Neo4j label as table, no subquery wrapping)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Direct JDBC - dbtable option (reads label as table)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {NEO4J_JDBC_URL_SQL}\")\n",
    "\n",
    "# Use dbtable to read a Neo4j label directly (no subquery wrapper)\n",
    "# Replace 'Aircraft' with any label that exists in your Neo4j database\n",
    "TEST_LABEL = \"Aircraft\"  # Change this to a label in your database\n",
    "\n",
    "# IMPORTANT: customSchema is REQUIRED when using dbtable with Neo4j JDBC\n",
    "# Without it, Spark schema inference returns NullType() for all columns,\n",
    "# causing \"No column has been read prior to this call\" error when reading data.\n",
    "# Adjust column names and types to match your actual Neo4j node properties.\n",
    "# NOTE: Use backticks around column names with special characters (like $)\n",
    "AIRCRAFT_SCHEMA = \"`v$id` STRING, aircraft_id STRING, tail_number STRING, icao24 STRING, model STRING, operator STRING, manufacturer STRING\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"dbtable\", TEST_LABEL) \\\n",
    "        .option(\"customSchema\", AIRCRAFT_SCHEMA) \\\n",
    "        .load()\n",
    "    \n",
    "    print(f\"\\n[PASS] Direct JDBC dbtable '{TEST_LABEL}' read successfully:\")\n",
    "    print(f\"Schema: {df.schema}\")\n",
    "    df.show(5, truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Direct JDBC dbtable failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")\n",
    "    print(\"\\nNote: Ensure the label exists in Neo4j and JAR is installed as cluster library.\")\n",
    "    print(\"Also verify customSchema column names match your Neo4j node properties.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-jdbc-with-sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct JDBC - SQL Translation (SQL automatically converted to Cypher)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Direct JDBC - SQL Translation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {NEO4J_JDBC_URL_SQL}\")\n",
    "\n",
    "# Use customSchema to bypass Spark's schema inference\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"SELECT 1 AS value\") \\\n",
    "        .option(\"customSchema\", \"value INT\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Direct JDBC (SQL translation) query executed:\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Direct JDBC with SQL translation failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ocgnv7ybdy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct JDBC - SQL Aggregate Query (COUNT)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Direct JDBC - SQL Aggregate (COUNT)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {NEO4J_JDBC_URL_SQL}\")\n",
    "\n",
    "# Aggregate functions work reliably with SQL translation\n",
    "# SQL: SELECT COUNT(*) AS flight_count FROM Flight\n",
    "# Cypher: MATCH (n:Flight) RETURN count(n) AS flight_count\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"SELECT COUNT(*) AS flight_count FROM Flight\") \\\n",
    "        .option(\"customSchema\", \"flight_count LONG\") \\\n",
    "        .load()\n",
    "\n",
    "    print(\"\\n[PASS] Direct JDBC SQL aggregate query executed:\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Direct JDBC aggregate query failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")\n",
    "    print(\"\\nNote: Ensure 'Flight' label exists in your Neo4j database, or change to a label that exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40s9rk0dug",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct JDBC - SQL JOIN Translation (NATURAL JOIN -> Cypher relationship)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Direct JDBC - SQL JOIN Translation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {NEO4J_JDBC_URL_SQL}\")\n",
    "\n",
    "# Neo4j JDBC translates SQL JOINs to Cypher relationship patterns:\n",
    "# SQL:    SELECT COUNT(*) FROM Flight f NATURAL JOIN DEPARTS_FROM r NATURAL JOIN Airport a\n",
    "# Cypher: MATCH (f:Flight)-[:DEPARTS_FROM]->(a:Airport) RETURN count(*) AS cnt\n",
    "#\n",
    "# See: https://neo4j.com/docs/jdbc-manual/current/sql2cypher/\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"\"\"SELECT COUNT(*) AS cnt\n",
    "                           FROM Flight f\n",
    "                           NATURAL JOIN DEPARTS_FROM r\n",
    "                           NATURAL JOIN Airport a\"\"\") \\\n",
    "        .option(\"customSchema\", \"cnt LONG\") \\\n",
    "        .load()\n",
    "\n",
    "    print(\"\\n[PASS] Direct JDBC SQL JOIN translation executed:\")\n",
    "    print(\"SQL JOINs translated to Cypher relationship pattern!\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Direct JDBC JOIN translation failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")\n",
    "    print(\"\\nNote: Requires Flight-[:DEPARTS_FROM]->Airport pattern in Neo4j.\")\n",
    "    print(\"Adjust labels/relationship types to match your graph model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uc-jdbc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Unity Catalog JDBC Connection\n",
    "\n",
    "This section creates and tests the Unity Catalog JDBC connection, which uses the SafeSpark wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-create-connection",
   "metadata": {},
   "outputs": [],
   "source": "# Create Unity Catalog JDBC Connection\nprint(\"=\" * 60)\nprint(\"SETUP: Create Unity Catalog JDBC Connection\")\nprint(\"=\" * 60)\n\n# Drop existing connection\nspark.sql(f\"DROP CONNECTION IF EXISTS {UC_CONNECTION_NAME}\")\nprint(f\"Dropped existing connection (if any): {UC_CONNECTION_NAME}\")\n\n# Create connection with explicit driver class\n# NOTE: customSchema must be in externalOptionsAllowList to bypass Spark schema inference\n# NOTE: java_dependencies includes both the full bundle and spark cleaner JARs\ncreate_sql = f\"\"\"\nCREATE CONNECTION {UC_CONNECTION_NAME} TYPE JDBC\nENVIRONMENT (\n  java_dependencies '{JAVA_DEPENDENCIES}'\n)\nOPTIONS (\n  url '{NEO4J_JDBC_URL_SQL}',\n  user '{NEO4J_USER}',\n  password '{NEO4J_PASSWORD}',\n  driver 'org.neo4j.jdbc.Neo4jDriver',\n  externalOptionsAllowList 'dbtable,query,partitionColumn,lowerBound,upperBound,numPartitions,fetchSize,customSchema'\n)\n\"\"\"\n\nprint(f\"\\n[INFO] java_dependencies: {JAVA_DEPENDENCIES}\")\n\ntry:\n    spark.sql(create_sql)\n    print(f\"\\n[PASS] Connection created: {UC_CONNECTION_NAME}\")\nexcept Exception as e:\n    print(f\"\\n[FAIL] Failed to create connection: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-describe-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFY: Connection Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.sql(f\"DESCRIBE CONNECTION {UC_CONNECTION_NAME}\")\n",
    "    print(\"\\nConnection details:\")\n",
    "    df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Cannot describe connection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uc-test-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Unity Catalog JDBC Tests\n",
    "\n",
    "These tests use the Unity Catalog connection through the SafeSpark JDBC wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-spark-dataframe-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test UC Connection via Spark DataFrame API\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Unity Catalog - Spark DataFrame API\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"query\", \"SELECT 1 AS test\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Unity Catalog Spark DataFrame API:\")\n",
    "    df.show()\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Unity Catalog Spark DataFrame API failed:\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-spark-cypher-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test UC Connection with native Cypher (FORCE_CYPHER hint)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Unity Catalog - Native Cypher (FORCE_CYPHER)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NOTE: Spark wraps query option in subquery for schema inference:\n",
    "#   SELECT * FROM (your_query) SPARK_GEN_SUBQ_N WHERE 1=0\n",
    "# This breaks native Cypher. Use customSchema to bypass schema inference.\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"query\", \"/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test\") \\\n",
    "        .option(\"customSchema\", \"test INT\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Unity Catalog with FORCE_CYPHER:\")\n",
    "    df.show()\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Unity Catalog with FORCE_CYPHER failed:\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-remote-query-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test UC Connection via remote_query() function\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Unity Catalog - remote_query() Function\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.sql(f\"\"\"\n",
    "        SELECT * FROM remote_query(\n",
    "            '{UC_CONNECTION_NAME}',\n",
    "            query => 'SELECT 1 AS test'\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n[PASS] Unity Catalog remote_query():\")\n",
    "    df.show()\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Unity Catalog remote_query() failed:\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ulxi8xl1gm",
   "source": [
    "# Test UC Connection with SQL Aggregate Query using Custom Schema\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Unity Catalog - SQL Aggregate with Custom Schema\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CustomSchema for Neo4j JDBC\n",
    "# ============================================\n",
    "# Spark's automatic schema inference wraps queries in a subquery:\n",
    "#   SELECT * FROM (your_query) SPARK_GEN_SUBQ WHERE 1=0\n",
    "# Neo4j JDBC returns NullType() for all columns during inference,\n",
    "# causing \"No column has been read\" errors when reading data.\n",
    "#\n",
    "# Possible Workaround: Use customSchema to explicitly define column types:\n",
    "# - Column names MUST match query result aliases exactly\n",
    "# - Use Spark SQL types: STRING, LONG, INT, DOUBLE, BOOLEAN, DECIMAL(p,s), etc.\n",
    "# - Partial schemas allowed: unspecified columns use default inference\n",
    "#\n",
    "# This also failed to work\n",
    "#\n",
    "# Reference: https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n",
    "\n",
    "# Define schema for aggregate query result\n",
    "FLIGHT_COUNT_SCHEMA = \"flight_count LONG\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"query\", \"SELECT COUNT(*) AS flight_count FROM Flight\") \\\n",
    "        .option(\"customSchema\", FLIGHT_COUNT_SCHEMA) \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Unity Catalog SQL Aggregate with customSchema:\")\n",
    "    print(f\"Schema applied: {FLIGHT_COUNT_SCHEMA}\")\n",
    "    print(f\"DataFrame schema: {df.schema}\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Unity Catalog SQL aggregate query failed:\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")\n",
    "    print(\"\\nNote: Ensure 'Flight' label exists in your Neo4j database.\")\n",
    "    print(\"Adjust the label name to match your graph model if needed.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}