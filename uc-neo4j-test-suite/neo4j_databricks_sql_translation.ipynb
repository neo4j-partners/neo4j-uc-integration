{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-issue-summary",
   "metadata": {},
   "source": [
    "# Neo4j JDBC Unity Catalog Connection - Support Ticket\n",
    "\n",
    "## Issue Summary\n",
    "\n",
    "**Problem**: Unity Catalog JDBC connection to Neo4j fails with `Connection was closed before the operation completed` error, despite:\n",
    "- Network connectivity working (TCP test passes)\n",
    "- Neo4j Python driver working\n",
    "- Neo4j Spark Connector working\n",
    "\n",
    "**Error Location**: `com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected`\n",
    "\n",
    "This notebook provides a systematic test progression to isolate the failure point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration\n",
    "\n",
    "**Prerequisites**: Run `setup.sh` to configure Databricks secrets before running this notebook.\n",
    "\n",
    "The setup script reads credentials from `.env` and stores them in the `neo4j-uc-creds` secret scope:\n",
    "- `host` - Neo4j host\n",
    "- `user` - Neo4j username\n",
    "- `password` - Neo4j password\n",
    "- `connection_name` - Unity Catalog connection name\n",
    "- `jdbc_jar_path` - Path to JDBC JAR in UC Volume\n",
    "- `database` - Neo4j database (optional, defaults to \"neo4j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Loaded from Databricks Secrets\n",
    "# =============================================================================\n",
    "# Secrets are configured using setup.sh which creates scope \"neo4j-uc-creds\"\n",
    "# with secrets: host, user, password, connection_name, jdbc_jar_path, database\n",
    "\n",
    "SCOPE_NAME = \"neo4j-uc-creds\"\n",
    "\n",
    "# Aura Connection Details (from secrets)\n",
    "NEO4J_HOST = dbutils.secrets.get(SCOPE_NAME, \"host\")\n",
    "NEO4J_USER = dbutils.secrets.get(SCOPE_NAME, \"user\")\n",
    "NEO4J_PASSWORD = dbutils.secrets.get(SCOPE_NAME, \"password\")\n",
    "\n",
    "# Database defaults to \"neo4j\" if not set\n",
    "try:\n",
    "    NEO4J_DATABASE = dbutils.secrets.get(SCOPE_NAME, \"database\")\n",
    "except:\n",
    "    NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# Unity Catalog Resources (from secrets)\n",
    "JDBC_JAR_PATH = dbutils.secrets.get(SCOPE_NAME, \"jdbc_jar_path\")\n",
    "UC_CONNECTION_NAME = dbutils.secrets.get(SCOPE_NAME, \"connection_name\")\n",
    "\n",
    "# Derived URLs (no need to edit)\n",
    "NEO4J_BOLT_URI = f\"neo4j+s://{NEO4J_HOST}\"\n",
    "NEO4J_JDBC_URL = f\"jdbc:neo4j+s://{NEO4J_HOST}:7687/{NEO4J_DATABASE}\"\n",
    "NEO4J_JDBC_URL_SQL = f\"{NEO4J_JDBC_URL}?enableSQLTranslation=true\"\n",
    "\n",
    "print(\"Configuration loaded from Databricks Secrets:\")\n",
    "print(f\"  Secret Scope: {SCOPE_NAME}\")\n",
    "print(f\"  Neo4j Host: {NEO4J_HOST}\")\n",
    "print(f\"  Bolt URI: {NEO4J_BOLT_URI}\")\n",
    "print(f\"  JDBC URL: {NEO4J_JDBC_URL}\")\n",
    "print(f\"  Connection Name: {UC_CONNECTION_NAME}\")\n",
    "print(f\"  JAR Path: {JDBC_JAR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Information\n",
    "\n",
    "Capture cluster and runtime details for support context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect environment information\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Spark version\n",
    "print(f\"\\nSpark Version: {spark.version}\")\n",
    "\n",
    "# Databricks Runtime\n",
    "try:\n",
    "    dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")\n",
    "    print(f\"Databricks Runtime: {dbr_version}\")\n",
    "except:\n",
    "    print(\"Databricks Runtime: Unable to determine\")\n",
    "\n",
    "# Python version\n",
    "import sys\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "# Check neo4j package\n",
    "try:\n",
    "    import neo4j\n",
    "    print(f\"Neo4j Python Driver: {neo4j.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Neo4j Python Driver: NOT INSTALLED\")\n",
    "\n",
    "# Check JAR file exists\n",
    "print(f\"\\nJDBC JAR Path: {JDBC_JAR_PATH}\")\n",
    "try:\n",
    "    files = dbutils.fs.ls(JDBC_JAR_PATH.rsplit('/', 1)[0])\n",
    "    jar_found = any(JDBC_JAR_PATH.split('/')[-1] in f.name for f in files)\n",
    "    print(f\"JAR File Exists: {jar_found}\")\n",
    "except Exception as e:\n",
    "    print(f\"JAR File Check Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Network Connectivity Test (TCP Layer)\n",
    "\n",
    "**Expected Result**: PASS - Proves network path is open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCP connectivity test using netcat\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Network Connectivity (TCP)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY FUNCTION connectionTest(host STRING, port STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON AS $$\n",
    "import subprocess\n",
    "try:\n",
    "    command = ['nc', '-zv', host, str(port)]\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    output = result.stdout.decode() + result.stderr.decode()\n",
    "    if result.returncode == 0:\n",
    "        status = \"SUCCESS\"\n",
    "        message = f\"Network connectivity to {host}:{port} is OPEN\"\n",
    "    else:\n",
    "        status = \"FAILURE\"\n",
    "        message = f\"Cannot reach {host}:{port} - check firewall rules\"\n",
    "    return f\"{status} (return_code={result.returncode}) | {message} | Details: {output.strip()}\"\n",
    "except Exception as e:\n",
    "    return f\"FAILURE (exception) | Error: {str(e)}\"\n",
    "$$\n",
    "\"\"\")\n",
    "\n",
    "result = spark.sql(f\"SELECT connectionTest('{NEO4J_HOST}', '7687') AS result\").collect()[0]['result']\n",
    "print(f\"\\nResult: {result}\")\n",
    "print(f\"\\nStatus: {'PASS' if 'SUCCESS' in result else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python-driver-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Neo4j Python Driver Test\n",
    "\n",
    "**Expected Result**: PASS - Proves credentials work and Neo4j is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "python-driver-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Neo4j Python driver connectivity\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Neo4j Python Driver\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    \n",
    "    # Verify connectivity\n",
    "    driver.verify_connectivity()\n",
    "    print(\"\\n[PASS] Driver connectivity verified\")\n",
    "    \n",
    "    # Test simple query\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"RETURN 1 AS test\")\n",
    "        record = result.single()\n",
    "        print(f\"[PASS] Query executed: RETURN 1 = {record['test']}\")\n",
    "        \n",
    "        # Get Neo4j version\n",
    "        result = session.run(\"CALL dbms.components() YIELD name, versions RETURN name, versions\")\n",
    "        for record in result:\n",
    "            print(f\"[INFO] Connected to: {record['name']} {record['versions']}\")\n",
    "    \n",
    "    driver.close()\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Connection failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark-connector-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Neo4j Spark Connector (Working Baseline)\n",
    "\n",
    "**Expected Result**: PASS - This is our working baseline that proves Spark can communicate with Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark-connector-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Neo4j Spark Connector (known working method)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Neo4j Spark Connector (org.neo4j.spark.DataSource)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .option(\"url\", NEO4J_BOLT_URI) \\\n",
    "        .option(\"authentication.type\", \"basic\") \\\n",
    "        .option(\"authentication.basic.username\", NEO4J_USER) \\\n",
    "        .option(\"authentication.basic.password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"RETURN 'Spark Connector Works!' AS message, 1 AS value\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Spark Connector query executed successfully:\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Spark Connector failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-jdbc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Direct JDBC Tests (Bypassing Unity Catalog)\n",
    "\n",
    "These tests use the Neo4j JDBC driver directly with Spark, **without** Unity Catalog's SafeSpark wrapper.\n",
    "\n",
    "**Note**: Requires the JDBC JAR to be installed as a cluster library (not just in a UC Volume).\n",
    "\n",
    "**Limitation Discovered**: Spark's JDBC driver wraps `query` option queries in a subquery for schema inference:\n",
    "```sql\n",
    "SELECT * FROM (your_query) SPARK_GEN_SUBQ_N WHERE 1=0\n",
    "```\n",
    "This breaks native Cypher even with `FORCE_CYPHER` hint (hint is inside subquery, outer wrapper is still SQL).\n",
    "\n",
    "**Schema Inference Issue**: When using `dbtable` option, Spark's schema inference returns `NullType()` for all columns from Neo4j JDBC. This causes `No column has been read prior to this call` error when reading data. **Fix**: Use `customSchema` option to explicitly specify column types.\n",
    "\n",
    "**Workarounds**:\n",
    "1. Use `dbtable` option with `customSchema` (required to avoid NullType inference)\n",
    "2. Use `query` option with `customSchema` for SQL queries\n",
    "3. Use Neo4j Spark Connector instead of JDBC (Section 4 - works without customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-jdbc-no-sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct JDBC - Using dbtable (reads Neo4j label as table, no subquery wrapping)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Direct JDBC - dbtable option (reads label as table)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {NEO4J_JDBC_URL_SQL}\")\n",
    "\n",
    "# Use dbtable to read a Neo4j label directly (no subquery wrapper)\n",
    "# Replace 'Aircraft' with any label that exists in your Neo4j database\n",
    "TEST_LABEL = \"Aircraft\"  # Change this to a label in your database\n",
    "\n",
    "# IMPORTANT: customSchema is REQUIRED when using dbtable with Neo4j JDBC\n",
    "# Without it, Spark schema inference returns NullType() for all columns,\n",
    "# causing \"No column has been read prior to this call\" error when reading data.\n",
    "# Adjust column names and types to match your actual Neo4j node properties.\n",
    "# NOTE: Use backticks around column names with special characters (like $)\n",
    "AIRCRAFT_SCHEMA = \"`v$id` STRING, aircraft_id STRING, tail_number STRING, icao24 STRING, model STRING, operator STRING, manufacturer STRING\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"dbtable\", TEST_LABEL) \\\n",
    "        .option(\"customSchema\", AIRCRAFT_SCHEMA) \\\n",
    "        .load()\n",
    "    \n",
    "    print(f\"\\n[PASS] Direct JDBC dbtable '{TEST_LABEL}' read successfully:\")\n",
    "    print(f\"Schema: {df.schema}\")\n",
    "    df.show(5, truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Direct JDBC dbtable failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")\n",
    "    print(\"\\nNote: Ensure the label exists in Neo4j and JAR is installed as cluster library.\")\n",
    "    print(\"Also verify customSchema column names match your Neo4j node properties.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-jdbc-with-sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct JDBC - SQL Translation (SQL automatically converted to Cypher)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Direct JDBC - SQL Translation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {NEO4J_JDBC_URL_SQL}\")\n",
    "\n",
    "# Use customSchema to bypass Spark's schema inference\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"SELECT 1 AS value\") \\\n",
    "        .option(\"customSchema\", \"value INT\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Direct JDBC (SQL translation) query executed:\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Direct JDBC with SQL translation failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ocgnv7ybdy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct JDBC - SQL Aggregate Query (COUNT)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Direct JDBC - SQL Aggregate (COUNT)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {NEO4J_JDBC_URL_SQL}\")\n",
    "\n",
    "# Aggregate functions work reliably with SQL translation\n",
    "# SQL: SELECT COUNT(*) AS flight_count FROM Flight\n",
    "# Cypher: MATCH (n:Flight) RETURN count(n) AS flight_count\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"SELECT COUNT(*) AS flight_count FROM Flight\") \\\n",
    "        .option(\"customSchema\", \"flight_count LONG\") \\\n",
    "        .load()\n",
    "\n",
    "    print(\"\\n[PASS] Direct JDBC SQL aggregate query executed:\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Direct JDBC aggregate query failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")\n",
    "    print(\"\\nNote: Ensure 'Flight' label exists in your Neo4j database, or change to a label that exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40s9rk0dug",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct JDBC - SQL JOIN Translation (NATURAL JOIN -> Cypher relationship)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Direct JDBC - SQL JOIN Translation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"URL: {NEO4J_JDBC_URL_SQL}\")\n",
    "\n",
    "# Neo4j JDBC translates SQL JOINs to Cypher relationship patterns:\n",
    "# SQL:    SELECT COUNT(*) FROM Flight f NATURAL JOIN DEPARTS_FROM r NATURAL JOIN Airport a\n",
    "# Cypher: MATCH (f:Flight)-[:DEPARTS_FROM]->(a:Airport) RETURN count(*) AS cnt\n",
    "#\n",
    "# See: https://neo4j.com/docs/jdbc-manual/current/sql2cypher/\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"\"\"SELECT COUNT(*) AS cnt\n",
    "                           FROM Flight f\n",
    "                           NATURAL JOIN DEPARTS_FROM r\n",
    "                           NATURAL JOIN Airport a\"\"\") \\\n",
    "        .option(\"customSchema\", \"cnt LONG\") \\\n",
    "        .load()\n",
    "\n",
    "    print(\"\\n[PASS] Direct JDBC SQL JOIN translation executed:\")\n",
    "    print(\"SQL JOINs translated to Cypher relationship pattern!\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Direct JDBC JOIN translation failed: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")\n",
    "    print(\"\\nNote: Requires Flight-[:DEPARTS_FROM]->Airport pattern in Neo4j.\")\n",
    "    print(\"Adjust labels/relationship types to match your graph model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uc-jdbc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Unity Catalog JDBC Connection\n",
    "\n",
    "This section creates and tests the Unity Catalog JDBC connection, which uses the SafeSpark wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-create-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Unity Catalog JDBC Connection\n",
    "print(\"=\" * 60)\n",
    "print(\"SETUP: Create Unity Catalog JDBC Connection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Drop existing connection\n",
    "spark.sql(f\"DROP CONNECTION IF EXISTS {UC_CONNECTION_NAME}\")\n",
    "print(f\"Dropped existing connection (if any): {UC_CONNECTION_NAME}\")\n",
    "\n",
    "# Create connection with explicit driver class\n",
    "# NOTE: customSchema must be in externalOptionsAllowList to bypass Spark schema inference\n",
    "create_sql = f\"\"\"\n",
    "CREATE CONNECTION {UC_CONNECTION_NAME} TYPE JDBC\n",
    "ENVIRONMENT (\n",
    "  java_dependencies '[\"{JDBC_JAR_PATH}\"]'\n",
    ")\n",
    "OPTIONS (\n",
    "  url '{NEO4J_JDBC_URL_SQL}',\n",
    "  user '{NEO4J_USER}',\n",
    "  password '{NEO4J_PASSWORD}',\n",
    "  driver 'org.neo4j.jdbc.Neo4jDriver',\n",
    "  externalOptionsAllowList 'dbtable,query,partitionColumn,lowerBound,upperBound,numPartitions,fetchSize,customSchema'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(create_sql)\n",
    "    print(f\"\\n[PASS] Connection created: {UC_CONNECTION_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Failed to create connection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-describe-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFY: Connection Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.sql(f\"DESCRIBE CONNECTION {UC_CONNECTION_NAME}\")\n",
    "    print(\"\\nConnection details:\")\n",
    "    df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Cannot describe connection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uc-test-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Unity Catalog JDBC Tests\n",
    "\n",
    "These tests use the Unity Catalog connection through the SafeSpark JDBC wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-spark-dataframe-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test UC Connection via Spark DataFrame API\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Unity Catalog - Spark DataFrame API\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"query\", \"SELECT 1 AS test\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Unity Catalog Spark DataFrame API:\")\n",
    "    df.show()\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Unity Catalog Spark DataFrame API failed:\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-spark-cypher-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test UC Connection with native Cypher (FORCE_CYPHER hint)\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Unity Catalog - Native Cypher (FORCE_CYPHER)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NOTE: Spark wraps query option in subquery for schema inference:\n",
    "#   SELECT * FROM (your_query) SPARK_GEN_SUBQ_N WHERE 1=0\n",
    "# This breaks native Cypher. Use customSchema to bypass schema inference.\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"query\", \"/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test\") \\\n",
    "        .option(\"customSchema\", \"test INT\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Unity Catalog with FORCE_CYPHER:\")\n",
    "    df.show()\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Unity Catalog with FORCE_CYPHER failed:\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-remote-query-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test UC Connection via remote_query() function\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Unity Catalog - remote_query() Function\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.sql(f\"\"\"\n",
    "        SELECT * FROM remote_query(\n",
    "            '{UC_CONNECTION_NAME}',\n",
    "            query => 'SELECT 1 AS test'\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n[PASS] Unity Catalog remote_query():\")\n",
    "    df.show()\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Unity Catalog remote_query() failed:\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ulxi8xl1gm",
   "source": [
    "# Test UC Connection with SQL Aggregate Query using Custom Schema\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Unity Catalog - SQL Aggregate with Custom Schema\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CustomSchema for Neo4j JDBC\n",
    "# ============================================\n",
    "# Spark's automatic schema inference wraps queries in a subquery:\n",
    "#   SELECT * FROM (your_query) SPARK_GEN_SUBQ WHERE 1=0\n",
    "# Neo4j JDBC returns NullType() for all columns during inference,\n",
    "# causing \"No column has been read\" errors when reading data.\n",
    "#\n",
    "# Possible Workaround: Use customSchema to explicitly define column types:\n",
    "# - Column names MUST match query result aliases exactly\n",
    "# - Use Spark SQL types: STRING, LONG, INT, DOUBLE, BOOLEAN, DECIMAL(p,s), etc.\n",
    "# - Partial schemas allowed: unspecified columns use default inference\n",
    "#\n",
    "# This also failed to work\n",
    "#\n",
    "# Reference: https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n",
    "\n",
    "# Define schema for aggregate query result\n",
    "FLIGHT_COUNT_SCHEMA = \"flight_count LONG\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"query\", \"SELECT COUNT(*) AS flight_count FROM Flight\") \\\n",
    "        .option(\"customSchema\", FLIGHT_COUNT_SCHEMA) \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"\\n[PASS] Unity Catalog SQL Aggregate with customSchema:\")\n",
    "    print(f\"Schema applied: {FLIGHT_COUNT_SCHEMA}\")\n",
    "    print(f\"DataFrame schema: {df.schema}\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nStatus: PASS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Unity Catalog SQL aggregate query failed:\")\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nStatus: FAIL\")\n",
    "    print(\"\\nNote: Ensure 'Flight' label exists in your Neo4j database.\")\n",
    "    print(\"Adjust the label name to match your graph model if needed.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "section8-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Neo4j Schema Synchronization with Unity Catalog (Proof of Concept)\n",
    "\n",
    "**Purpose**: Demonstrate how Neo4j graph schema can be discovered via JDBC DatabaseMetaData and mapped to Unity Catalog objects.\n",
    "\n",
    "**Approach**:\n",
    "1. **Phase 1**: Discover Neo4j schema using JDBC `DatabaseMetaData` API via `spark._jvm`\n",
    "2. **Phase 2**: Test three approaches for creating Unity Catalog objects from discovered schema\n",
    "3. **Phase 3**: Verify queries work through UC governance\n",
    "\n",
    "**Key Insight**: Unity Catalog Foreign Catalogs only support specific databases (PostgreSQL, MySQL, etc.) - not generic JDBC. Therefore, we must manually create UC views/tables backed by the JDBC connection.\n",
    "\n",
    "**Options Tested**:\n",
    "- **Option A**: Views with inferred schema (schema discovered at query time)\n",
    "- **Option B**: Tables with explicit schema (schema from in-memory metadata)\n",
    "- **Option C**: Hybrid approach (schema registry table + views)\n",
    "\n",
    "**Reference**: \n",
    "- [Neo4j JDBC Manual](https://neo4j.com/docs/jdbc-manual/current/)\n",
    "- [META.md](../META.md) - Full proposal document"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-schema-discovery",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: Schema Discovery via JDBC DatabaseMetaData\n",
    "# =============================================================================\n",
    "# Use spark._jvm (Py4J gateway) to access JDBC DatabaseMetaData directly.\n",
    "# This discovers all labels, properties, and relationships without hardcoding.\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: JDBC DatabaseMetaData Schema Discovery\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def get_neo4j_schema_via_jdbc(spark, jdbc_url, user, password):\n",
    "    \"\"\"\n",
    "    Discover Neo4j schema using JDBC DatabaseMetaData API.\n",
    "    \n",
    "    Returns dict with 'labels' and 'relationships' containing full schema info.\n",
    "    \"\"\"\n",
    "    schema = {\"labels\": {}, \"relationships\": []}\n",
    "    \n",
    "    # Access JVM gateway\n",
    "    jvm = spark._jvm\n",
    "    gateway = spark._sc._gateway\n",
    "    \n",
    "    # Create JDBC connection\n",
    "    props = jvm.java.util.Properties()\n",
    "    props.setProperty(\"user\", user)\n",
    "    props.setProperty(\"password\", password)\n",
    "    \n",
    "    connection = None\n",
    "    try:\n",
    "        connection = jvm.java.sql.DriverManager.getConnection(jdbc_url, props)\n",
    "        metadata = connection.getMetaData()\n",
    "        \n",
    "        print(f\"\\n[INFO] Connected to: {metadata.getDatabaseProductName()} {metadata.getDatabaseProductVersion()}\")\n",
    "        print(f\"[INFO] JDBC Driver: {metadata.getDriverName()} {metadata.getDriverVersion()}\")\n",
    "        \n",
    "        # --- Discover Labels (Tables) ---\n",
    "        print(\"\\n[INFO] Discovering node labels via getTables(TABLE)...\")\n",
    "        types_array = gateway.new_array(gateway.jvm.java.lang.String, 1)\n",
    "        types_array[0] = \"TABLE\"\n",
    "        \n",
    "        rs = metadata.getTables(None, None, None, types_array)\n",
    "        while rs.next():\n",
    "            label_name = rs.getString(\"TABLE_NAME\")\n",
    "            schema[\"labels\"][label_name] = {\"columns\": [], \"primary_key\": None}\n",
    "        rs.close()\n",
    "        \n",
    "        print(f\"[INFO] Found {len(schema['labels'])} labels\")\n",
    "        \n",
    "        # --- Discover Columns for each Label ---\n",
    "        print(\"\\n[INFO] Discovering properties via getColumns()...\")\n",
    "        for label_name in schema[\"labels\"]:\n",
    "            rs = metadata.getColumns(None, None, label_name, None)\n",
    "            while rs.next():\n",
    "                col_info = {\n",
    "                    \"name\": rs.getString(\"COLUMN_NAME\"),\n",
    "                    \"type_name\": rs.getString(\"TYPE_NAME\"),\n",
    "                    \"sql_type\": rs.getInt(\"DATA_TYPE\"),\n",
    "                    \"nullable\": rs.getString(\"IS_NULLABLE\") == \"YES\",\n",
    "                    \"is_generated\": rs.getString(\"IS_GENERATEDCOLUMN\") == \"YES\"\n",
    "                }\n",
    "                schema[\"labels\"][label_name][\"columns\"].append(col_info)\n",
    "            rs.close()\n",
    "            \n",
    "            # Get primary key\n",
    "            rs = metadata.getPrimaryKeys(None, None, label_name)\n",
    "            while rs.next():\n",
    "                schema[\"labels\"][label_name][\"primary_key\"] = rs.getString(\"COLUMN_NAME\")\n",
    "            rs.close()\n",
    "        \n",
    "        # --- Discover Relationships ---\n",
    "        print(\"\\n[INFO] Discovering relationships via getTables(RELATIONSHIP)...\")\n",
    "        types_array[0] = \"RELATIONSHIP\"\n",
    "        rs = metadata.getTables(None, None, None, types_array)\n",
    "        while rs.next():\n",
    "            rel_name = rs.getString(\"TABLE_NAME\")\n",
    "            remarks = rs.getString(\"REMARKS\") or \"\"\n",
    "            # Parse remarks to get from/to labels (format: \"FromLabel\\nREL_TYPE\\nToLabel\")\n",
    "            parts = remarks.split(\"\\n\") if remarks else []\n",
    "            if len(parts) >= 3:\n",
    "                schema[\"relationships\"].append({\n",
    "                    \"from_label\": parts[0],\n",
    "                    \"type\": parts[1],\n",
    "                    \"to_label\": parts[2],\n",
    "                    \"table_name\": rel_name\n",
    "                })\n",
    "            else:\n",
    "                schema[\"relationships\"].append({\n",
    "                    \"from_label\": None,\n",
    "                    \"type\": rel_name,\n",
    "                    \"to_label\": None,\n",
    "                    \"table_name\": rel_name\n",
    "                })\n",
    "        rs.close()\n",
    "        \n",
    "        print(f\"[INFO] Found {len(schema['relationships'])} relationship patterns\")\n",
    "        \n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "    \n",
    "    return schema\n",
    "\n",
    "# Execute schema discovery\n",
    "try:\n",
    "    # First, trigger driver loading with a minimal query\n",
    "    _ = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n",
    "        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n",
    "        .option(\"user\", NEO4J_USER) \\\n",
    "        .option(\"password\", NEO4J_PASSWORD) \\\n",
    "        .option(\"query\", \"SELECT 1\") \\\n",
    "        .option(\"customSchema\", \"result INT\") \\\n",
    "        .load().take(1)\n",
    "    \n",
    "    # Now discover schema\n",
    "    NEO4J_SCHEMA = get_neo4j_schema_via_jdbc(spark, NEO4J_JDBC_URL_SQL, NEO4J_USER, NEO4J_PASSWORD)\n",
    "    print(\"\\n[PASS] Schema discovery completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Schema discovery failed: {e}\")\n",
    "    NEO4J_SCHEMA = {\"labels\": {}, \"relationships\": []}"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-display-schema",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Display In-Memory Schema Model\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"IN-MEMORY SCHEMA MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if NEO4J_SCHEMA and NEO4J_SCHEMA[\"labels\"]:\n",
    "    # Display labels with columns\n",
    "    print(f\"\\nNODE LABELS ({len(NEO4J_SCHEMA['labels'])} discovered):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for label_name, label_info in NEO4J_SCHEMA[\"labels\"].items():\n",
    "        columns = label_info[\"columns\"]\n",
    "        pk = label_info[\"primary_key\"]\n",
    "        \n",
    "        print(f\"\\n  {label_name}:\")\n",
    "        print(f\"    Primary Key: {pk or '(none)'}\")\n",
    "        print(f\"    Columns ({len(columns)}):\")\n",
    "        \n",
    "        for col in columns[:8]:  # Show first 8 columns\n",
    "            gen_marker = \" [generated]\" if col[\"is_generated\"] else \"\"\n",
    "            null_marker = \" (nullable)\" if col[\"nullable\"] else \"\"\n",
    "            print(f\"      - {col['name']}: {col['type_name']}{null_marker}{gen_marker}\")\n",
    "        \n",
    "        if len(columns) > 8:\n",
    "            print(f\"      ... and {len(columns) - 8} more columns\")\n",
    "    \n",
    "    # Display relationships\n",
    "    print(f\"\\nRELATIONSHIP PATTERNS ({len(NEO4J_SCHEMA['relationships'])} discovered):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for rel in NEO4J_SCHEMA[\"relationships\"][:10]:  # Show first 10\n",
    "        if rel[\"from_label\"] and rel[\"to_label\"]:\n",
    "            print(f\"  (:{rel['from_label']})-[:{rel['type']}]->(:{rel['to_label']})\")\n",
    "        else:\n",
    "            print(f\"  [:{rel['type']}] (pattern details unavailable)\")\n",
    "    \n",
    "    if len(NEO4J_SCHEMA[\"relationships\"]) > 10:\n",
    "        print(f\"  ... and {len(NEO4J_SCHEMA['relationships']) - 10} more patterns\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_columns = sum(len(l[\"columns\"]) for l in NEO4J_SCHEMA[\"labels\"].values())\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"SUMMARY: {len(NEO4J_SCHEMA['labels'])} labels, {total_columns} total columns, {len(NEO4J_SCHEMA['relationships'])} relationships\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n[WARN] No schema discovered. Run the schema discovery cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-option-a-setup",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTION A: Create Views with Inferred Schema\n",
    "# =============================================================================\n",
    "# Create Unity Catalog views that query through JDBC connection.\n",
    "# Schema is inferred from JDBC ResultSetMetaData at query time.\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTION A: Views with Inferred Schema\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select first label for testing\n",
    "if NEO4J_SCHEMA and NEO4J_SCHEMA[\"labels\"]:\n",
    "    TEST_LABEL_A = list(NEO4J_SCHEMA[\"labels\"].keys())[0]\n",
    "else:\n",
    "    TEST_LABEL_A = \"Aircraft\"  # Fallback\n",
    "\n",
    "VIEW_NAME_A = f\"neo4j_view_{TEST_LABEL_A.lower()}\"\n",
    "\n",
    "print(f\"\\n[INFO] Creating view for label: {TEST_LABEL_A}\")\n",
    "print(f\"[INFO] View name: {VIEW_NAME_A}\")\n",
    "\n",
    "# Generate and display the DDL\n",
    "view_ddl_a = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {VIEW_NAME_A} AS\n",
    "SELECT * FROM (\n",
    "    SELECT * FROM read_files(\n",
    "        'jdbc',\n",
    "        connection => '{UC_CONNECTION_NAME}',\n",
    "        dbtable => '{TEST_LABEL_A}'\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-option-a-dataframe",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Alternative approach using temp view + DataFrame\n",
    "print(\"\\n[INFO] Approach: Create view via DataFrame registration\")\n",
    "\n",
    "try:\n",
    "    # Read data through UC connection\n",
    "    df_a = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"dbtable\", TEST_LABEL_A) \\\n",
    "        .load()\n",
    "    \n",
    "    # Register as temp view (for testing - in production use permanent view)\n",
    "    df_a.createOrReplaceTempView(VIEW_NAME_A)\n",
    "    \n",
    "    print(f\"\\n[PASS] Temp view '{VIEW_NAME_A}' created\")\n",
    "    \n",
    "    # Show inferred schema\n",
    "    print(\"\\n[INFO] Inferred Schema:\")\n",
    "    df_a.printSchema()\n",
    "    \n",
    "    # Test query\n",
    "    print(\"\\n[INFO] Sample data (LIMIT 3):\")\n",
    "    spark.sql(f\"SELECT * FROM {VIEW_NAME_A} LIMIT 3\").show(truncate=False)\n",
    "    \n",
    "    # Verify via DESCRIBE\n",
    "    print(\"\\n[INFO] DESCRIBE output:\")\n",
    "    spark.sql(f\"DESCRIBE {VIEW_NAME_A}\").show(truncate=False)\n",
    "    \n",
    "    OPTION_A_SUCCESS = True\n",
    "    print(\"\\n[PASS] Option A completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Option A failed: {e}\")\n",
    "    OPTION_A_SUCCESS = False"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-option-b",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTION B: Tables with Explicit Schema\n",
    "# =============================================================================\n",
    "# Create Unity Catalog table with explicit column definitions from discovered schema.\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTION B: Tables with Explicit Schema\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select second label (or same if only one)\n",
    "labels_list = list(NEO4J_SCHEMA[\"labels\"].keys()) if NEO4J_SCHEMA else []\n",
    "TEST_LABEL_B = labels_list[1] if len(labels_list) > 1 else (labels_list[0] if labels_list else \"Airport\")\n",
    "\n",
    "TABLE_NAME_B = f\"neo4j_table_{TEST_LABEL_B.lower()}\"\n",
    "\n",
    "print(f\"\\n[INFO] Creating table for label: {TEST_LABEL_B}\")\n",
    "print(f\"[INFO] Table name: {TABLE_NAME_B}\")\n",
    "\n",
    "# Build explicit schema from discovered metadata\n",
    "def build_spark_schema_string(label_name, schema_dict):\n",
    "    \"\"\"Convert discovered schema to Spark SQL schema string.\"\"\"\n",
    "    if label_name not in schema_dict.get(\"labels\", {}):\n",
    "        return None\n",
    "    \n",
    "    columns = schema_dict[\"labels\"][label_name][\"columns\"]\n",
    "    \n",
    "    # Map JDBC types to Spark SQL types\n",
    "    type_mapping = {\n",
    "        \"STRING\": \"STRING\",\n",
    "        \"VARCHAR\": \"STRING\",\n",
    "        \"INTEGER\": \"INT\",\n",
    "        \"BIGINT\": \"LONG\",\n",
    "        \"LONG\": \"LONG\",\n",
    "        \"DOUBLE\": \"DOUBLE\",\n",
    "        \"FLOAT\": \"FLOAT\",\n",
    "        \"BOOLEAN\": \"BOOLEAN\",\n",
    "        \"DATE\": \"DATE\",\n",
    "        \"TIMESTAMP\": \"TIMESTAMP\",\n",
    "    }\n",
    "    \n",
    "    col_defs = []\n",
    "    for col in columns:\n",
    "        col_name = col[\"name\"]\n",
    "        # Handle special characters in column names\n",
    "        if \"$\" in col_name or \" \" in col_name:\n",
    "            col_name = f\"`{col_name}`\"\n",
    "        spark_type = type_mapping.get(col[\"type_name\"].upper(), \"STRING\")\n",
    "        col_defs.append(f\"{col_name} {spark_type}\")\n",
    "    \n",
    "    return \", \".join(col_defs)\n",
    "\n",
    "# Generate schema string\n",
    "schema_string_b = build_spark_schema_string(TEST_LABEL_B, NEO4J_SCHEMA)\n",
    "\n",
    "if schema_string_b:\n",
    "    print(f\"\\n[INFO] Generated customSchema:\")\n",
    "    print(f\"  {schema_string_b[:100]}...\" if len(schema_string_b) > 100 else f\"  {schema_string_b}\")\n",
    "else:\n",
    "    print(f\"\\n[WARN] Could not build schema for {TEST_LABEL_B}, using fallback\")\n",
    "    schema_string_b = \"`v$id` STRING\"  # Minimal fallback\n",
    "\n",
    "try:\n",
    "    # Read with explicit schema\n",
    "    df_b = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"dbtable\", TEST_LABEL_B) \\\n",
    "        .option(\"customSchema\", schema_string_b) \\\n",
    "        .load()\n",
    "    \n",
    "    # Register as temp view (simulating table creation)\n",
    "    df_b.createOrReplaceTempView(TABLE_NAME_B)\n",
    "    \n",
    "    print(f\"\\n[PASS] Temp table '{TABLE_NAME_B}' created with explicit schema\")\n",
    "    \n",
    "    # Show schema (should match our definition)\n",
    "    print(\"\\n[INFO] Applied Schema:\")\n",
    "    df_b.printSchema()\n",
    "    \n",
    "    # Test query\n",
    "    print(\"\\n[INFO] Sample data (LIMIT 3):\")\n",
    "    spark.sql(f\"SELECT * FROM {TABLE_NAME_B} LIMIT 3\").show(truncate=False)\n",
    "    \n",
    "    OPTION_B_SUCCESS = True\n",
    "    print(\"\\n[PASS] Option B completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Option B failed: {e}\")\n",
    "    OPTION_B_SUCCESS = False"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-option-c",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTION C: Hybrid Approach with Schema Registry\n",
    "# =============================================================================\n",
    "# Create a schema registry table storing discovered metadata + views for data access.\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTION C: Hybrid Approach with Schema Registry\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "REGISTRY_TABLE = \"neo4j_schema_registry\"\n",
    "\n",
    "# Select third label (or cycle back)\n",
    "labels_list = list(NEO4J_SCHEMA[\"labels\"].keys()) if NEO4J_SCHEMA else []\n",
    "TEST_LABEL_C = labels_list[2] if len(labels_list) > 2 else (labels_list[0] if labels_list else \"Flight\")\n",
    "VIEW_NAME_C = f\"neo4j_hybrid_{TEST_LABEL_C.lower()}\"\n",
    "\n",
    "print(f\"\\n[INFO] Creating schema registry: {REGISTRY_TABLE}\")\n",
    "print(f\"[INFO] Creating view for label: {TEST_LABEL_C}\")\n",
    "\n",
    "try:\n",
    "    # --- Step 1: Create Schema Registry Table ---\n",
    "    # Build registry data from discovered schema\n",
    "    registry_rows = []\n",
    "    \n",
    "    from datetime import datetime\n",
    "    discovered_at = datetime.now().isoformat()\n",
    "    \n",
    "    for label_name, label_info in NEO4J_SCHEMA.get(\"labels\", {}).items():\n",
    "        for col in label_info[\"columns\"]:\n",
    "            registry_rows.append({\n",
    "                \"label_name\": label_name,\n",
    "                \"column_name\": col[\"name\"],\n",
    "                \"column_type\": col[\"type_name\"],\n",
    "                \"is_nullable\": col[\"nullable\"],\n",
    "                \"is_generated\": col[\"is_generated\"],\n",
    "                \"is_primary_key\": col[\"name\"] == label_info.get(\"primary_key\"),\n",
    "                \"discovered_at\": discovered_at\n",
    "            })\n",
    "    \n",
    "    # Add relationship patterns\n",
    "    for rel in NEO4J_SCHEMA.get(\"relationships\", []):\n",
    "        registry_rows.append({\n",
    "            \"label_name\": f\"[REL] {rel['type']}\",\n",
    "            \"column_name\": f\"({rel.get('from_label', '?')})->({rel.get('to_label', '?')})\",\n",
    "            \"column_type\": \"RELATIONSHIP\",\n",
    "            \"is_nullable\": False,\n",
    "            \"is_generated\": False,\n",
    "            \"is_primary_key\": False,\n",
    "            \"discovered_at\": discovered_at\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and register as table\n",
    "    if registry_rows:\n",
    "        df_registry = spark.createDataFrame(registry_rows)\n",
    "        df_registry.createOrReplaceTempView(REGISTRY_TABLE)\n",
    "        \n",
    "        print(f\"\\n[PASS] Schema registry '{REGISTRY_TABLE}' created with {len(registry_rows)} entries\")\n",
    "        \n",
    "        # Show registry contents\n",
    "        print(\"\\n[INFO] Schema Registry Contents (sample):\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT label_name, column_name, column_type, is_primary_key \n",
    "            FROM {REGISTRY_TABLE} \n",
    "            WHERE column_type != 'RELATIONSHIP'\n",
    "            LIMIT 10\n",
    "        \"\"\").show(truncate=False)\n",
    "        \n",
    "        # Show relationship patterns\n",
    "        print(\"\\n[INFO] Relationship Patterns in Registry:\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT label_name as relationship, column_name as pattern\n",
    "            FROM {REGISTRY_TABLE} \n",
    "            WHERE column_type = 'RELATIONSHIP'\n",
    "            LIMIT 5\n",
    "        \"\"\").show(truncate=False)\n",
    "    else:\n",
    "        print(\"\\n[WARN] No schema data to populate registry\")\n",
    "    \n",
    "    # --- Step 2: Create View for Data Access ---\n",
    "    df_c = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "        .option(\"dbtable\", TEST_LABEL_C) \\\n",
    "        .load()\n",
    "    \n",
    "    df_c.createOrReplaceTempView(VIEW_NAME_C)\n",
    "    \n",
    "    print(f\"\\n[PASS] View '{VIEW_NAME_C}' created\")\n",
    "    \n",
    "    # Test query\n",
    "    print(\"\\n[INFO] Sample data from view (LIMIT 3):\")\n",
    "    spark.sql(f\"SELECT * FROM {VIEW_NAME_C} LIMIT 3\").show(truncate=False)\n",
    "    \n",
    "    # --- Step 3: Demonstrate combined usage ---\n",
    "    print(\"\\n[INFO] Combined Query - Schema + Data:\")\n",
    "    print(f\"  Registry shows {TEST_LABEL_C} has these columns:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT column_name, column_type \n",
    "        FROM {REGISTRY_TABLE} \n",
    "        WHERE label_name = '{TEST_LABEL_C}'\n",
    "        LIMIT 5\n",
    "    \"\"\").show(truncate=False)\n",
    "    \n",
    "    OPTION_C_SUCCESS = True\n",
    "    print(\"\\n[PASS] Option C completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Option C failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    OPTION_C_SUCCESS = False"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-comparison",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFICATION AND COMPARISON\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"VERIFICATION AND COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"APPROACH COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Option A results\n",
    "if 'OPTION_A_SUCCESS' in dir() and OPTION_A_SUCCESS:\n",
    "    row_count_a = spark.sql(f\"SELECT COUNT(*) as cnt FROM {VIEW_NAME_A}\").collect()[0][\"cnt\"]\n",
    "    comparison_data.append({\n",
    "        \"Approach\": \"Option A: Inferred Schema View\",\n",
    "        \"Object\": VIEW_NAME_A,\n",
    "        \"Status\": \"SUCCESS\",\n",
    "        \"Row Count\": row_count_a,\n",
    "        \"Schema Source\": \"Inferred at query time\"\n",
    "    })\n",
    "else:\n",
    "    comparison_data.append({\n",
    "        \"Approach\": \"Option A: Inferred Schema View\",\n",
    "        \"Object\": \"N/A\",\n",
    "        \"Status\": \"FAILED\",\n",
    "        \"Row Count\": 0,\n",
    "        \"Schema Source\": \"N/A\"\n",
    "    })\n",
    "\n",
    "# Option B results\n",
    "if 'OPTION_B_SUCCESS' in dir() and OPTION_B_SUCCESS:\n",
    "    row_count_b = spark.sql(f\"SELECT COUNT(*) as cnt FROM {TABLE_NAME_B}\").collect()[0][\"cnt\"]\n",
    "    comparison_data.append({\n",
    "        \"Approach\": \"Option B: Explicit Schema Table\",\n",
    "        \"Object\": TABLE_NAME_B,\n",
    "        \"Status\": \"SUCCESS\",\n",
    "        \"Row Count\": row_count_b,\n",
    "        \"Schema Source\": \"From DatabaseMetaData\"\n",
    "    })\n",
    "else:\n",
    "    comparison_data.append({\n",
    "        \"Approach\": \"Option B: Explicit Schema Table\",\n",
    "        \"Object\": \"N/A\",\n",
    "        \"Status\": \"FAILED\",\n",
    "        \"Row Count\": 0,\n",
    "        \"Schema Source\": \"N/A\"\n",
    "    })\n",
    "\n",
    "# Option C results\n",
    "if 'OPTION_C_SUCCESS' in dir() and OPTION_C_SUCCESS:\n",
    "    row_count_c = spark.sql(f\"SELECT COUNT(*) as cnt FROM {VIEW_NAME_C}\").collect()[0][\"cnt\"]\n",
    "    registry_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {REGISTRY_TABLE}\").collect()[0][\"cnt\"]\n",
    "    comparison_data.append({\n",
    "        \"Approach\": \"Option C: Hybrid (Registry + View)\",\n",
    "        \"Object\": f\"{REGISTRY_TABLE} + {VIEW_NAME_C}\",\n",
    "        \"Status\": \"SUCCESS\",\n",
    "        \"Row Count\": row_count_c,\n",
    "        \"Schema Source\": f\"Registry ({registry_count} entries)\"\n",
    "    })\n",
    "else:\n",
    "    comparison_data.append({\n",
    "        \"Approach\": \"Option C: Hybrid (Registry + View)\",\n",
    "        \"Object\": \"N/A\",\n",
    "        \"Status\": \"FAILED\",\n",
    "        \"Row Count\": 0,\n",
    "        \"Schema Source\": \"N/A\"\n",
    "    })\n",
    "\n",
    "# Display comparison\n",
    "df_comparison = spark.createDataFrame(comparison_data)\n",
    "df_comparison.show(truncate=False)\n",
    "\n",
    "# Pros and Cons\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"PROS AND CONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "OPTION A (Inferred Schema Views):\n",
    "  Pros:\n",
    "    - Simplest to implement\n",
    "    - Always reflects current Neo4j schema\n",
    "    - No schema maintenance needed\n",
    "  Cons:\n",
    "    - Schema inference may fail for some types\n",
    "    - Less control over column types\n",
    "    - Schema not visible until query time\n",
    "\n",
    "OPTION B (Explicit Schema Tables):\n",
    "  Pros:\n",
    "    - Full control over column types\n",
    "    - Predictable schema\n",
    "    - Avoids inference issues\n",
    "  Cons:\n",
    "    - Must regenerate when Neo4j schema changes\n",
    "    - Requires schema discovery step\n",
    "    - More code to maintain\n",
    "\n",
    "OPTION C (Hybrid with Registry):\n",
    "  Pros:\n",
    "    - Schema metadata is queryable\n",
    "    - Good for documentation/discovery\n",
    "    - Views provide live data access\n",
    "    - Best of both worlds\n",
    "  Cons:\n",
    "    - Most complex to implement\n",
    "    - Registry needs refresh mechanism\n",
    "    - Two objects to manage per label\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n[INFO] All approaches use the UC JDBC connection for governance\")\n",
    "print(\"[INFO] Choose based on your needs: simplicity (A), control (B), or visibility (C)\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-relationship-test",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIONAL: Relationship Traversal View Test\n",
    "# =============================================================================\n",
    "# Test creating a view that traverses relationships using SQL JOINs.\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIONAL: Relationship Traversal View Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find a relationship pattern to test\n",
    "test_pattern = None\n",
    "for rel in NEO4J_SCHEMA.get(\"relationships\", []):\n",
    "    if rel.get(\"from_label\") and rel.get(\"to_label\"):\n",
    "        test_pattern = rel\n",
    "        break\n",
    "\n",
    "if test_pattern:\n",
    "    from_label = test_pattern[\"from_label\"]\n",
    "    rel_type = test_pattern[\"type\"]\n",
    "    to_label = test_pattern[\"to_label\"]\n",
    "    \n",
    "    print(f\"\\n[INFO] Testing relationship pattern:\")\n",
    "    print(f\"  (:{from_label})-[:{rel_type}]->(:{to_label})\")\n",
    "    \n",
    "    # SQL JOIN that translates to Cypher relationship traversal\n",
    "    join_sql = f\"\"\"\n",
    "        SELECT COUNT(*) AS relationship_count\n",
    "        FROM {from_label} f\n",
    "        NATURAL JOIN {rel_type} r\n",
    "        NATURAL JOIN {to_label} t\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n[INFO] SQL Query:\")\n",
    "    print(f\"  {join_sql.strip()}\")\n",
    "    \n",
    "    try:\n",
    "        df_rel = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "            .option(\"query\", join_sql) \\\n",
    "            .option(\"customSchema\", \"relationship_count LONG\") \\\n",
    "            .load()\n",
    "        \n",
    "        count = df_rel.collect()[0][\"relationship_count\"]\n",
    "        print(f\"\\n[PASS] Relationship traversal works!\")\n",
    "        print(f\"[INFO] Found {count} relationships of type {rel_type}\")\n",
    "        \n",
    "        # Create a traversal view\n",
    "        TRAVERSAL_VIEW = f\"neo4j_rel_{rel_type.lower()}\"\n",
    "        \n",
    "        # For the view, get actual data (limited)\n",
    "        data_sql = f\"\"\"\n",
    "            SELECT * FROM {from_label} f\n",
    "            NATURAL JOIN {rel_type} r\n",
    "            NATURAL JOIN {to_label} t\n",
    "            LIMIT 100\n",
    "        \"\"\"\n",
    "        \n",
    "        df_traversal = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n",
    "            .option(\"query\", data_sql) \\\n",
    "            .load()\n",
    "        \n",
    "        df_traversal.createOrReplaceTempView(TRAVERSAL_VIEW)\n",
    "        print(f\"\\n[PASS] Traversal view '{TRAVERSAL_VIEW}' created\")\n",
    "        \n",
    "        print(\"\\n[INFO] Sample traversal data:\")\n",
    "        spark.sql(f\"SELECT * FROM {TRAVERSAL_VIEW} LIMIT 3\").show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FAIL] Relationship traversal failed: {e}\")\n",
    "        print(\"[INFO] This may be due to relationship pattern not existing in data\")\n",
    "else:\n",
    "    print(\"\\n[INFO] No relationship patterns discovered to test\")\n",
    "    print(\"[INFO] Skipping relationship traversal test\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "section8-cleanup",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP\n",
    "# =============================================================================\n",
    "# Drop temporary views/tables created during testing.\n",
    "# Uncomment the lines below to clean up.\n",
    "print(\"=\" * 70)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Temporary objects created during this demo:\n",
    "\n",
    "Views/Tables:\n",
    "\"\"\")\n",
    "\n",
    "# List objects that may have been created\n",
    "objects_to_clean = []\n",
    "if 'VIEW_NAME_A' in dir():\n",
    "    objects_to_clean.append(f\"  - {VIEW_NAME_A} (Option A view)\")\n",
    "if 'TABLE_NAME_B' in dir():\n",
    "    objects_to_clean.append(f\"  - {TABLE_NAME_B} (Option B table)\")\n",
    "if 'REGISTRY_TABLE' in dir():\n",
    "    objects_to_clean.append(f\"  - {REGISTRY_TABLE} (Option C registry)\")\n",
    "if 'VIEW_NAME_C' in dir():\n",
    "    objects_to_clean.append(f\"  - {VIEW_NAME_C} (Option C view)\")\n",
    "if 'TRAVERSAL_VIEW' in dir():\n",
    "    objects_to_clean.append(f\"  - {TRAVERSAL_VIEW} (Relationship view)\")\n",
    "\n",
    "for obj in objects_to_clean:\n",
    "    print(obj)\n",
    "\n",
    "print(\"\"\"\n",
    "To clean up, uncomment and run:\n",
    "\"\"\")\n",
    "\n",
    "cleanup_commands = []\n",
    "if 'VIEW_NAME_A' in dir():\n",
    "    cleanup_commands.append(f\"# spark.sql('DROP VIEW IF EXISTS {VIEW_NAME_A}')\")\n",
    "if 'TABLE_NAME_B' in dir():\n",
    "    cleanup_commands.append(f\"# spark.sql('DROP VIEW IF EXISTS {TABLE_NAME_B}')\")\n",
    "if 'REGISTRY_TABLE' in dir():\n",
    "    cleanup_commands.append(f\"# spark.sql('DROP VIEW IF EXISTS {REGISTRY_TABLE}')\")\n",
    "if 'VIEW_NAME_C' in dir():\n",
    "    cleanup_commands.append(f\"# spark.sql('DROP VIEW IF EXISTS {VIEW_NAME_C}')\")\n",
    "if 'TRAVERSAL_VIEW' in dir():\n",
    "    cleanup_commands.append(f\"# spark.sql('DROP VIEW IF EXISTS {TRAVERSAL_VIEW}')\")\n",
    "\n",
    "for cmd in cleanup_commands:\n",
    "    print(cmd)\n",
    "\n",
    "print(\"\"\"\n",
    "Note: These are temporary views that will be automatically dropped \n",
    "when the Spark session ends. Uncomment above to drop manually.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-summary",
   "metadata": {},
   "source": [
    "### Section 8 Summary\n",
    "\n",
    "This proof of concept demonstrated three approaches for synchronizing Neo4j graph schema with Unity Catalog:\n",
    "\n",
    "| Approach | Method | Best For |\n",
    "|----------|--------|----------|\n",
    "| **Option A** | Views with inferred schema | Simple use cases, always-current schema |\n",
    "| **Option B** | Tables with explicit schema | Predictable types, avoiding inference issues |\n",
    "| **Option C** | Hybrid (registry + views) | Schema visibility, documentation, governance |\n",
    "\n",
    "**Key Findings**:\n",
    "\n",
    "1. **Schema Discovery Works**: JDBC `DatabaseMetaData` API successfully discovers all Neo4j labels, properties, and relationships without hardcoding\n",
    "\n",
    "2. **UC Integration Works**: All three approaches successfully create queryable Unity Catalog objects backed by the JDBC connection\n",
    "\n",
    "3. **Governance Applies**: Queries go through UC connection, inheriting permissions and audit logging\n",
    "\n",
    "4. **SQL-to-Cypher Translation**: SQL JOINs correctly translate to Cypher relationship traversals\n",
    "\n",
    "**Limitations**:\n",
    "\n",
    "- Unity Catalog Foreign Catalogs don't support generic JDBC (only specific databases)\n",
    "- Must manually create UC objects (no automatic schema sync)\n",
    "- Temp views used for demo; production would use permanent tables/views\n",
    "\n",
    "**Next Steps** (out of scope for this POC):\n",
    "\n",
    "- Automated schema refresh mechanism\n",
    "- Production table/view creation with proper catalog.schema paths\n",
    "- Performance optimization for large graphs\n",
    "- Error handling and recovery\n",
    "\n",
    "**References**:\n",
    "- [META.md](../META.md) - Full proposal document\n",
    "- [Neo4j JDBC Manual](https://neo4j.com/docs/jdbc-manual/current/)\n",
    "- [Databricks JDBC Connection](https://docs.databricks.com/aws/en/connect/jdbc-connection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
