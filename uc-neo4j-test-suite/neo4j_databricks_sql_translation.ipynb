{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-issue-summary",
   "metadata": {},
   "source": [
    "# Neo4j JDBC Unity Catalog Connection - Support Ticket\n",
    "\n",
    "## Issue Summary\n",
    "\n",
    "**Problem**: Unity Catalog JDBC connection to Neo4j fails with `Connection was closed before the operation completed` error, despite:\n",
    "- Network connectivity working (TCP test passes)\n",
    "- Neo4j Python driver working\n",
    "- Neo4j Spark Connector working\n",
    "\n",
    "**Error Location**: `com.databricks.safespark.jdbc.grpc_client.JdbcConnectClient.awaitWhileConnected`\n",
    "\n",
    "This notebook provides a systematic test progression to isolate the failure point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": "---\n\n## Configuration\n\n**Prerequisites**: Run `setup.sh` to configure Databricks secrets before running this notebook.\n\nThe setup script reads credentials from `.env` and stores them in the `neo4j-uc-creds` secret scope:\n- `host` - Neo4j host\n- `user` - Neo4j username\n- `password` - Neo4j password\n- `connection_name` - Unity Catalog connection name\n- `jdbc_jar_path` - Path to Neo4j JDBC full bundle JAR in UC Volume\n- `cleaner_jar_path` - Path to Neo4j JDBC Spark cleaner JAR in UC Volume\n- `database` - Neo4j database (optional, defaults to \"neo4j\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-vars",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION - Loaded from Databricks Secrets\n# =============================================================================\n# Secrets are configured using setup.sh which creates scope \"neo4j-uc-creds\"\n# with secrets: host, user, password, connection_name, jdbc_jar_path, cleaner_jar_path, database\n\nSCOPE_NAME = \"neo4j-uc-creds\"\n\n# Aura Connection Details (from secrets)\nNEO4J_HOST = dbutils.secrets.get(SCOPE_NAME, \"host\")\nNEO4J_USER = dbutils.secrets.get(SCOPE_NAME, \"user\")\nNEO4J_PASSWORD = dbutils.secrets.get(SCOPE_NAME, \"password\")\n\n# Database defaults to \"neo4j\" if not set\ntry:\n    NEO4J_DATABASE = dbutils.secrets.get(SCOPE_NAME, \"database\")\nexcept:\n    NEO4J_DATABASE = \"neo4j\"\n\n# Unity Catalog Resources (from secrets)\nJDBC_JAR_PATH = dbutils.secrets.get(SCOPE_NAME, \"jdbc_jar_path\")\nCLEANER_JAR_PATH = dbutils.secrets.get(SCOPE_NAME, \"cleaner_jar_path\")\nUC_CONNECTION_NAME = dbutils.secrets.get(SCOPE_NAME, \"connection_name\")\n\n# Combined java_dependencies for CREATE CONNECTION\nJAVA_DEPENDENCIES = f'[\"{JDBC_JAR_PATH}\", \"{CLEANER_JAR_PATH}\"]'\n\n# Derived URLs (no need to edit)\nNEO4J_BOLT_URI = f\"neo4j+s://{NEO4J_HOST}\"\nNEO4J_JDBC_URL = f\"jdbc:neo4j+s://{NEO4J_HOST}:7687/{NEO4J_DATABASE}\"\nNEO4J_JDBC_URL_SQL = f\"{NEO4J_JDBC_URL}?enableSQLTranslation=true\"\n\nprint(\"Configuration loaded from Databricks Secrets:\")\nprint(f\"  Secret Scope: {SCOPE_NAME}\")\nprint(f\"  Neo4j Host: {NEO4J_HOST}\")\nprint(f\"  Bolt URI: {NEO4J_BOLT_URI}\")\nprint(f\"  JDBC URL: {NEO4J_JDBC_URL}\")\nprint(f\"  Connection Name: {UC_CONNECTION_NAME}\")\nprint(f\"  JDBC JAR Path: {JDBC_JAR_PATH}\")\nprint(f\"  Cleaner JAR Path: {CLEANER_JAR_PATH}\")\nprint(f\"  Java Dependencies: {JAVA_DEPENDENCIES}\")"
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Information\n",
    "\n",
    "Capture cluster and runtime details for support context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-info",
   "metadata": {},
   "outputs": [],
   "source": "# Collect environment information\nprint(\"=\" * 60)\nprint(\"ENVIRONMENT INFORMATION\")\nprint(\"=\" * 60)\n\n# Spark version\nprint(f\"\\nSpark Version: {spark.version}\")\n\n# Databricks Runtime\ntry:\n    dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")\n    print(f\"Databricks Runtime: {dbr_version}\")\nexcept:\n    print(\"Databricks Runtime: Unable to determine\")\n\n# Python version\nimport sys\nprint(f\"Python Version: {sys.version}\")\n\n# Check neo4j package\ntry:\n    import neo4j\n    print(f\"Neo4j Python Driver: {neo4j.__version__}\")\nexcept ImportError:\n    print(\"Neo4j Python Driver: NOT INSTALLED\")\n\n# Check JAR files exist\nprint(f\"\\nJDBC JAR Path: {JDBC_JAR_PATH}\")\nprint(f\"Cleaner JAR Path: {CLEANER_JAR_PATH}\")\ntry:\n    files = dbutils.fs.ls(JDBC_JAR_PATH.rsplit('/', 1)[0])\n    file_names = [f.name for f in files]\n    jdbc_jar_found = JDBC_JAR_PATH.split('/')[-1] in file_names\n    cleaner_jar_found = CLEANER_JAR_PATH.split('/')[-1] in file_names\n    print(f\"JDBC JAR File Exists: {jdbc_jar_found}\")\n    print(f\"Cleaner JAR File Exists: {cleaner_jar_found}\")\nexcept Exception as e:\n    print(f\"JAR File Check Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Network Connectivity Test (TCP Layer)\n",
    "\n",
    "**Expected Result**: PASS - Proves network path is open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-test",
   "metadata": {},
   "outputs": [],
   "source": "# TCP connectivity test using netcat\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Network Connectivity (TCP)\")\nprint(\"=\" * 60)\nprint(f\"\\nTarget: {NEO4J_HOST}:7687 (Bolt protocol port)\")\nprint(\"Testing: Can Databricks reach Neo4j at the network level?\")\n\nspark.sql(\"\"\"\nCREATE OR REPLACE TEMPORARY FUNCTION connectionTest(host STRING, port STRING)\nRETURNS STRING\nLANGUAGE PYTHON AS $$\nimport subprocess\nimport time\ntry:\n    start = time.time()\n    command = ['nc', '-zv', host, str(port)]\n    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=10)\n    elapsed = (time.time() - start) * 1000  # ms\n    output = result.stdout.decode() + result.stderr.decode()\n    if result.returncode == 0:\n        status = \"SUCCESS\"\n        message = f\"TCP connection established in {elapsed:.1f}ms\"\n    else:\n        status = \"FAILURE\"\n        message = f\"Cannot reach {host}:{port} - check firewall rules\"\n    return f\"{status}|{elapsed:.1f}|{message}|{output.strip()}\"\nexcept Exception as e:\n    return f\"FAILURE|0|Error: {str(e)}|\"\n$$\n\"\"\")\n\nstart_time = time.time()\nresult = spark.sql(f\"SELECT connectionTest('{NEO4J_HOST}', '7687') AS result\").collect()[0]['result']\ntotal_time = (time.time() - start_time) * 1000\n\nparts = result.split('|')\nstatus = parts[0]\nlatency = parts[1]\nmessage = parts[2]\ndetails = parts[3] if len(parts) > 3 else \"\"\n\nif status == \"SUCCESS\":\n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> CONNECTIVITY VERIFIED <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] {message}\")\n    print(f\"\\nConnection Details:\")\n    print(f\"  - Host: {NEO4J_HOST}\")\n    print(f\"  - Port: 7687 (Bolt)\")\n    print(f\"  - TCP Latency: {latency}ms\")\n    print(f\"  - Total Test Time: {total_time:.1f}ms\")\n    if details:\n        print(f\"  - Raw Output: {details}\")\n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Network path to Neo4j is OPEN\")\n    print(\"        Firewall rules allow Bolt protocol traffic\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\nelse:\n    print(f\"\\n[FAIL] {message}\")\n    print(f\"Details: {details}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "markdown",
   "id": "python-driver-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Neo4j Python Driver Test\n",
    "\n",
    "**Expected Result**: PASS - Proves credentials work and Neo4j is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "python-driver-test",
   "metadata": {},
   "outputs": [],
   "source": "# Test Neo4j Python driver connectivity\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Neo4j Python Driver\")\nprint(\"=\" * 60)\nprint(f\"\\nTarget: {NEO4J_BOLT_URI}\")\nprint(\"Testing: Can we authenticate and execute queries via Bolt protocol?\")\n\nfrom neo4j import GraphDatabase\n\ntry:\n    start_time = time.time()\n    driver = GraphDatabase.driver(NEO4J_BOLT_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n    \n    # Verify connectivity\n    driver.verify_connectivity()\n    connect_time = (time.time() - start_time) * 1000\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> AUTHENTICATION SUCCESSFUL <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] Driver connected and authenticated in {connect_time:.1f}ms\")\n    \n    # Test simple query\n    with driver.session() as session:\n        query_start = time.time()\n        result = session.run(\"RETURN 1 AS test\")\n        record = result.single()\n        query_time = (time.time() - query_start) * 1000\n        print(f\"[PASS] Query executed: RETURN 1 = {record['test']} ({query_time:.1f}ms)\")\n        \n        # Get Neo4j version\n        result = session.run(\"CALL dbms.components() YIELD name, versions RETURN name, versions\")\n        neo4j_info = []\n        for record in result:\n            neo4j_info.append(f\"{record['name']} {record['versions']}\")\n    \n    total_time = (time.time() - start_time) * 1000\n    driver.close()\n    \n    print(f\"\\nConnection Details:\")\n    print(f\"  - URI: {NEO4J_BOLT_URI}\")\n    print(f\"  - User: {NEO4J_USER}\")\n    print(f\"  - Database: {NEO4J_DATABASE}\")\n    print(f\"  - Neo4j Server: {', '.join(neo4j_info)}\")\n    print(f\"  - Connection Time: {connect_time:.1f}ms\")\n    print(f\"  - Total Test Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Neo4j Python Driver connection WORKING\")\n    print(\"        Credentials valid, Bolt protocol functional\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Connection failed: {e}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "markdown",
   "id": "spark-connector-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Neo4j Spark Connector (Working Baseline)\n",
    "\n",
    "**Expected Result**: PASS - This is our working baseline that proves Spark can communicate with Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark-connector-test",
   "metadata": {},
   "outputs": [],
   "source": "# Test Neo4j Spark Connector (known working method)\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Neo4j Spark Connector (org.neo4j.spark.DataSource)\")\nprint(\"=\" * 60)\nprint(f\"\\nTarget: {NEO4J_BOLT_URI}\")\nprint(\"Testing: Can Spark connect to Neo4j using the native Spark Connector?\")\nprint(\"Method: org.neo4j.spark.DataSource (uses Bolt protocol internally)\")\n\ntry:\n    start_time = time.time()\n    df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n        .option(\"url\", NEO4J_BOLT_URI) \\\n        .option(\"authentication.type\", \"basic\") \\\n        .option(\"authentication.basic.username\", NEO4J_USER) \\\n        .option(\"authentication.basic.password\", NEO4J_PASSWORD) \\\n        .option(\"query\", \"RETURN 'Spark Connector Works!' AS message, 1 AS value\") \\\n        .load()\n    \n    # Force execution and collect results\n    results = df.collect()\n    total_time = (time.time() - start_time) * 1000\n    row_count = len(results)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> SPARK CONNECTOR WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] Spark Connector query executed successfully in {total_time:.1f}ms\")\n    \n    print(f\"\\nQuery Results:\")\n    df.show(truncate=False)\n    \n    print(f\"Connection Details:\")\n    print(f\"  - Connector: org.neo4j.spark.DataSource\")\n    print(f\"  - URL: {NEO4J_BOLT_URI}\")\n    print(f\"  - Auth Type: basic\")\n    print(f\"  - Rows Returned: {row_count}\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Neo4j Spark Connector WORKING\")\n    print(\"        Spark can communicate with Neo4j via Bolt protocol\")\n    print(\"        This is the recommended approach for Spark-Neo4j integration\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Spark Connector failed: {e}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "markdown",
   "id": "direct-jdbc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Direct JDBC Tests (Bypassing Unity Catalog)\n",
    "\n",
    "These tests use the Neo4j JDBC driver directly with Spark, **without** Unity Catalog's SafeSpark wrapper.\n",
    "\n",
    "**Note**: Requires the JDBC JAR to be installed as a cluster library (not just in a UC Volume).\n",
    "\n",
    "**Limitation Discovered**: Spark's JDBC driver wraps `query` option queries in a subquery for schema inference:\n",
    "```sql\n",
    "SELECT * FROM (your_query) SPARK_GEN_SUBQ_N WHERE 1=0\n",
    "```\n",
    "This breaks native Cypher even with `FORCE_CYPHER` hint (hint is inside subquery, outer wrapper is still SQL).\n",
    "\n",
    "**Schema Inference Issue**: When using `dbtable` option, Spark's schema inference returns `NullType()` for all columns from Neo4j JDBC. This causes `No column has been read prior to this call` error when reading data. **Fix**: Use `customSchema` option to explicitly specify column types.\n",
    "\n",
    "**Workarounds**:\n",
    "1. Use `dbtable` option with `customSchema` (required to avoid NullType inference)\n",
    "2. Use `query` option with `customSchema` for SQL queries\n",
    "3. Use Neo4j Spark Connector instead of JDBC (Section 4 - works without customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-jdbc-no-sql",
   "metadata": {},
   "outputs": [],
   "source": "# Direct JDBC - Using dbtable (reads Neo4j label as table, no subquery wrapping)\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Direct JDBC - dbtable option (reads label as table)\")\nprint(\"=\" * 60)\nprint(f\"\\nJDBC URL: {NEO4J_JDBC_URL_SQL}\")\nprint(\"Testing: Can Spark read Neo4j nodes via JDBC using dbtable option?\")\nprint(\"Method: spark.read.format('jdbc') with Neo4j JDBC Driver\")\n\n# Use dbtable to read a Neo4j label directly (no subquery wrapper)\n# Replace 'Aircraft' with any label that exists in your Neo4j database\nTEST_LABEL = \"Aircraft\"  # Change this to a label in your database\n\n# IMPORTANT: customSchema is REQUIRED when using dbtable with Neo4j JDBC\n# Without it, Spark schema inference returns NullType() for all columns,\n# causing \"No column has been read prior to this call\" error when reading data.\n# Adjust column names and types to match your actual Neo4j node properties.\n# NOTE: Use backticks around column names with special characters (like $)\nAIRCRAFT_SCHEMA = \"`v$id` STRING, aircraft_id STRING, tail_number STRING, icao24 STRING, model STRING, operator STRING, manufacturer STRING\"\n\nprint(f\"\\nLabel: {TEST_LABEL}\")\nprint(f\"Custom Schema: {AIRCRAFT_SCHEMA}\")\n\ntry:\n    start_time = time.time()\n    df = spark.read.format(\"jdbc\") \\\n        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n        .option(\"user\", NEO4J_USER) \\\n        .option(\"password\", NEO4J_PASSWORD) \\\n        .option(\"dbtable\", TEST_LABEL) \\\n        .option(\"customSchema\", AIRCRAFT_SCHEMA) \\\n        .load()\n    \n    # Force execution and get count\n    results = df.take(5)\n    total_time = (time.time() - start_time) * 1000\n    row_count = len(results)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> DIRECT JDBC CONNECTION WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] Direct JDBC dbtable '{TEST_LABEL}' read successfully in {total_time:.1f}ms\")\n    \n    print(f\"\\nDataFrame Schema:\")\n    for field in df.schema.fields:\n        print(f\"  - {field.name}: {field.dataType}\")\n    \n    print(f\"\\nSample Data ({row_count} rows shown):\")\n    df.show(5, truncate=False)\n    \n    print(f\"Connection Details:\")\n    print(f\"  - Driver: org.neo4j.jdbc.Neo4jDriver\")\n    print(f\"  - URL: {NEO4J_JDBC_URL_SQL}\")\n    print(f\"  - Label/Table: {TEST_LABEL}\")\n    print(f\"  - SQL Translation: Enabled\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Direct JDBC Connection WORKING\")\n    print(\"        Neo4j JDBC Driver loaded and functional\")\n    print(\"        SQL-to-Cypher translation enabled\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Direct JDBC dbtable failed: {e}\")\n    print(\"\\nStatus: FAIL\")\n    print(\"\\nTroubleshooting:\")\n    print(\"  1. Ensure neo4j-jdbc-full-bundle JAR is installed as cluster library\")\n    print(\"  2. Verify the label exists in Neo4j\")\n    print(\"  3. Check customSchema column names match your Neo4j node properties\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-jdbc-with-sql",
   "metadata": {},
   "outputs": [],
   "source": "# Direct JDBC - SQL Translation (SQL automatically converted to Cypher)\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Direct JDBC - SQL Translation\")\nprint(\"=\" * 60)\nprint(f\"\\nJDBC URL: {NEO4J_JDBC_URL_SQL}\")\nprint(\"Testing: Can Neo4j JDBC translate simple SQL to Cypher?\")\nprint(\"Query: SELECT 1 AS value\")\n\ntry:\n    start_time = time.time()\n    df = spark.read.format(\"jdbc\") \\\n        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n        .option(\"user\", NEO4J_USER) \\\n        .option(\"password\", NEO4J_PASSWORD) \\\n        .option(\"query\", \"SELECT 1 AS value\") \\\n        .option(\"customSchema\", \"value INT\") \\\n        .load()\n    \n    # Force execution\n    results = df.collect()\n    total_time = (time.time() - start_time) * 1000\n    value = results[0]['value'] if results else None\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> SQL TRANSLATION WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] SQL query translated and executed in {total_time:.1f}ms\")\n    \n    print(f\"\\nQuery Results:\")\n    df.show(truncate=False)\n    \n    print(f\"Translation Details:\")\n    print(f\"  - Input SQL: SELECT 1 AS value\")\n    print(f\"  - Output Cypher: RETURN 1 AS value (translated automatically)\")\n    print(f\"  - Result Value: {value}\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: SQL-to-Cypher Translation WORKING\")\n    print(\"        Neo4j JDBC Driver successfully translates SQL to Cypher\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Direct JDBC with SQL translation failed: {e}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ocgnv7ybdy",
   "metadata": {},
   "outputs": [],
   "source": "# Direct JDBC - SQL Aggregate Query (COUNT)\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Direct JDBC - SQL Aggregate (COUNT)\")\nprint(\"=\" * 60)\nprint(f\"\\nJDBC URL: {NEO4J_JDBC_URL_SQL}\")\nprint(\"Testing: Can Neo4j JDBC translate SQL aggregates to Cypher?\")\nprint(\"SQL Query: SELECT COUNT(*) AS flight_count FROM Flight\")\nprint(\"Expected Cypher: MATCH (n:Flight) RETURN count(n) AS flight_count\")\n\ntry:\n    start_time = time.time()\n    df = spark.read.format(\"jdbc\") \\\n        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n        .option(\"user\", NEO4J_USER) \\\n        .option(\"password\", NEO4J_PASSWORD) \\\n        .option(\"query\", \"SELECT COUNT(*) AS flight_count FROM Flight\") \\\n        .option(\"customSchema\", \"flight_count LONG\") \\\n        .load()\n\n    # Force execution\n    results = df.collect()\n    total_time = (time.time() - start_time) * 1000\n    count = results[0]['flight_count'] if results else 0\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> SQL AGGREGATE TRANSLATION WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] SQL aggregate query executed in {total_time:.1f}ms\")\n    \n    print(f\"\\nQuery Results:\")\n    df.show(truncate=False)\n    \n    print(f\"Translation Details:\")\n    print(f\"  - Input SQL: SELECT COUNT(*) AS flight_count FROM Flight\")\n    print(f\"  - Translated to Cypher: MATCH (n:Flight) RETURN count(n)\")\n    print(f\"  - Flight Count: {count:,}\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: SQL Aggregate Translation WORKING\")\n    print(\"        COUNT(*) on Neo4j label successfully translated to Cypher\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n\nexcept Exception as e:\n    print(f\"\\n[FAIL] Direct JDBC aggregate query failed: {e}\")\n    print(\"\\nStatus: FAIL\")\n    print(\"\\nTroubleshooting:\")\n    print(\"  - Ensure 'Flight' label exists in your Neo4j database\")\n    print(\"  - Or change the query to use a label that exists in your graph\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40s9rk0dug",
   "metadata": {},
   "outputs": [],
   "source": "# Direct JDBC - SQL JOIN Translation (NATURAL JOIN -> Cypher relationship)\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Direct JDBC - SQL JOIN Translation\")\nprint(\"=\" * 60)\nprint(f\"\\nJDBC URL: {NEO4J_JDBC_URL_SQL}\")\nprint(\"Testing: Can Neo4j JDBC translate SQL JOINs to Cypher relationships?\")\nprint(\"\\nSQL Query:\")\nprint(\"  SELECT COUNT(*) AS cnt FROM Flight f\")\nprint(\"  NATURAL JOIN DEPARTS_FROM r NATURAL JOIN Airport a\")\nprint(\"\\nExpected Cypher:\")\nprint(\"  MATCH (f:Flight)-[:DEPARTS_FROM]->(a:Airport) RETURN count(*)\")\nprint(\"\\nRef: https://neo4j.com/docs/jdbc-manual/current/sql2cypher/\")\n\ntry:\n    start_time = time.time()\n    df = spark.read.format(\"jdbc\") \\\n        .option(\"url\", NEO4J_JDBC_URL_SQL) \\\n        .option(\"driver\", \"org.neo4j.jdbc.Neo4jDriver\") \\\n        .option(\"user\", NEO4J_USER) \\\n        .option(\"password\", NEO4J_PASSWORD) \\\n        .option(\"query\", \"\"\"SELECT COUNT(*) AS cnt\n                           FROM Flight f\n                           NATURAL JOIN DEPARTS_FROM r\n                           NATURAL JOIN Airport a\"\"\") \\\n        .option(\"customSchema\", \"cnt LONG\") \\\n        .load()\n\n    # Force execution\n    results = df.collect()\n    total_time = (time.time() - start_time) * 1000\n    count = results[0]['cnt'] if results else 0\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> SQL JOIN TRANSLATION WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] SQL JOIN query translated and executed in {total_time:.1f}ms\")\n    \n    print(f\"\\nQuery Results:\")\n    df.show(truncate=False)\n    \n    print(f\"Translation Details:\")\n    print(f\"  - SQL Tables: Flight, DEPARTS_FROM, Airport\")\n    print(f\"  - Cypher Pattern: (f:Flight)-[:DEPARTS_FROM]->(a:Airport)\")\n    print(f\"  - Matching Relationships: {count:,}\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: SQL JOIN to Cypher Relationship Translation WORKING\")\n    print(\"        NATURAL JOIN successfully mapped to graph traversal\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n\nexcept Exception as e:\n    print(f\"\\n[FAIL] Direct JDBC JOIN translation failed: {e}\")\n    print(\"\\nStatus: FAIL\")\n    print(\"\\nTroubleshooting:\")\n    print(\"  - Requires Flight-[:DEPARTS_FROM]->Airport pattern in Neo4j\")\n    print(\"  - Adjust labels/relationship types to match your graph model\")"
  },
  {
   "cell_type": "markdown",
   "id": "uc-jdbc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Unity Catalog JDBC Connection\n",
    "\n",
    "This section creates and tests the Unity Catalog JDBC connection, which uses the SafeSpark wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-create-connection",
   "metadata": {},
   "outputs": [],
   "source": "# Create Unity Catalog JDBC Connection\nimport time\n\nprint(\"=\" * 60)\nprint(\"SETUP: Create Unity Catalog JDBC Connection\")\nprint(\"=\" * 60)\nprint(f\"\\nConnection Name: {UC_CONNECTION_NAME}\")\nprint(f\"JDBC URL: {NEO4J_JDBC_URL_SQL}\")\nprint(f\"Driver: org.neo4j.jdbc.Neo4jDriver\")\n\n# Drop existing connection\nspark.sql(f\"DROP CONNECTION IF EXISTS {UC_CONNECTION_NAME}\")\nprint(f\"\\n[INFO] Dropped existing connection (if any): {UC_CONNECTION_NAME}\")\n\n# Create connection with explicit driver class\n# NOTE: customSchema must be in externalOptionsAllowList to bypass Spark schema inference\n# NOTE: java_dependencies includes both the full bundle and spark cleaner JARs\ncreate_sql = f\"\"\"\nCREATE CONNECTION {UC_CONNECTION_NAME} TYPE JDBC\nENVIRONMENT (\n  java_dependencies '{JAVA_DEPENDENCIES}'\n)\nOPTIONS (\n  url '{NEO4J_JDBC_URL_SQL}',\n  user '{NEO4J_USER}',\n  password '{NEO4J_PASSWORD}',\n  driver 'org.neo4j.jdbc.Neo4jDriver',\n  externalOptionsAllowList 'dbtable,query,partitionColumn,lowerBound,upperBound,numPartitions,fetchSize,customSchema'\n)\n\"\"\"\n\nprint(f\"[INFO] Java Dependencies:\")\nprint(f\"  - {JDBC_JAR_PATH}\")\nprint(f\"  - {CLEANER_JAR_PATH}\")\n\ntry:\n    start_time = time.time()\n    spark.sql(create_sql)\n    create_time = (time.time() - start_time) * 1000\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> UC CONNECTION CREATED <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] Connection '{UC_CONNECTION_NAME}' created in {create_time:.1f}ms\")\n    \n    print(f\"\\nConnection Configuration:\")\n    print(f\"  - Name: {UC_CONNECTION_NAME}\")\n    print(f\"  - Type: JDBC\")\n    print(f\"  - Driver: org.neo4j.jdbc.Neo4jDriver\")\n    print(f\"  - SQL Translation: Enabled\")\n    print(f\"  - Spark Cleaner: Included\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Failed to create connection: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-describe-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify connection configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFY: Connection Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df = spark.sql(f\"DESCRIBE CONNECTION {UC_CONNECTION_NAME}\")\n",
    "    print(\"\\nConnection details:\")\n",
    "    df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[FAIL] Cannot describe connection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uc-test-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Unity Catalog JDBC Tests\n",
    "\n",
    "These tests use the Unity Catalog connection through the SafeSpark JDBC wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-spark-dataframe-test",
   "metadata": {},
   "outputs": [],
   "source": "# Test UC Connection via Spark DataFrame API\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Unity Catalog - Spark DataFrame API\")\nprint(\"=\" * 60)\nprint(f\"\\nConnection: {UC_CONNECTION_NAME}\")\nprint(\"Testing: Can we query Neo4j via UC JDBC connection using DataFrame API?\")\nprint(\"Query: SELECT 1 AS test\")\n\ntry:\n    start_time = time.time()\n    df = spark.read.format(\"jdbc\") \\\n        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n        .option(\"query\", \"SELECT 1 AS test\") \\\n        .load()\n    \n    # Force execution\n    results = df.collect()\n    total_time = (time.time() - start_time) * 1000\n    value = results[0]['test'] if results else None\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> UC JDBC CONNECTION WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] Unity Catalog JDBC query executed in {total_time:.1f}ms\")\n    \n    print(f\"\\nQuery Results:\")\n    df.show()\n    \n    print(f\"Connection Details:\")\n    print(f\"  - UC Connection: {UC_CONNECTION_NAME}\")\n    print(f\"  - Method: spark.read.format('jdbc').option('databricks.connection', ...)\")\n    print(f\"  - Result Value: {value}\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Unity Catalog JDBC Connection WORKING\")\n    print(\"        SafeSpark wrapper successfully connected to Neo4j\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Unity Catalog Spark DataFrame API failed:\")\n    print(f\"\\nError: {e}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-spark-cypher-test",
   "metadata": {},
   "outputs": [],
   "source": "# Test UC Connection with native Cypher (FORCE_CYPHER hint)\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Unity Catalog - Native Cypher (FORCE_CYPHER)\")\nprint(\"=\" * 60)\nprint(f\"\\nConnection: {UC_CONNECTION_NAME}\")\nprint(\"Testing: Can we execute native Cypher via UC connection?\")\nprint(\"Query: /*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test\")\nprint(\"\\nNote: FORCE_CYPHER hint bypasses SQL translation and sends Cypher directly\")\n\ntry:\n    start_time = time.time()\n    df = spark.read.format(\"jdbc\") \\\n        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n        .option(\"query\", \"/*+ NEO4J FORCE_CYPHER */ RETURN 1 AS test\") \\\n        .option(\"customSchema\", \"test INT\") \\\n        .load()\n    \n    # Force execution\n    results = df.collect()\n    total_time = (time.time() - start_time) * 1000\n    value = results[0]['test'] if results else None\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> NATIVE CYPHER VIA UC WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] Native Cypher query executed via UC in {total_time:.1f}ms\")\n    \n    print(f\"\\nQuery Results:\")\n    df.show()\n    \n    print(f\"Execution Details:\")\n    print(f\"  - UC Connection: {UC_CONNECTION_NAME}\")\n    print(f\"  - Query Type: Native Cypher (FORCE_CYPHER hint)\")\n    print(f\"  - Custom Schema: test INT\")\n    print(f\"  - Result Value: {value}\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Native Cypher via Unity Catalog WORKING\")\n    print(\"        FORCE_CYPHER hint successfully bypasses SQL translation\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Unity Catalog with FORCE_CYPHER failed:\")\n    print(f\"\\nError: {e}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uc-remote-query-test",
   "metadata": {},
   "outputs": [],
   "source": "# Test UC Connection via remote_query() function\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Unity Catalog - remote_query() Function\")\nprint(\"=\" * 60)\nprint(f\"\\nConnection: {UC_CONNECTION_NAME}\")\nprint(\"Testing: Can we use Spark SQL remote_query() with UC connection?\")\nprint(\"Query: SELECT * FROM remote_query('connection', query => 'SELECT 1 AS test')\")\n\ntry:\n    start_time = time.time()\n    df = spark.sql(f\"\"\"\n        SELECT * FROM remote_query(\n            '{UC_CONNECTION_NAME}',\n            query => 'SELECT 1 AS test'\n        )\n    \"\"\")\n    \n    # Force execution\n    results = df.collect()\n    total_time = (time.time() - start_time) * 1000\n    value = results[0]['test'] if results else None\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> UC remote_query() WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] remote_query() executed in {total_time:.1f}ms\")\n    \n    print(f\"\\nQuery Results:\")\n    df.show()\n    \n    print(f\"Execution Details:\")\n    print(f\"  - UC Connection: {UC_CONNECTION_NAME}\")\n    print(f\"  - Method: Spark SQL remote_query() function\")\n    print(f\"  - Result Value: {value}\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: remote_query() Function WORKING\")\n    print(\"        Spark SQL can query Neo4j via Unity Catalog connection\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Unity Catalog remote_query() failed:\")\n    print(f\"\\nError: {e}\")\n    print(\"\\nStatus: FAIL\")"
  },
  {
   "cell_type": "code",
   "id": "6ulxi8xl1gm",
   "source": "# Test UC Connection with SQL Aggregate Query using Custom Schema\nimport time\n\nprint(\"=\" * 60)\nprint(\"TEST: Unity Catalog - SQL Aggregate with Custom Schema\")\nprint(\"=\" * 60)\nprint(f\"\\nConnection: {UC_CONNECTION_NAME}\")\nprint(\"Testing: Can we execute SQL aggregates via UC with customSchema?\")\nprint(\"SQL Query: SELECT COUNT(*) AS flight_count FROM Flight\")\n\n# CustomSchema for Neo4j JDBC\n# ============================================\n# Spark's automatic schema inference wraps queries in a subquery:\n#   SELECT * FROM (your_query) SPARK_GEN_SUBQ WHERE 1=0\n# Neo4j JDBC returns NullType() for all columns during inference,\n# causing \"No column has been read\" errors when reading data.\n#\n# Workaround: Use customSchema to explicitly define column types\n# Reference: https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n\nFLIGHT_COUNT_SCHEMA = \"flight_count LONG\"\nprint(f\"Custom Schema: {FLIGHT_COUNT_SCHEMA}\")\n\ntry:\n    start_time = time.time()\n    df = spark.read.format(\"jdbc\") \\\n        .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n        .option(\"query\", \"SELECT COUNT(*) AS flight_count FROM Flight\") \\\n        .option(\"customSchema\", FLIGHT_COUNT_SCHEMA) \\\n        .load()\n    \n    # Force execution\n    results = df.collect()\n    total_time = (time.time() - start_time) * 1000\n    count = results[0]['flight_count'] if results else 0\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\">>> UC SQL AGGREGATE WORKING <<<\")\n    print(\"=\" * 60)\n    print(f\"\\n[PASS] SQL aggregate via UC executed in {total_time:.1f}ms\")\n    \n    print(f\"\\nQuery Results:\")\n    df.show(truncate=False)\n    \n    print(f\"Execution Details:\")\n    print(f\"  - UC Connection: {UC_CONNECTION_NAME}\")\n    print(f\"  - SQL Query: SELECT COUNT(*) AS flight_count FROM Flight\")\n    print(f\"  - Custom Schema: {FLIGHT_COUNT_SCHEMA}\")\n    print(f\"  - DataFrame Schema: {df.schema}\")\n    print(f\"  - Flight Count: {count:,}\")\n    print(f\"  - Execution Time: {total_time:.1f}ms\")\n    \n    print(\"\\n\" + \"-\" * 60)\n    print(\"RESULT: Unity Catalog SQL Aggregate WORKING\")\n    print(\"        SQL-to-Cypher translation functional via UC connection\")\n    print(\"-\" * 60)\n    print(\"\\nStatus: PASS\")\n    \nexcept Exception as e:\n    print(f\"\\n[FAIL] Unity Catalog SQL aggregate query failed:\")\n    print(f\"\\nError: {e}\")\n    print(\"\\nStatus: FAIL\")\n    print(\"\\nTroubleshooting:\")\n    print(\"  - Ensure 'Flight' label exists in your Neo4j database\")\n    print(\"  - Adjust the label name to match your graph model if needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}