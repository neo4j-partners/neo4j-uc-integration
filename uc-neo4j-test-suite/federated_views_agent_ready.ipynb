{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Federated Views: Agent-Ready Neo4j + Delta Lakehouse\n",
    "\n",
    "Creates Unity Catalog views that encapsulate `remote_query()` calls to Neo4j, making\n",
    "graph data queryable as regular UC tables. These views enable **Genie** (or any SQL tool)\n",
    "to transparently federate queries across Neo4j and Delta lakehouse without Spark Connector\n",
    "or direct Python drivers.\n",
    "\n",
    "The full chain: **Natural Language → SQL (Genie) → Cypher (JDBC driver) → Neo4j** — all\n",
    "through Unity Catalog federation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────────┐\n",
    "│                  Genie / SQL Tool / Agent                           │\n",
    "│           (natural language or SQL queries)                         │\n",
    "└─────────────────────────────┬────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌──────────────────────────────────────────────────────────────────────┐\n",
    "│                      Spark SQL Engine                               │\n",
    "│                                                                      │\n",
    "│   Queries run against UC views that look like regular tables:       │\n",
    "│   - aircraft, systems, sensors, sensor_readings  (Delta — direct)   │\n",
    "│   - neo4j_maintenance_events  (view over remote_query)              │\n",
    "│   - neo4j_flights             (view over remote_query)              │\n",
    "│   - neo4j_flight_airports     (view over remote_query)             │\n",
    "│                                                                      │\n",
    "├────────────────────────────┬─────────────────────────────────────────┤\n",
    "│  Delta Lakehouse (direct)  │  Neo4j (via remote_query → JDBC)       │\n",
    "│                            │  enableSQLTranslation=true             │\n",
    "│                            │  SQL → Cypher automatic translation    │\n",
    "└────────────────────────────┴─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key advantage:** GROUP BY, ORDER BY, and JOINs with Delta tables all work because\n",
    "Spark handles the aggregation *after* `remote_query()` returns the rows via the views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. **Lakehouse tables** already exist in your catalog (created by lab setup):\n",
    "   `aircraft`, `systems`, `sensors`, `sensor_readings`\n",
    "\n",
    "2. **Neo4j UC JDBC connection** configured per [GUIDE_NEO4J_UC.md](../GUIDE_NEO4J_UC.md)\n",
    "\n",
    "3. **Cluster configuration:**\n",
    "   - SafeSpark memory settings applied (see guide)\n",
    "   - `neo4j-uc-creds` secret scope configured via `setup.sh`\n",
    "\n",
    "4. **Databricks preview features** enabled:\n",
    "   - Custom JDBC on UC Compute\n",
    "   - `remote_query` table-valued function\n",
    "\n",
    "**Note:** No Spark Connector or cluster libraries required — this notebook uses\n",
    "pure UC JDBC federation only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "config",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "SCOPE_NAME = \"neo4j-uc-creds\"\n",
    "\n",
    "# Lakehouse configuration — update to match your environment\n",
    "LAKEHOUSE_CATALOG = \"aws-databricks-neo4j-lab\"   # Your Unity Catalog name\n",
    "LAKEHOUSE_SCHEMA = \"lakehouse\"                    # Schema containing Delta tables\n",
    "\n",
    "# Neo4j credentials from Databricks Secrets\n",
    "NEO4J_HOST = dbutils.secrets.get(SCOPE_NAME, \"host\")\n",
    "NEO4J_USER = dbutils.secrets.get(SCOPE_NAME, \"user\")\n",
    "NEO4J_PASSWORD = dbutils.secrets.get(SCOPE_NAME, \"password\")\n",
    "try:\n",
    "    NEO4J_DATABASE = dbutils.secrets.get(SCOPE_NAME, \"database\")\n",
    "except Exception:\n",
    "    NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "UC_CONNECTION_NAME = dbutils.secrets.get(SCOPE_NAME, \"connection_name\")\n",
    "\n",
    "# Set catalog and schema context\n",
    "spark.sql(f\"USE CATALOG `{LAKEHOUSE_CATALOG}`\")\n",
    "spark.sql(f\"USE SCHEMA `{LAKEHOUSE_SCHEMA}`\")\n",
    "\n",
    "print(f\"Lakehouse: {LAKEHOUSE_CATALOG}.{LAKEHOUSE_SCHEMA}\")\n",
    "print(f\"Neo4j Host: {NEO4J_HOST}\")\n",
    "print(f\"UC Connection: {UC_CONNECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Verify Data Sources\n",
    "\n",
    "Confirm both the lakehouse tables and Neo4j UC connection are accessible."
   ]
  },
  {
   "cell_type": "code",
   "id": "verify-lakehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify Delta lakehouse tables\n",
    "print(\"=\" * 60)\n",
    "print(\"DELTA LAKEHOUSE TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table in [\"aircraft\", \"systems\", \"sensors\", \"sensor_readings\"]:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {table}\").collect()[0][\"cnt\"]\n",
    "    print(f\"  {table}: {count:,} rows\")\n",
    "\n",
    "print(\"\\nSample aircraft data:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
    "    FROM aircraft LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "id": "verify-neo4j",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify Neo4j UC JDBC connection\n",
    "print(\"=\" * 60)\n",
    "print(\"NEO4J KNOWLEDGE GRAPH (via UC JDBC)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "neo4j_counts = {\n",
    "    \"Aircraft\": \"SELECT COUNT(*) AS cnt FROM Aircraft\",\n",
    "    \"MaintenanceEvent\": \"SELECT COUNT(*) AS cnt FROM MaintenanceEvent\",\n",
    "    \"Flight\": \"SELECT COUNT(*) AS cnt FROM Flight\",\n",
    "    \"Airport\": \"SELECT COUNT(*) AS cnt FROM Airport\",\n",
    "}\n",
    "\n",
    "for label, query in neo4j_counts.items():\n",
    "    result = spark.sql(f\"\"\"\n",
    "        SELECT * FROM remote_query('{UC_CONNECTION_NAME}', query => '{query}')\n",
    "    \"\"\").collect()\n",
    "    print(f\"  {label}: {result[0]['cnt']:,} nodes\")\n",
    "\n",
    "print(\"\\nBoth data sources verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "views-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Create UC Views over Neo4j\n",
    "\n",
    "Each view wraps a `remote_query()` call, exposing Neo4j graph data as a regular UC\n",
    "table. Spark handles all aggregation, GROUP BY, ORDER BY, and JOINs on top of\n",
    "the rows returned by `remote_query()`.\n",
    "\n",
    "**Why views?** The `remote_query()` function has limitations when used inline —\n",
    "Spark wraps queries in subqueries for schema inference, breaking GROUP BY/ORDER BY.\n",
    "Views solve this by returning the raw rows, letting Spark SQL handle everything else."
   ]
  },
  {
   "cell_type": "code",
   "id": "create-maintenance-view",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# =============================================================================\n# VIEW: neo4j_maintenance_events\n# =============================================================================\n# Exposes MaintenanceEvent nodes from Neo4j as a queryable UC view.\n# Uses DataFrame API with customSchema to bypass Spark's subquery schema inference,\n# which breaks non-aggregate SELECT queries through Neo4j JDBC.\n\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n    .option(\"query\", \"\"\"SELECT aircraft_id, fault, severity, corrective_action, reported_at\n                        FROM MaintenanceEvent\"\"\") \\\n    .option(\"customSchema\", \"aircraft_id STRING, fault STRING, severity STRING, corrective_action STRING, reported_at STRING\") \\\n    .load()\ndf.createOrReplaceTempView(\"neo4j_maintenance_events\")\n\ncount = spark.sql(\"SELECT COUNT(*) AS cnt FROM neo4j_maintenance_events\").collect()[0][\"cnt\"]\nprint(f\"neo4j_maintenance_events: {count:,} rows\")\n\nprint(\"\\nSchema:\")\nspark.sql(\"SELECT * FROM neo4j_maintenance_events LIMIT 1\").printSchema()\n\nprint(\"Sample data:\")\nspark.sql(\"\"\"\n    SELECT aircraft_id, fault, severity, corrective_action\n    FROM neo4j_maintenance_events LIMIT 5\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "id": "create-flights-view",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# =============================================================================\n# VIEW: neo4j_flights\n# =============================================================================\n# Exposes Flight nodes from Neo4j as a queryable UC view.\n# Uses DataFrame API with customSchema to bypass Spark's subquery schema inference.\n\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n    .option(\"query\", \"\"\"SELECT aircraft_id, flight_number, operator,\n                               origin, destination,\n                               scheduled_departure, scheduled_arrival\n                        FROM Flight\"\"\") \\\n    .option(\"customSchema\", \"aircraft_id STRING, flight_number STRING, operator STRING, origin STRING, destination STRING, scheduled_departure STRING, scheduled_arrival STRING\") \\\n    .load()\ndf.createOrReplaceTempView(\"neo4j_flights\")\n\ncount = spark.sql(\"SELECT COUNT(*) AS cnt FROM neo4j_flights\").collect()[0][\"cnt\"]\nprint(f\"neo4j_flights: {count:,} rows\")\n\nprint(\"\\nSchema:\")\nspark.sql(\"SELECT * FROM neo4j_flights LIMIT 1\").printSchema()\n\nprint(\"Sample data:\")\nspark.sql(\"\"\"\n    SELECT aircraft_id, flight_number, operator, origin, destination\n    FROM neo4j_flights LIMIT 5\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "id": "create-flight-airports-view",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# =============================================================================\n# VIEW: neo4j_flight_airports\n# =============================================================================\n# Exposes Flight→Airport graph traversals from Neo4j.\n# Uses NATURAL JOIN syntax which the Neo4j JDBC driver translates to\n# MATCH (f:Flight)-[:DEPARTS_FROM]->(a:Airport) in Cypher.\n# Uses DataFrame API with customSchema to bypass Spark's subquery schema inference.\n\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n    .option(\"query\", \"\"\"SELECT f.flight_number, f.aircraft_id, a.iata AS airport_code,\n                               a.name AS airport_name\n                        FROM Flight f\n                        NATURAL JOIN DEPARTS_FROM r\n                        NATURAL JOIN Airport a\"\"\") \\\n    .option(\"customSchema\", \"flight_number STRING, aircraft_id STRING, airport_code STRING, airport_name STRING\") \\\n    .load()\ndf.createOrReplaceTempView(\"neo4j_flight_airports\")\n\ncount = spark.sql(\"SELECT COUNT(*) AS cnt FROM neo4j_flight_airports\").collect()[0][\"cnt\"]\nprint(f\"neo4j_flight_airports: {count:,} rows\")\n\nprint(\"\\nSchema:\")\nspark.sql(\"SELECT * FROM neo4j_flight_airports LIMIT 1\").printSchema()\n\nprint(\"Sample data:\")\nspark.sql(\"\"\"\n    SELECT flight_number, aircraft_id, airport_code, airport_name\n    FROM neo4j_flight_airports LIMIT 5\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "id": "verify-all-views",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify all views are accessible\n",
    "print(\"=\" * 60)\n",
    "print(\"ALL VIEWS CREATED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "views = {\n",
    "    \"neo4j_maintenance_events\": \"Neo4j MaintenanceEvent nodes\",\n",
    "    \"neo4j_flights\": \"Neo4j Flight nodes\",\n",
    "    \"neo4j_flight_airports\": \"Neo4j Flight→Airport traversals\",\n",
    "}\n",
    "\n",
    "for view_name, description in views.items():\n",
    "    count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {view_name}\").collect()[0][\"cnt\"]\n",
    "    print(f\"  {view_name}: {count:,} rows  ({description})\")\n",
    "\n",
    "print(\"\\nDelta tables (direct):\")\n",
    "for table in [\"aircraft\", \"systems\", \"sensors\", \"sensor_readings\"]:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {table}\").collect()[0][\"cnt\"]\n",
    "    print(f\"  {table}: {count:,} rows\")\n",
    "\n",
    "print(\"\\nAll 7 tables/views are ready for federated queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sql-tests-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: SQL Tests Against Views\n",
    "\n",
    "Test that the Neo4j views support standard SQL operations — GROUP BY, ORDER BY,\n",
    "WHERE filters, aggregations — that would fail with inline `remote_query()` calls.\n",
    "These are the operations Genie needs to generate."
   ]
  },
  {
   "cell_type": "code",
   "id": "test-group-by",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 1: GROUP BY on Neo4j view\n",
    "# =============================================================================\n",
    "# This fails with inline remote_query() but works with views.\n",
    "\n",
    "print(\"TEST 1: GROUP BY — Maintenance events by severity\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        severity,\n",
    "        COUNT(*) AS event_count\n",
    "    FROM neo4j_maintenance_events\n",
    "    GROUP BY severity\n",
    "    ORDER BY event_count DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "assert result.count() > 0, \"Expected at least one severity level\"\n",
    "print(\"[PASS] GROUP BY on Neo4j view works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-order-by",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 2: ORDER BY on Neo4j view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 2: ORDER BY — Flights ordered by flight_number\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT flight_number, aircraft_id, operator, origin, destination\n",
    "    FROM neo4j_flights\n",
    "    ORDER BY flight_number\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "rows = result.collect()\n",
    "flight_numbers = [r[\"flight_number\"] for r in rows]\n",
    "assert flight_numbers == sorted(flight_numbers), \"Expected sorted flight numbers\"\n",
    "print(\"[PASS] ORDER BY on Neo4j view works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-where-filter",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 3: WHERE filter on Neo4j view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 3: WHERE — Critical maintenance events only\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT aircraft_id, fault, severity, corrective_action\n",
    "    FROM neo4j_maintenance_events\n",
    "    WHERE severity = 'CRITICAL'\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "count = result.count()\n",
    "print(f\"Found {count} CRITICAL events\")\n",
    "\n",
    "# Verify all returned rows are CRITICAL\n",
    "severities = [r[\"severity\"] for r in result.collect()]\n",
    "assert all(s == \"CRITICAL\" for s in severities), \"Expected all CRITICAL\"\n",
    "print(\"[PASS] WHERE filter on Neo4j view works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-aggregations",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 4: Aggregations on Neo4j view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 4: Aggregations — Maintenance stats per aircraft\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        aircraft_id,\n",
    "        COUNT(*) AS total_events,\n",
    "        SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical,\n",
    "        SUM(CASE WHEN severity = 'MAJOR' THEN 1 ELSE 0 END) AS major,\n",
    "        SUM(CASE WHEN severity = 'MINOR' THEN 1 ELSE 0 END) AS minor\n",
    "    FROM neo4j_maintenance_events\n",
    "    GROUP BY aircraft_id\n",
    "    ORDER BY total_events DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "assert result.count() > 0, \"Expected maintenance stats\"\n",
    "print(\"[PASS] Aggregations on Neo4j view work\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-distinct",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 5: DISTINCT on Neo4j view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 5: DISTINCT — Unique airports from graph traversal\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT airport_code, airport_name\n",
    "    FROM neo4j_flight_airports\n",
    "    ORDER BY airport_code\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "count = result.count()\n",
    "print(f\"Found {count} unique airports\")\n",
    "assert count > 0, \"Expected at least one airport\"\n",
    "print(\"[PASS] DISTINCT on Neo4j graph traversal view works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-flight-group-by",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 6: GROUP BY on graph traversal view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 6: GROUP BY — Flights per departure airport\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        airport_code,\n",
    "        airport_name,\n",
    "        COUNT(*) AS departure_count\n",
    "    FROM neo4j_flight_airports\n",
    "    GROUP BY airport_code, airport_name\n",
    "    ORDER BY departure_count DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "assert result.count() > 0, \"Expected at least one airport with departures\"\n",
    "print(\"[PASS] GROUP BY on graph traversal view works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federated-tests-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Federated Queries — Views + Delta Tables\n",
    "\n",
    "The real power: JOIN Neo4j views with Delta lakehouse tables in a single query.\n",
    "These are the same federated patterns from `federated_lakehouse_query.ipynb`, but\n",
    "now using views instead of Spark Connector — purely through UC federation."
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-fleet-summary",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 1: Fleet Summary\n",
    "# =============================================================================\n",
    "# Combines Neo4j maintenance/flight counts with Delta sensor averages.\n",
    "# Equivalent to Section 2 of federated_lakehouse_query.ipynb but using views.\n",
    "\n",
    "print(\"Federated Query 1: Fleet Summary\")\n",
    "print(\"Neo4j views + Delta sensor_readings\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        maint.total_maintenance_events,\n",
    "        maint.critical_events,\n",
    "        flights.total_flights,\n",
    "        airports.unique_airports,\n",
    "        ROUND(sensor.avg_egt, 1) AS avg_egt_celsius,\n",
    "        ROUND(sensor.avg_vibration, 4) AS avg_vibration_ips,\n",
    "        ROUND(sensor.avg_fuel_flow, 2) AS avg_fuel_flow_kgs,\n",
    "        ROUND(sensor.avg_n1_speed, 0) AS avg_n1_speed_rpm,\n",
    "        sensor.total_readings\n",
    "    FROM (\n",
    "        SELECT\n",
    "            COUNT(*) AS total_maintenance_events,\n",
    "            SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical_events\n",
    "        FROM neo4j_maintenance_events\n",
    "    ) maint\n",
    "    CROSS JOIN (\n",
    "        SELECT COUNT(*) AS total_flights\n",
    "        FROM neo4j_flights\n",
    "    ) flights\n",
    "    CROSS JOIN (\n",
    "        SELECT COUNT(DISTINCT airport_code) AS unique_airports\n",
    "        FROM neo4j_flight_airports\n",
    "    ) airports\n",
    "    CROSS JOIN (\n",
    "        SELECT\n",
    "            AVG(CASE WHEN sen.type = 'EGT' THEN r.value END) AS avg_egt,\n",
    "            AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END) AS avg_vibration,\n",
    "            AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END) AS avg_fuel_flow,\n",
    "            AVG(CASE WHEN sen.type = 'N1Speed' THEN r.value END) AS avg_n1_speed,\n",
    "            COUNT(*) AS total_readings\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "    ) sensor\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "print(\"[PASS] Fleet summary federated query works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-sensor-maintenance",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 2: Sensor Health + Maintenance Correlation\n",
    "# =============================================================================\n",
    "# Per-aircraft correlation of sensor health (Delta) with maintenance events (Neo4j).\n",
    "# Equivalent to Section 3 of federated_lakehouse_query.ipynb.\n",
    "\n",
    "print(\"Federated Query 2: Sensor Health + Maintenance Correlation\")\n",
    "print(\"Delta: sensor_readings, sensors, systems, aircraft\")\n",
    "print(\"Neo4j: neo4j_maintenance_events (view)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH aircraft_ref AS (\n",
    "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
    "        FROM aircraft\n",
    "    ),\n",
    "    sensor_health AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
    "            ROUND(MAX(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS max_egt,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS avg_vibration,\n",
    "            ROUND(MAX(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS max_vibration\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        GROUP BY sys.aircraft_id\n",
    "    ),\n",
    "    maintenance_summary AS (\n",
    "        SELECT\n",
    "            aircraft_id,\n",
    "            COUNT(*) AS total_events,\n",
    "            SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical,\n",
    "            SUM(CASE WHEN severity = 'MAJOR' THEN 1 ELSE 0 END) AS major,\n",
    "            SUM(CASE WHEN severity = 'MINOR' THEN 1 ELSE 0 END) AS minor\n",
    "        FROM neo4j_maintenance_events\n",
    "        GROUP BY aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        a.tail_number,\n",
    "        a.model,\n",
    "        a.operator,\n",
    "        COALESCE(m.total_events, 0) AS maint_events,\n",
    "        COALESCE(m.critical, 0) AS critical,\n",
    "        COALESCE(m.major, 0) AS major,\n",
    "        COALESCE(m.minor, 0) AS minor,\n",
    "        s.avg_egt AS avg_egt_c,\n",
    "        s.max_egt AS max_egt_c,\n",
    "        s.avg_vibration AS avg_vib_ips,\n",
    "        s.max_vibration AS max_vib_ips\n",
    "    FROM aircraft_ref a\n",
    "    LEFT JOIN maintenance_summary m ON a.aircraft_id = m.aircraft_id\n",
    "    LEFT JOIN sensor_health s ON a.aircraft_id = s.aircraft_id\n",
    "    ORDER BY m.total_events DESC NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "result.show(20, truncate=False)\n",
    "assert result.count() > 0, \"Expected aircraft rows\"\n",
    "print(\"[PASS] Sensor + maintenance correlation works via views\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-flights-engine",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 3: Flight Operations + Engine Performance\n",
    "# =============================================================================\n",
    "# Correlates flight activity (Neo4j) with engine sensor data (Delta).\n",
    "# Equivalent to Section 4 of federated_lakehouse_query.ipynb.\n",
    "\n",
    "print(\"Federated Query 3: Flight Operations + Engine Performance\")\n",
    "print(\"Delta: sensor_readings (Engine sensors), sensors, systems, aircraft\")\n",
    "print(\"Neo4j: neo4j_flights (view)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH aircraft_ref AS (\n",
    "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, operator\n",
    "        FROM aircraft\n",
    "    ),\n",
    "    flight_activity AS (\n",
    "        SELECT\n",
    "            aircraft_id,\n",
    "            COUNT(*) AS total_flights,\n",
    "            COUNT(DISTINCT origin) AS unique_origins,\n",
    "            COUNT(DISTINCT destination) AS unique_destinations\n",
    "        FROM neo4j_flights\n",
    "        GROUP BY aircraft_id\n",
    "    ),\n",
    "    engine_health AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END), 2) AS avg_fuel_flow,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'N1Speed' THEN r.value END), 0) AS avg_n1_speed\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        WHERE sys.type = 'Engine'\n",
    "        GROUP BY sys.aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        a.tail_number,\n",
    "        a.model,\n",
    "        a.operator,\n",
    "        f.total_flights,\n",
    "        f.unique_origins AS origins,\n",
    "        f.unique_destinations AS destinations,\n",
    "        e.avg_egt AS avg_egt_c,\n",
    "        e.avg_fuel_flow AS fuel_kgs,\n",
    "        e.avg_n1_speed AS n1_rpm\n",
    "    FROM aircraft_ref a\n",
    "    JOIN flight_activity f ON a.aircraft_id = f.aircraft_id\n",
    "    JOIN engine_health e ON a.aircraft_id = e.aircraft_id\n",
    "    ORDER BY f.total_flights DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(20, truncate=False)\n",
    "assert result.count() > 0, \"Expected aircraft with flights and engine data\"\n",
    "print(\"[PASS] Flight ops + engine performance works via views\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-dashboard",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 4: Fleet Health Dashboard\n",
    "# =============================================================================\n",
    "# Comprehensive view combining ALL data sources — Delta tables + Neo4j views.\n",
    "# Equivalent to Section 5 of federated_lakehouse_query.ipynb but using only\n",
    "# UC federation (no Spark Connector).\n",
    "\n",
    "print(\"Federated Query 4: Fleet Health Dashboard\")\n",
    "print(\"Delta: sensor_readings, sensors, systems, aircraft\")\n",
    "print(\"Neo4j: neo4j_maintenance_events + neo4j_flights (views)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH aircraft_ref AS (\n",
    "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
    "        FROM aircraft\n",
    "    ),\n",
    "    sensor_stats AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS avg_vib,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END), 2) AS avg_fuel,\n",
    "            COUNT(*) AS reading_count\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        GROUP BY sys.aircraft_id\n",
    "    ),\n",
    "    maint AS (\n",
    "        SELECT\n",
    "            aircraft_id,\n",
    "            COUNT(*) AS events,\n",
    "            SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical\n",
    "        FROM neo4j_maintenance_events\n",
    "        GROUP BY aircraft_id\n",
    "    ),\n",
    "    flights AS (\n",
    "        SELECT aircraft_id, COUNT(*) AS flight_count\n",
    "        FROM neo4j_flights\n",
    "        GROUP BY aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        a.tail_number,\n",
    "        a.model,\n",
    "        a.operator,\n",
    "        COALESCE(f.flight_count, 0) AS flights,\n",
    "        COALESCE(m.events, 0) AS maint_events,\n",
    "        COALESCE(m.critical, 0) AS critical,\n",
    "        s.avg_egt AS egt_c,\n",
    "        s.avg_vib AS vib_ips,\n",
    "        s.avg_fuel AS fuel_kgs,\n",
    "        s.reading_count AS readings\n",
    "    FROM aircraft_ref a\n",
    "    LEFT JOIN flights f ON a.aircraft_id = f.aircraft_id\n",
    "    LEFT JOIN maint m ON a.aircraft_id = m.aircraft_id\n",
    "    LEFT JOIN sensor_stats s ON a.aircraft_id = s.aircraft_id\n",
    "    ORDER BY COALESCE(m.critical, 0) DESC, COALESCE(m.events, 0) DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(20, truncate=False)\n",
    "assert result.count() > 0, \"Expected fleet health rows\"\n",
    "print(\"[PASS] Fleet health dashboard works entirely through UC federation\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-high-egt-critical",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 5: High EGT Aircraft with Critical Maintenance\n",
    "# =============================================================================\n",
    "# The kind of question a Genie agent would answer:\n",
    "# \"Which aircraft with high EGT readings also had critical maintenance events?\"\n",
    "\n",
    "print(\"Federated Query 5: High EGT + Critical Maintenance\")\n",
    "print(\"Natural language equivalent: 'Which aircraft with high EGT also had critical maintenance?'\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH aircraft_egt AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(r.value), 1) AS avg_egt,\n",
    "            ROUND(MAX(r.value), 1) AS max_egt\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        WHERE sen.type = 'EGT'\n",
    "        GROUP BY sys.aircraft_id\n",
    "    ),\n",
    "    critical_maint AS (\n",
    "        SELECT\n",
    "            aircraft_id,\n",
    "            COUNT(*) AS critical_count,\n",
    "            COLLECT_LIST(fault) AS faults\n",
    "        FROM neo4j_maintenance_events\n",
    "        WHERE severity = 'CRITICAL'\n",
    "        GROUP BY aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        a.tail_number,\n",
    "        a.model,\n",
    "        a.operator,\n",
    "        e.avg_egt AS avg_egt_c,\n",
    "        e.max_egt AS max_egt_c,\n",
    "        cm.critical_count,\n",
    "        cm.faults\n",
    "    FROM aircraft a\n",
    "    JOIN aircraft_egt e ON a.`:ID(Aircraft)` = e.aircraft_id\n",
    "    JOIN critical_maint cm ON a.`:ID(Aircraft)` = cm.aircraft_id\n",
    "    ORDER BY e.avg_egt DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(20, truncate=False)\n",
    "print(f\"Found {result.count()} aircraft with both EGT data and critical maintenance\")\n",
    "print(\"[PASS] Cross-source correlation query works via views\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-route-engine-health",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 6: Route Coverage + Engine Health\n",
    "# =============================================================================\n",
    "# Uses the graph traversal view (Flight→Airport) joined with Delta sensor data.\n",
    "# \"Which departure airports see aircraft with the highest average EGT?\"\n",
    "\n",
    "print(\"Federated Query 6: Route Coverage + Engine Health\")\n",
    "print(\"Neo4j: neo4j_flight_airports (graph traversal view)\")\n",
    "print(\"Delta: sensor_readings, sensors, systems\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH airport_aircraft AS (\n",
    "        SELECT DISTINCT airport_code, airport_name, aircraft_id\n",
    "        FROM neo4j_flight_airports\n",
    "    ),\n",
    "    aircraft_egt AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(r.value), 1) AS avg_egt\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        WHERE sen.type = 'EGT'\n",
    "        GROUP BY sys.aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        aa.airport_code,\n",
    "        aa.airport_name,\n",
    "        COUNT(DISTINCT aa.aircraft_id) AS aircraft_count,\n",
    "        ROUND(AVG(e.avg_egt), 1) AS avg_fleet_egt_c\n",
    "    FROM airport_aircraft aa\n",
    "    JOIN aircraft_egt e ON aa.aircraft_id = e.aircraft_id\n",
    "    GROUP BY aa.airport_code, aa.airport_name\n",
    "    ORDER BY avg_fleet_egt_c DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "assert result.count() > 0, \"Expected airport-level engine health data\"\n",
    "print(\"[PASS] Graph traversal view + Delta sensor join works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated that **UC views over `remote_query()`** make Neo4j graph data\n",
    "fully queryable with standard SQL — enabling GROUP BY, ORDER BY, JOINs, and aggregations\n",
    "that fail with inline `remote_query()` calls.\n",
    "\n",
    "### Views Created\n",
    "\n",
    "| View | Neo4j Source | Description |\n",
    "|------|-------------|-------------|\n",
    "| `neo4j_maintenance_events` | `MaintenanceEvent` nodes | Maintenance events with severity, fault, corrective action |\n",
    "| `neo4j_flights` | `Flight` nodes | Flight operations with origin/destination |\n",
    "| `neo4j_flight_airports` | `Flight→Airport` traversal | Graph relationship traversal via NATURAL JOIN |\n",
    "\n",
    "### SQL Tests Passed\n",
    "\n",
    "| Test | Operation | Status |\n",
    "|------|-----------|--------|\n",
    "| 1 | GROUP BY on Neo4j view | PASS |\n",
    "| 2 | ORDER BY on Neo4j view | PASS |\n",
    "| 3 | WHERE filter on Neo4j view | PASS |\n",
    "| 4 | Aggregations (COUNT, SUM, CASE) | PASS |\n",
    "| 5 | DISTINCT on graph traversal view | PASS |\n",
    "| 6 | GROUP BY on graph traversal view | PASS |\n",
    "\n",
    "### Federated Queries Passed\n",
    "\n",
    "| Query | Neo4j Views | Delta Tables | Equivalent |\n",
    "|-------|------------|-------------|------------|\n",
    "| Fleet Summary | all 3 views | sensor_readings, sensors | Section 2 of federated_lakehouse_query |\n",
    "| Sensor + Maintenance | maintenance_events | all 4 Delta tables | Section 3 |\n",
    "| Flight Ops + Engine | flights | sensor_readings, sensors, systems | Section 4 |\n",
    "| Fleet Health Dashboard | maintenance + flights | all 4 Delta tables | Section 5 |\n",
    "| High EGT + Critical Maint | maintenance_events | sensor_readings, sensors, systems | New |\n",
    "| Route + Engine Health | flight_airports | sensor_readings, sensors, systems | New |\n",
    "\n",
    "### Agent-Ready\n",
    "\n",
    "These views make the data **Genie-ready**. A Genie space configured with all 7 tables/views\n",
    "can answer natural language questions that transparently federate across Neo4j and Delta:\n",
    "\n",
    "```\n",
    "NL → SQL (Genie) → Spark SQL → Delta (direct) + remote_query() → JDBC → Cypher → Neo4j\n",
    "```\n",
    "\n",
    "No Spark Connector. No Python drivers. Pure UC federation.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Create **permanent views** (not TEMPORARY) in Unity Catalog for production use\n",
    "- Set up a **Genie space** with all 7 tables/views and example SQL instructions\n",
    "- Connect Genie to an **agent** via MCP server or Conversation API\n",
    "- See [FEDERATED_AGENTS.md](../FEDERATED_AGENTS.md) for the full agent architecture\n",
    "\n",
    "### References\n",
    "\n",
    "- [GUIDE_NEO4J_UC.md](../GUIDE_NEO4J_UC.md) — Full UC JDBC integration guide\n",
    "- [FEDERATED_AGENTS.md](../FEDERATED_AGENTS.md) — Agent architecture with federation\n",
    "- [federated_lakehouse_query.ipynb](federated_lakehouse_query.ipynb) — Original federated queries (Spark Connector + remote_query)\n",
    "- [Neo4j JDBC SQL2Cypher](https://neo4j.com/docs/jdbc-manual/current/sql2cypher/) — SQL translation rules\n",
    "- [Databricks remote_query()](https://docs.databricks.com/sql/language-manual/functions/remote_query) — Table-valued function reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}