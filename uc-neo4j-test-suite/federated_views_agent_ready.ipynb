{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Federated Views: Agent-Ready Neo4j + Delta Lakehouse\n",
    "\n",
    "Creates Unity Catalog views that encapsulate `remote_query()` calls to Neo4j, making\n",
    "graph data queryable as regular UC tables. These views enable **Genie** (or any SQL tool)\n",
    "to transparently federate queries across Neo4j and Delta lakehouse without Spark Connector\n",
    "or direct Python drivers.\n",
    "\n",
    "The full chain: **Natural Language → SQL (Genie) → Cypher (JDBC driver) → Neo4j** — all\n",
    "through Unity Catalog federation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture",
   "metadata": {},
   "source": "## Architecture\n\n```\n┌──────────────────────────────────────────────────────────────────────┐\n│                  Genie / SQL Tool / Agent                           │\n│           (natural language or SQL queries)                         │\n└─────────────────────────────┬────────────────────────────────────────┘\n                              │\n                              ▼\n┌──────────────────────────────────────────────────────────────────────┐\n│                      Spark SQL Engine                               │\n│                                                                      │\n│   Queries run against UC tables (all look like regular tables):     │\n│   - aircraft, systems, sensors, sensor_readings  (Delta — direct)   │\n│   - neo4j_maintenance_events  (materialized from Neo4j via JDBC)    │\n│   - neo4j_flights             (materialized from Neo4j via JDBC)    │\n│   - neo4j_airports            (materialized from Neo4j via JDBC)    │\n│   - neo4j_flight_airports     (JOIN of flights + airports)          │\n│                                                                      │\n├────────────────────────────┬─────────────────────────────────────────┤\n│  Delta Lakehouse (direct)  │  Neo4j (materialized via JDBC dbtable) │\n│                            │  Read via dbtable + customSchema       │\n│                            │  Written as managed Delta tables       │\n│                            │  Re-run notebook to refresh            │\n└────────────────────────────┴─────────────────────────────────────────┘\n```\n\n**Key advantage:** GROUP BY, ORDER BY, and JOINs all work because the Neo4j data is\nmaterialized as regular Delta tables. Genie sees all 8 tables identically.\n\n**Why materialized?** Neo4j JDBC has two schema inference issues that prevent live\n`CREATE VIEW` over `remote_query()`:\n1. `query` option: Spark wraps in subquery → Neo4j can't parse it\n2. `dbtable` option: Neo4j returns NullType → requires `customSchema` (DataFrame API only)"
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. **Lakehouse tables** already exist in your catalog (created by lab setup):\n",
    "   `aircraft`, `systems`, `sensors`, `sensor_readings`\n",
    "\n",
    "2. **Neo4j UC JDBC connection** configured per [GUIDE_NEO4J_UC.md](../GUIDE_NEO4J_UC.md)\n",
    "\n",
    "3. **Cluster configuration:**\n",
    "   - SafeSpark memory settings applied (see guide)\n",
    "   - `neo4j-uc-creds` secret scope configured via `setup.sh`\n",
    "\n",
    "4. **Databricks preview features** enabled:\n",
    "   - Custom JDBC on UC Compute\n",
    "   - `remote_query` table-valued function\n",
    "\n",
    "**Note:** No Spark Connector or cluster libraries required — this notebook uses\n",
    "pure UC JDBC federation only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "config",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "SCOPE_NAME = \"neo4j-uc-creds\"\n",
    "\n",
    "# Lakehouse configuration — update to match your environment\n",
    "LAKEHOUSE_CATALOG = \"aws-databricks-neo4j-lab\"   # Your Unity Catalog name\n",
    "LAKEHOUSE_SCHEMA = \"lakehouse\"                    # Schema containing Delta tables\n",
    "\n",
    "# Neo4j credentials from Databricks Secrets\n",
    "NEO4J_HOST = dbutils.secrets.get(SCOPE_NAME, \"host\")\n",
    "NEO4J_USER = dbutils.secrets.get(SCOPE_NAME, \"user\")\n",
    "NEO4J_PASSWORD = dbutils.secrets.get(SCOPE_NAME, \"password\")\n",
    "try:\n",
    "    NEO4J_DATABASE = dbutils.secrets.get(SCOPE_NAME, \"database\")\n",
    "except Exception:\n",
    "    NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "UC_CONNECTION_NAME = dbutils.secrets.get(SCOPE_NAME, \"connection_name\")\n",
    "\n",
    "# Set catalog and schema context\n",
    "spark.sql(f\"USE CATALOG `{LAKEHOUSE_CATALOG}`\")\n",
    "spark.sql(f\"USE SCHEMA `{LAKEHOUSE_SCHEMA}`\")\n",
    "\n",
    "print(f\"Lakehouse: {LAKEHOUSE_CATALOG}.{LAKEHOUSE_SCHEMA}\")\n",
    "print(f\"Neo4j Host: {NEO4J_HOST}\")\n",
    "print(f\"UC Connection: {UC_CONNECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Verify Data Sources\n",
    "\n",
    "Confirm both the lakehouse tables and Neo4j UC connection are accessible."
   ]
  },
  {
   "cell_type": "code",
   "id": "verify-lakehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify Delta lakehouse tables\n",
    "print(\"=\" * 60)\n",
    "print(\"DELTA LAKEHOUSE TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table in [\"aircraft\", \"systems\", \"sensors\", \"sensor_readings\"]:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {table}\").collect()[0][\"cnt\"]\n",
    "    print(f\"  {table}: {count:,} rows\")\n",
    "\n",
    "print(\"\\nSample aircraft data:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
    "    FROM aircraft LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "id": "verify-neo4j",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify Neo4j UC JDBC connection\n",
    "print(\"=\" * 60)\n",
    "print(\"NEO4J KNOWLEDGE GRAPH (via UC JDBC)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "neo4j_counts = {\n",
    "    \"Aircraft\": \"SELECT COUNT(*) AS cnt FROM Aircraft\",\n",
    "    \"MaintenanceEvent\": \"SELECT COUNT(*) AS cnt FROM MaintenanceEvent\",\n",
    "    \"Flight\": \"SELECT COUNT(*) AS cnt FROM Flight\",\n",
    "    \"Airport\": \"SELECT COUNT(*) AS cnt FROM Airport\",\n",
    "}\n",
    "\n",
    "for label, query in neo4j_counts.items():\n",
    "    result = spark.sql(f\"\"\"\n",
    "        SELECT * FROM remote_query('{UC_CONNECTION_NAME}', query => '{query}')\n",
    "    \"\"\").collect()\n",
    "    print(f\"  {label}: {result[0]['cnt']:,} nodes\")\n",
    "\n",
    "print(\"\\nBoth data sources verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "views-header",
   "metadata": {},
   "source": "---\n\n## Section 2: Materialize Neo4j Data as UC Delta Tables\n\nEach Neo4j label is read via the DataFrame API with `dbtable` + `customSchema`, then\nwritten as a **managed Delta table** in Unity Catalog. These tables are visible in\nCatalog Explorer and can be added to a Genie space for natural language querying.\n\n**Why `dbtable` + `customSchema`?** Two Neo4j JDBC limitations require this approach:\n1. The `query` option triggers Spark's subquery wrapping for schema inference, which\n   Neo4j's SQL translator cannot handle.\n2. The `dbtable` option avoids subquery wrapping but Neo4j JDBC returns `NullType`\n   during schema inference, requiring `customSchema` to specify column types explicitly.\n\n**Why materialized tables?** The `customSchema` fix is only available on the DataFrame\nAPI (`spark.read.format(\"jdbc\")`), not on `remote_query()`. Since `CREATE VIEW` requires\n`remote_query()`, we materialize the data as Delta tables instead. Re-run this section\nto refresh data from Neo4j."
  },
  {
   "cell_type": "code",
   "id": "create-maintenance-view",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# =============================================================================\n# TABLE: neo4j_maintenance_events (materialized Delta table)\n# =============================================================================\n# Reads MaintenanceEvent nodes from Neo4j via dbtable + customSchema, then\n# writes as a managed Delta table in Unity Catalog.\n# customSchema is required because Neo4j JDBC returns NullType during inference.\n# Re-run this cell to refresh data from Neo4j.\n\nMAINTENANCE_SCHEMA = \"\"\"`v$id` STRING, aircraft_id STRING, system_id STRING, component_id STRING,\n    event_id STRING, severity STRING, fault STRING, corrective_action STRING, reported_at STRING\"\"\"\n\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n    .option(\"dbtable\", \"MaintenanceEvent\") \\\n    .option(\"customSchema\", MAINTENANCE_SCHEMA) \\\n    .load() \\\n    .select(\"aircraft_id\", \"fault\", \"severity\", \"corrective_action\", \"reported_at\")\n\ndf.write.mode(\"overwrite\").saveAsTable(\n    f\"`{LAKEHOUSE_CATALOG}`.`{LAKEHOUSE_SCHEMA}`.neo4j_maintenance_events\"\n)\n\ncount = spark.sql(\"SELECT COUNT(*) AS cnt FROM neo4j_maintenance_events\").collect()[0][\"cnt\"]\nprint(f\"neo4j_maintenance_events: {count:,} rows\")\n\nprint(\"\\nSchema:\")\nspark.sql(\"SELECT * FROM neo4j_maintenance_events LIMIT 1\").printSchema()\n\nprint(\"Sample data:\")\nspark.sql(\"\"\"\n    SELECT aircraft_id, fault, severity, corrective_action\n    FROM neo4j_maintenance_events LIMIT 5\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "id": "create-flights-view",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# =============================================================================\n# TABLE: neo4j_flights (materialized Delta table)\n# =============================================================================\n# Reads Flight nodes from Neo4j via dbtable + customSchema, then writes as a\n# managed Delta table. Re-run this cell to refresh data from Neo4j.\n\nFLIGHT_SCHEMA = \"\"\"`v$id` STRING, aircraft_id STRING, flight_id STRING, operator STRING,\n    flight_number STRING, origin STRING, destination STRING,\n    scheduled_departure STRING, scheduled_arrival STRING\"\"\"\n\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n    .option(\"dbtable\", \"Flight\") \\\n    .option(\"customSchema\", FLIGHT_SCHEMA) \\\n    .load() \\\n    .select(\"aircraft_id\", \"flight_number\", \"operator\", \"origin\", \"destination\",\n            \"scheduled_departure\", \"scheduled_arrival\")\n\ndf.write.mode(\"overwrite\").saveAsTable(\n    f\"`{LAKEHOUSE_CATALOG}`.`{LAKEHOUSE_SCHEMA}`.neo4j_flights\"\n)\n\ncount = spark.sql(\"SELECT COUNT(*) AS cnt FROM neo4j_flights\").collect()[0][\"cnt\"]\nprint(f\"neo4j_flights: {count:,} rows\")\n\nprint(\"\\nSchema:\")\nspark.sql(\"SELECT * FROM neo4j_flights LIMIT 1\").printSchema()\n\nprint(\"Sample data:\")\nspark.sql(\"\"\"\n    SELECT aircraft_id, flight_number, operator, origin, destination\n    FROM neo4j_flights LIMIT 5\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "id": "create-flight-airports-view",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# =============================================================================\n# TABLES: neo4j_airports + neo4j_flight_airports (materialized Delta tables)\n# =============================================================================\n# Reads Airport nodes from Neo4j via dbtable + customSchema, then creates a\n# flight-to-airport mapping table via Spark SQL JOIN.\n# Re-run this cell to refresh data from Neo4j.\n\nfrom pyspark.sql.functions import col\n\nAIRPORT_SCHEMA = \"\"\"`v$id` STRING, airport_id STRING, name STRING, city STRING,\n    country STRING, iata STRING, icao STRING, lat STRING, lon STRING\"\"\"\n\nairports_df = spark.read.format(\"jdbc\") \\\n    .option(\"databricks.connection\", UC_CONNECTION_NAME) \\\n    .option(\"dbtable\", \"Airport\") \\\n    .option(\"customSchema\", AIRPORT_SCHEMA) \\\n    .load() \\\n    .select(\"iata\", col(\"name\").alias(\"airport_name\"), \"city\", \"country\", \"icao\", \"lat\", \"lon\")\n\nairports_df.write.mode(\"overwrite\").saveAsTable(\n    f\"`{LAKEHOUSE_CATALOG}`.`{LAKEHOUSE_SCHEMA}`.neo4j_airports\"\n)\n\n# Create flight_airports as a JOIN of the two materialized tables\nspark.sql(f\"\"\"\n    CREATE OR REPLACE TABLE `{LAKEHOUSE_CATALOG}`.`{LAKEHOUSE_SCHEMA}`.neo4j_flight_airports AS\n    SELECT f.flight_number, f.aircraft_id, a.iata AS airport_code, a.airport_name\n    FROM `{LAKEHOUSE_CATALOG}`.`{LAKEHOUSE_SCHEMA}`.neo4j_flights f\n    JOIN `{LAKEHOUSE_CATALOG}`.`{LAKEHOUSE_SCHEMA}`.neo4j_airports a ON f.origin = a.iata\n\"\"\")\n\ncount = spark.sql(\"SELECT COUNT(*) AS cnt FROM neo4j_flight_airports\").collect()[0][\"cnt\"]\nprint(f\"neo4j_flight_airports: {count:,} rows\")\n\nprint(\"\\nSchema:\")\nspark.sql(\"SELECT * FROM neo4j_flight_airports LIMIT 1\").printSchema()\n\nprint(\"Sample data:\")\nspark.sql(\"\"\"\n    SELECT flight_number, aircraft_id, airport_code, airport_name\n    FROM neo4j_flight_airports LIMIT 5\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "id": "verify-all-views",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Verify all materialized Neo4j tables are accessible\nprint(\"=\" * 60)\nprint(\"ALL NEO4J TABLES MATERIALIZED\")\nprint(\"=\" * 60)\nprint(f\"Catalog: {LAKEHOUSE_CATALOG}.{LAKEHOUSE_SCHEMA}\\n\")\n\nneo4j_tables = {\n    \"neo4j_maintenance_events\": \"Neo4j MaintenanceEvent nodes\",\n    \"neo4j_flights\": \"Neo4j Flight nodes\",\n    \"neo4j_airports\": \"Neo4j Airport nodes\",\n    \"neo4j_flight_airports\": \"Spark SQL JOIN of flights + airports\",\n}\n\nfor table_name, description in neo4j_tables.items():\n    count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {table_name}\").collect()[0][\"cnt\"]\n    print(f\"  {table_name}: {count:,} rows  ({description})\")\n\nprint(\"\\nDelta tables (direct from lakehouse):\")\nfor table in [\"aircraft\", \"systems\", \"sensors\", \"sensor_readings\"]:\n    count = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {table}\").collect()[0][\"cnt\"]\n    print(f\"  {table}: {count:,} rows\")\n\nprint(f\"\\nAll 8 tables are ready for federated queries and Genie.\")"
  },
  {
   "cell_type": "markdown",
   "id": "sql-tests-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: SQL Tests Against Views\n",
    "\n",
    "Test that the Neo4j views support standard SQL operations — GROUP BY, ORDER BY,\n",
    "WHERE filters, aggregations — that would fail with inline `remote_query()` calls.\n",
    "These are the operations Genie needs to generate."
   ]
  },
  {
   "cell_type": "code",
   "id": "test-group-by",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 1: GROUP BY on Neo4j view\n",
    "# =============================================================================\n",
    "# This fails with inline remote_query() but works with views.\n",
    "\n",
    "print(\"TEST 1: GROUP BY — Maintenance events by severity\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        severity,\n",
    "        COUNT(*) AS event_count\n",
    "    FROM neo4j_maintenance_events\n",
    "    GROUP BY severity\n",
    "    ORDER BY event_count DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "assert result.count() > 0, \"Expected at least one severity level\"\n",
    "print(\"[PASS] GROUP BY on Neo4j view works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-order-by",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 2: ORDER BY on Neo4j view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 2: ORDER BY — Flights ordered by flight_number\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT flight_number, aircraft_id, operator, origin, destination\n",
    "    FROM neo4j_flights\n",
    "    ORDER BY flight_number\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "rows = result.collect()\n",
    "flight_numbers = [r[\"flight_number\"] for r in rows]\n",
    "assert flight_numbers == sorted(flight_numbers), \"Expected sorted flight numbers\"\n",
    "print(\"[PASS] ORDER BY on Neo4j view works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-where-filter",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 3: WHERE filter on Neo4j view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 3: WHERE — Critical maintenance events only\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT aircraft_id, fault, severity, corrective_action\n",
    "    FROM neo4j_maintenance_events\n",
    "    WHERE severity = 'CRITICAL'\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "count = result.count()\n",
    "print(f\"Found {count} CRITICAL events\")\n",
    "\n",
    "# Verify all returned rows are CRITICAL\n",
    "severities = [r[\"severity\"] for r in result.collect()]\n",
    "assert all(s == \"CRITICAL\" for s in severities), \"Expected all CRITICAL\"\n",
    "print(\"[PASS] WHERE filter on Neo4j view works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-aggregations",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 4: Aggregations on Neo4j view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 4: Aggregations — Maintenance stats per aircraft\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        aircraft_id,\n",
    "        COUNT(*) AS total_events,\n",
    "        SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical,\n",
    "        SUM(CASE WHEN severity = 'MAJOR' THEN 1 ELSE 0 END) AS major,\n",
    "        SUM(CASE WHEN severity = 'MINOR' THEN 1 ELSE 0 END) AS minor\n",
    "    FROM neo4j_maintenance_events\n",
    "    GROUP BY aircraft_id\n",
    "    ORDER BY total_events DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "assert result.count() > 0, \"Expected maintenance stats\"\n",
    "print(\"[PASS] Aggregations on Neo4j view work\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-distinct",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 5: DISTINCT on Neo4j view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 5: DISTINCT — Unique airports from graph traversal\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT airport_code, airport_name\n",
    "    FROM neo4j_flight_airports\n",
    "    ORDER BY airport_code\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "count = result.count()\n",
    "print(f\"Found {count} unique airports\")\n",
    "assert count > 0, \"Expected at least one airport\"\n",
    "print(\"[PASS] DISTINCT on Neo4j graph traversal view works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "test-flight-group-by",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST 6: GROUP BY on graph traversal view\n",
    "# =============================================================================\n",
    "\n",
    "print(\"TEST 6: GROUP BY — Flights per departure airport\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        airport_code,\n",
    "        airport_name,\n",
    "        COUNT(*) AS departure_count\n",
    "    FROM neo4j_flight_airports\n",
    "    GROUP BY airport_code, airport_name\n",
    "    ORDER BY departure_count DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "assert result.count() > 0, \"Expected at least one airport with departures\"\n",
    "print(\"[PASS] GROUP BY on graph traversal view works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federated-tests-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Federated Queries — Views + Delta Tables\n",
    "\n",
    "The real power: JOIN Neo4j views with Delta lakehouse tables in a single query.\n",
    "These are the same federated patterns from `federated_lakehouse_query.ipynb`, but\n",
    "now using views instead of Spark Connector — purely through UC federation."
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-fleet-summary",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 1: Fleet Summary\n",
    "# =============================================================================\n",
    "# Combines Neo4j maintenance/flight counts with Delta sensor averages.\n",
    "# Equivalent to Section 2 of federated_lakehouse_query.ipynb but using views.\n",
    "\n",
    "print(\"Federated Query 1: Fleet Summary\")\n",
    "print(\"Neo4j views + Delta sensor_readings\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        maint.total_maintenance_events,\n",
    "        maint.critical_events,\n",
    "        flights.total_flights,\n",
    "        airports.unique_airports,\n",
    "        ROUND(sensor.avg_egt, 1) AS avg_egt_celsius,\n",
    "        ROUND(sensor.avg_vibration, 4) AS avg_vibration_ips,\n",
    "        ROUND(sensor.avg_fuel_flow, 2) AS avg_fuel_flow_kgs,\n",
    "        ROUND(sensor.avg_n1_speed, 0) AS avg_n1_speed_rpm,\n",
    "        sensor.total_readings\n",
    "    FROM (\n",
    "        SELECT\n",
    "            COUNT(*) AS total_maintenance_events,\n",
    "            SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical_events\n",
    "        FROM neo4j_maintenance_events\n",
    "    ) maint\n",
    "    CROSS JOIN (\n",
    "        SELECT COUNT(*) AS total_flights\n",
    "        FROM neo4j_flights\n",
    "    ) flights\n",
    "    CROSS JOIN (\n",
    "        SELECT COUNT(DISTINCT airport_code) AS unique_airports\n",
    "        FROM neo4j_flight_airports\n",
    "    ) airports\n",
    "    CROSS JOIN (\n",
    "        SELECT\n",
    "            AVG(CASE WHEN sen.type = 'EGT' THEN r.value END) AS avg_egt,\n",
    "            AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END) AS avg_vibration,\n",
    "            AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END) AS avg_fuel_flow,\n",
    "            AVG(CASE WHEN sen.type = 'N1Speed' THEN r.value END) AS avg_n1_speed,\n",
    "            COUNT(*) AS total_readings\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "    ) sensor\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "print(\"[PASS] Fleet summary federated query works\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-sensor-maintenance",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 2: Sensor Health + Maintenance Correlation\n",
    "# =============================================================================\n",
    "# Per-aircraft correlation of sensor health (Delta) with maintenance events (Neo4j).\n",
    "# Equivalent to Section 3 of federated_lakehouse_query.ipynb.\n",
    "\n",
    "print(\"Federated Query 2: Sensor Health + Maintenance Correlation\")\n",
    "print(\"Delta: sensor_readings, sensors, systems, aircraft\")\n",
    "print(\"Neo4j: neo4j_maintenance_events (view)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH aircraft_ref AS (\n",
    "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
    "        FROM aircraft\n",
    "    ),\n",
    "    sensor_health AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
    "            ROUND(MAX(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS max_egt,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS avg_vibration,\n",
    "            ROUND(MAX(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS max_vibration\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        GROUP BY sys.aircraft_id\n",
    "    ),\n",
    "    maintenance_summary AS (\n",
    "        SELECT\n",
    "            aircraft_id,\n",
    "            COUNT(*) AS total_events,\n",
    "            SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical,\n",
    "            SUM(CASE WHEN severity = 'MAJOR' THEN 1 ELSE 0 END) AS major,\n",
    "            SUM(CASE WHEN severity = 'MINOR' THEN 1 ELSE 0 END) AS minor\n",
    "        FROM neo4j_maintenance_events\n",
    "        GROUP BY aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        a.tail_number,\n",
    "        a.model,\n",
    "        a.operator,\n",
    "        COALESCE(m.total_events, 0) AS maint_events,\n",
    "        COALESCE(m.critical, 0) AS critical,\n",
    "        COALESCE(m.major, 0) AS major,\n",
    "        COALESCE(m.minor, 0) AS minor,\n",
    "        s.avg_egt AS avg_egt_c,\n",
    "        s.max_egt AS max_egt_c,\n",
    "        s.avg_vibration AS avg_vib_ips,\n",
    "        s.max_vibration AS max_vib_ips\n",
    "    FROM aircraft_ref a\n",
    "    LEFT JOIN maintenance_summary m ON a.aircraft_id = m.aircraft_id\n",
    "    LEFT JOIN sensor_health s ON a.aircraft_id = s.aircraft_id\n",
    "    ORDER BY m.total_events DESC NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "result.show(20, truncate=False)\n",
    "assert result.count() > 0, \"Expected aircraft rows\"\n",
    "print(\"[PASS] Sensor + maintenance correlation works via views\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-flights-engine",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 3: Flight Operations + Engine Performance\n",
    "# =============================================================================\n",
    "# Correlates flight activity (Neo4j) with engine sensor data (Delta).\n",
    "# Equivalent to Section 4 of federated_lakehouse_query.ipynb.\n",
    "\n",
    "print(\"Federated Query 3: Flight Operations + Engine Performance\")\n",
    "print(\"Delta: sensor_readings (Engine sensors), sensors, systems, aircraft\")\n",
    "print(\"Neo4j: neo4j_flights (view)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH aircraft_ref AS (\n",
    "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, operator\n",
    "        FROM aircraft\n",
    "    ),\n",
    "    flight_activity AS (\n",
    "        SELECT\n",
    "            aircraft_id,\n",
    "            COUNT(*) AS total_flights,\n",
    "            COUNT(DISTINCT origin) AS unique_origins,\n",
    "            COUNT(DISTINCT destination) AS unique_destinations\n",
    "        FROM neo4j_flights\n",
    "        GROUP BY aircraft_id\n",
    "    ),\n",
    "    engine_health AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END), 2) AS avg_fuel_flow,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'N1Speed' THEN r.value END), 0) AS avg_n1_speed\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        WHERE sys.type = 'Engine'\n",
    "        GROUP BY sys.aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        a.tail_number,\n",
    "        a.model,\n",
    "        a.operator,\n",
    "        f.total_flights,\n",
    "        f.unique_origins AS origins,\n",
    "        f.unique_destinations AS destinations,\n",
    "        e.avg_egt AS avg_egt_c,\n",
    "        e.avg_fuel_flow AS fuel_kgs,\n",
    "        e.avg_n1_speed AS n1_rpm\n",
    "    FROM aircraft_ref a\n",
    "    JOIN flight_activity f ON a.aircraft_id = f.aircraft_id\n",
    "    JOIN engine_health e ON a.aircraft_id = e.aircraft_id\n",
    "    ORDER BY f.total_flights DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(20, truncate=False)\n",
    "assert result.count() > 0, \"Expected aircraft with flights and engine data\"\n",
    "print(\"[PASS] Flight ops + engine performance works via views\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-dashboard",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 4: Fleet Health Dashboard\n",
    "# =============================================================================\n",
    "# Comprehensive view combining ALL data sources — Delta tables + Neo4j views.\n",
    "# Equivalent to Section 5 of federated_lakehouse_query.ipynb but using only\n",
    "# UC federation (no Spark Connector).\n",
    "\n",
    "print(\"Federated Query 4: Fleet Health Dashboard\")\n",
    "print(\"Delta: sensor_readings, sensors, systems, aircraft\")\n",
    "print(\"Neo4j: neo4j_maintenance_events + neo4j_flights (views)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH aircraft_ref AS (\n",
    "        SELECT `:ID(Aircraft)` AS aircraft_id, tail_number, model, manufacturer, operator\n",
    "        FROM aircraft\n",
    "    ),\n",
    "    sensor_stats AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'EGT' THEN r.value END), 1) AS avg_egt,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'Vibration' THEN r.value END), 4) AS avg_vib,\n",
    "            ROUND(AVG(CASE WHEN sen.type = 'FuelFlow' THEN r.value END), 2) AS avg_fuel,\n",
    "            COUNT(*) AS reading_count\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        GROUP BY sys.aircraft_id\n",
    "    ),\n",
    "    maint AS (\n",
    "        SELECT\n",
    "            aircraft_id,\n",
    "            COUNT(*) AS events,\n",
    "            SUM(CASE WHEN severity = 'CRITICAL' THEN 1 ELSE 0 END) AS critical\n",
    "        FROM neo4j_maintenance_events\n",
    "        GROUP BY aircraft_id\n",
    "    ),\n",
    "    flights AS (\n",
    "        SELECT aircraft_id, COUNT(*) AS flight_count\n",
    "        FROM neo4j_flights\n",
    "        GROUP BY aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        a.tail_number,\n",
    "        a.model,\n",
    "        a.operator,\n",
    "        COALESCE(f.flight_count, 0) AS flights,\n",
    "        COALESCE(m.events, 0) AS maint_events,\n",
    "        COALESCE(m.critical, 0) AS critical,\n",
    "        s.avg_egt AS egt_c,\n",
    "        s.avg_vib AS vib_ips,\n",
    "        s.avg_fuel AS fuel_kgs,\n",
    "        s.reading_count AS readings\n",
    "    FROM aircraft_ref a\n",
    "    LEFT JOIN flights f ON a.aircraft_id = f.aircraft_id\n",
    "    LEFT JOIN maint m ON a.aircraft_id = m.aircraft_id\n",
    "    LEFT JOIN sensor_stats s ON a.aircraft_id = s.aircraft_id\n",
    "    ORDER BY COALESCE(m.critical, 0) DESC, COALESCE(m.events, 0) DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(20, truncate=False)\n",
    "assert result.count() > 0, \"Expected fleet health rows\"\n",
    "print(\"[PASS] Fleet health dashboard works entirely through UC federation\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-high-egt-critical",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 5: High EGT Aircraft with Critical Maintenance\n",
    "# =============================================================================\n",
    "# The kind of question a Genie agent would answer:\n",
    "# \"Which aircraft with high EGT readings also had critical maintenance events?\"\n",
    "\n",
    "print(\"Federated Query 5: High EGT + Critical Maintenance\")\n",
    "print(\"Natural language equivalent: 'Which aircraft with high EGT also had critical maintenance?'\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH aircraft_egt AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(r.value), 1) AS avg_egt,\n",
    "            ROUND(MAX(r.value), 1) AS max_egt\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        WHERE sen.type = 'EGT'\n",
    "        GROUP BY sys.aircraft_id\n",
    "    ),\n",
    "    critical_maint AS (\n",
    "        SELECT\n",
    "            aircraft_id,\n",
    "            COUNT(*) AS critical_count,\n",
    "            COLLECT_LIST(fault) AS faults\n",
    "        FROM neo4j_maintenance_events\n",
    "        WHERE severity = 'CRITICAL'\n",
    "        GROUP BY aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        a.tail_number,\n",
    "        a.model,\n",
    "        a.operator,\n",
    "        e.avg_egt AS avg_egt_c,\n",
    "        e.max_egt AS max_egt_c,\n",
    "        cm.critical_count,\n",
    "        cm.faults\n",
    "    FROM aircraft a\n",
    "    JOIN aircraft_egt e ON a.`:ID(Aircraft)` = e.aircraft_id\n",
    "    JOIN critical_maint cm ON a.`:ID(Aircraft)` = cm.aircraft_id\n",
    "    ORDER BY e.avg_egt DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(20, truncate=False)\n",
    "print(f\"Found {result.count()} aircraft with both EGT data and critical maintenance\")\n",
    "print(\"[PASS] Cross-source correlation query works via views\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "federated-route-engine-health",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEDERATED QUERY 6: Route Coverage + Engine Health\n",
    "# =============================================================================\n",
    "# Uses the graph traversal view (Flight→Airport) joined with Delta sensor data.\n",
    "# \"Which departure airports see aircraft with the highest average EGT?\"\n",
    "\n",
    "print(\"Federated Query 6: Route Coverage + Engine Health\")\n",
    "print(\"Neo4j: neo4j_flight_airports (graph traversal view)\")\n",
    "print(\"Delta: sensor_readings, sensors, systems\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH airport_aircraft AS (\n",
    "        SELECT DISTINCT airport_code, airport_name, aircraft_id\n",
    "        FROM neo4j_flight_airports\n",
    "    ),\n",
    "    aircraft_egt AS (\n",
    "        SELECT\n",
    "            sys.aircraft_id,\n",
    "            ROUND(AVG(r.value), 1) AS avg_egt\n",
    "        FROM sensor_readings r\n",
    "        JOIN sensors sen ON r.sensor_id = sen.`:ID(Sensor)`\n",
    "        JOIN systems sys ON sen.system_id = sys.`:ID(System)`\n",
    "        WHERE sen.type = 'EGT'\n",
    "        GROUP BY sys.aircraft_id\n",
    "    )\n",
    "    SELECT\n",
    "        aa.airport_code,\n",
    "        aa.airport_name,\n",
    "        COUNT(DISTINCT aa.aircraft_id) AS aircraft_count,\n",
    "        ROUND(AVG(e.avg_egt), 1) AS avg_fleet_egt_c\n",
    "    FROM airport_aircraft aa\n",
    "    JOIN aircraft_egt e ON aa.aircraft_id = e.aircraft_id\n",
    "    GROUP BY aa.airport_code, aa.airport_name\n",
    "    ORDER BY avg_fleet_egt_c DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)\n",
    "assert result.count() > 0, \"Expected airport-level engine health data\"\n",
    "print(\"[PASS] Graph traversal view + Delta sensor join works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n\n## Summary\n\nThis notebook materializes Neo4j graph data as **managed Delta tables** in Unity Catalog,\nmaking it queryable alongside lakehouse tables with standard SQL — GROUP BY, ORDER BY,\nJOINs, and aggregations all work seamlessly.\n\n### Neo4j Tables Materialized\n\n| Table | Neo4j Source | Description |\n|-------|-------------|-------------|\n| `neo4j_maintenance_events` | `MaintenanceEvent` nodes | Maintenance events with severity, fault, corrective action |\n| `neo4j_flights` | `Flight` nodes | Flight operations with origin/destination |\n| `neo4j_airports` | `Airport` nodes | Airport reference data (iata, name, city, country) |\n| `neo4j_flight_airports` | Flights + Airports | Flight-to-departure-airport mapping |\n\nAll tables are in: `aws-databricks-neo4j-lab.lakehouse`\n\n### SQL Tests Passed\n\n| Test | Operation | Status |\n|------|-----------|--------|\n| 1 | GROUP BY on Neo4j table | PASS |\n| 2 | ORDER BY on Neo4j table | PASS |\n| 3 | WHERE filter on Neo4j table | PASS |\n| 4 | Aggregations (COUNT, SUM, CASE) | PASS |\n| 5 | DISTINCT on flight-airport table | PASS |\n| 6 | GROUP BY on flight-airport table | PASS |\n\n### Federated Queries Passed\n\n| Query | Neo4j Tables | Delta Tables | Equivalent |\n|-------|-------------|-------------|------------|\n| Fleet Summary | all tables | sensor_readings, sensors | Section 2 of federated_lakehouse_query |\n| Sensor + Maintenance | maintenance_events | all 4 Delta tables | Section 3 |\n| Flight Ops + Engine | flights | sensor_readings, sensors, systems | Section 4 |\n| Fleet Health Dashboard | maintenance + flights | all 4 Delta tables | Section 5 |\n| High EGT + Critical Maint | maintenance_events | sensor_readings, sensors, systems | New |\n| Route + Engine Health | flight_airports | sensor_readings, sensors, systems | New |\n\n### Agent-Ready\n\nThese materialized tables make the data **Genie-ready**. A Genie space configured with\nall 8 tables can answer natural language questions that federate across Neo4j and Delta:\n\n```\nNL → SQL (Genie) → Spark SQL → Delta tables (lakehouse + materialized Neo4j data)\n```\n\nNo Spark Connector. No Python drivers. Re-run this notebook to refresh Neo4j data.\n\n### Next Steps\n\n- Set up a **Genie space** with all 8 tables and example SQL instructions\n- Connect Genie to an **agent** via MCP server or Conversation API\n- See [FEDERATED_AGENTS.md](../FEDERATED_AGENTS.md) for the full agent architecture\n\n### References\n\n- [GUIDE_NEO4J_UC.md](../GUIDE_NEO4J_UC.md) — Full UC JDBC integration guide\n- [FEDERATED_AGENTS.md](../FEDERATED_AGENTS.md) — Agent architecture with federation\n- [federated_lakehouse_query.ipynb](federated_lakehouse_query.ipynb) — Original federated queries (Spark Connector + remote_query)\n- [Neo4j JDBC SQL2Cypher](https://neo4j.com/docs/jdbc-manual/current/sql2cypher/) — SQL translation rules\n- [Databricks remote_query()](https://docs.databricks.com/sql/language-manual/functions/remote_query) — Table-valued function reference"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}